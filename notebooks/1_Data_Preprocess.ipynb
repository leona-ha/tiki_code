{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"TIKI: Data Compliance and Quality\"\n",
    "author: \"Leona Hammelrath\"\n",
    "date: \"10 April 2024\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "\n",
    "format: html\n",
    "execute:\n",
    "    eval: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "First Data for Passive Data Collection using Smartwatches and GPS from the PREACT Study. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Treatment personalization is highly discussed to counteract insufficient response rates in psychotherapy. In the quest for criteria allowing informed selection or adaptation, ambulatory assessment data (i.e. EMA, passive sensing)are a key component, as processes happening outside of therapy sessions can be depicted in high temporal and/or spatial resolution.\n",
    "\n",
    "PREACT is a multicenter prospective-longitudinal study investigating different predictors of non-response (i.e. EEG, fMRI) in around 500 patients undergoing cognitive behavioral therapy for internalizing disorders (https://forschungsgruppe5187.de/de). \n",
    "\n",
    "## Methods\n",
    "Patients can enroll for therapy-accompanying ambulatory assessment. They are provided with a customized study app and a state-of-the-art smartwatch collecting passive data like GPS and heart rate for up to 365 days. In parallel, three 14-day EMA phases (pre-, mid- and post-therapy) cover transdiagnostic (i.e. emotion regulation), contextual and therapy-related aspects.  \n",
    "\n",
    "Here, we present first results on data compliance and quality for the passive sensing data as well as EMA assessments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import regex as re\n",
    "\n",
    "\n",
    "from config import datapath, proj_sheet,preprocessed_path, raw_path, redcap_path\n",
    "\n",
    "today = date.today().strftime(\"%d%m%Y\")\n",
    "today_day = pd.to_datetime('today').normalize()\n",
    "\n",
    "today = \"19082024\"\n",
    "\n",
    "df_monitoring = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{proj_sheet}/export?format=csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual passive + ema_data\n",
    "datapath1 = raw_path + f\"export_tiki_{today}/\"\n",
    "file_pattern = os.path.join(datapath1, \"epoch_part*.csv\")\n",
    "file_list = glob.glob(file_pattern)\n",
    "file_list.sort()\n",
    "df_complete = pd.concat((pd.read_csv(f, encoding=\"latin-1\", low_memory=False) for f in file_list), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df_complete[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_complete[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr[:\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m      6\u001b[0m df_complete[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_end\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_complete[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendTimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m df_complete[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartTimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m df_complete[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartTimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_complete[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartTimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m],unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m df_complete[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendTimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_complete[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendTimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m],unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m## Timezone offset \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Handle NaN in timezoneOffset by skipping or replacing with a default\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Here, we skip adjustments for NaN offsets by using np.where to check for NaN\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1046\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[0;32m-> 1046\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m _maybe_cache(arg, \u001b[38;5;28mformat\u001b[39m, cache, convert_listlike)\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m   1048\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:241\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mCreate a cache of unique dates from an array of dates\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    Cache of converted, unique dates. Can be empty\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[0;32m--> 241\u001b[0m cache_array \u001b[38;5;241m=\u001b[39m Series(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Perform a quicker unique check\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_cache(arg):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:490\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 490\u001b[0m     data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mmaybe_iterable_to_list(data)\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_list_like(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;66;03m# GH 29405: Pre-2.0, this defaulted to float.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mobject\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/common.py:300\u001b[0m, in \u001b[0;36mmaybe_iterable_to_list\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmaybe_iterable_to_list\u001b[39m(obj: Iterable[T] \u001b[38;5;241m|\u001b[39m T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection[T] \u001b[38;5;241m|\u001b[39m T:\n\u001b[1;32m    297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    If obj is Iterable but not list-like, consume into list.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, abc\u001b[38;5;241m.\u001b[39mIterable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, abc\u001b[38;5;241m.\u001b[39mSized):\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(obj)\n\u001b[1;32m    302\u001b[0m     obj \u001b[38;5;241m=\u001b[39m cast(Collection, obj)\n",
      "File \u001b[0;32m<frozen abc>:119\u001b[0m, in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n",
      "File \u001b[0;32m<frozen abc>:123\u001b[0m, in \u001b[0;36m__subclasscheck__\u001b[0;34m(cls, subclass)\u001b[0m\n",
      "File \u001b[0;32m<frozen abc>:123\u001b[0m, in \u001b[0;36m__subclasscheck__\u001b[0;34m(cls, subclass)\u001b[0m\n",
      "    \u001b[0;31m[... skipping similar frames: __subclasscheck__ at line 123 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m<frozen abc>:123\u001b[0m, in \u001b[0;36m__subclasscheck__\u001b[0;34m(cls, subclass)\u001b[0m\n",
      "File \u001b[0;32m<frozen _collections_abc>:409\u001b[0m, in \u001b[0;36m__subclasshook__\u001b[0;34m(cls, C)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### 1.1 Import epoch level passive + GPS data\n",
    "\n",
    "df_complete[\"customer\"] = df_complete.customer.str.split(\"@\").str.get(0)\n",
    "df_complete[\"customer\"] = df_complete[\"customer\"].str[:4]\n",
    "\n",
    "df_complete[\"start_end\"] = df_complete[\"endTimestamp\"] - df_complete[\"startTimestamp\"]\n",
    "df_complete[\"startTimestamp\"] = pd.to_datetime(df_complete[\"startTimestamp\"],unit='ms')\n",
    "df_complete[\"endTimestamp\"] = pd.to_datetime(df_complete[\"endTimestamp\"],unit='ms')\n",
    "\n",
    "## Timezone offset \n",
    "\n",
    "# Handle NaN in timezoneOffset by skipping or replacing with a default\n",
    "# Here, we skip adjustments for NaN offsets by using np.where to check for NaN\n",
    "df_complete['startTimestamp'] = np.where(df_complete['timezoneOffset'].isna(),\n",
    "                               df_complete['startTimestamp'],  # If NaN, keep original timestamp\n",
    "                               df_complete['startTimestamp'] + pd.to_timedelta(df_complete['timezoneOffset'], unit='m'))  # Else, apply offset\n",
    "\n",
    "df_complete['endTimestamp'] = np.where(df_complete['endTimestamp'].isna(),\n",
    "                               df_complete['endTimestamp'],  # If NaN, keep original timestamp\n",
    "                               df_complete['endTimestamp'] + pd.to_timedelta(df_complete['timezoneOffset'], unit='m'))  # Else, apply offset\n",
    "\n",
    "\n",
    "df_complete[\"startTimestamp_day\"] = df_complete.startTimestamp.dt.normalize()\n",
    "df_complete[\"startTimestamp_hour\"] = df_complete.startTimestamp.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with backup data\n",
    "backup_path = preprocessed_path + \"backup_data_passive.feather\"\n",
    "df_backup = pd.read_feather(backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_timestamp = df_backup['startTimestamp'].max()\n",
    "\n",
    "# Filter the second dataframe to include only entries after the latest timestamp\n",
    "df_complete_filtered = df_complete[df_complete['startTimestamp'] > latest_timestamp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_filtered = df_complete_filtered.drop(columns=['valueType', 'createdAt', 'source', \n",
    "                                                              'trustworthiness', 'medicalGrade', 'generation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = df_monitoring.copy()\n",
    "df_monitoring.rename(columns = {\"Pseudonym\": \"customer\", \"EMA_ID\": \"ema_id\", \"Status\": \"status\",\n",
    "                                \"Studienversion\":\"study_version\", \"FOR_ID\":\"for_id\", \n",
    "                           \"Start EMA Baseline\": \"ema_base_start\", \"Ende EMA Baseline\": \"ema_base_end\", \n",
    "                           \"Freischaltung/ Start EMA T20\": \"ema_t20_start\",\"Ende EMA T20\":\"ema_t20_end\", \n",
    "                               \"Termin 1. Gespräch\": \"first_call_date\", \"Freischaltung/ Start EMA Post\":\"ema_post_start\",\n",
    "                               \"Ende EMA Post\":\"ema_post_end\", \"T20=Post\":\"t20_post\" }, inplace=True)\n",
    "\n",
    "df_monitoring[\"customer\"] = df_monitoring[\"customer\"].str[:4]\n",
    "df_monitoring[\"for_id\"] = df_monitoring.for_id.str.strip()\n",
    "\n",
    "df_monitoring[\"ema_base_start\"] = pd.to_datetime(df_monitoring[\"ema_base_start\"], dayfirst=True)\n",
    "df_monitoring[\"ema_base_end\"] = pd.to_datetime(df_monitoring[\"ema_base_end\"], dayfirst=True)\n",
    "\n",
    "df_monitoring_short = df_monitoring[[\"customer\", \"status\", \"study_version\", \"ema_base_start\",\"ema_base_end\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_filtered= df_complete_filtered.merge(df_monitoring_short, on=\"customer\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = [\"booleanValue\", \"stringValue\",\"customer\", \"type\", \"status\", \"study_version\"] \n",
    "\n",
    "# Fill NaN values with -99 for the specified columns\n",
    "for col in object_cols:\n",
    "    df_complete_filtered[col] = df_complete_filtered[col].fillna(-99)\n",
    "\n",
    "# Convert \"booleanValue\" to boolean\n",
    "df_complete_filtered['booleanValue'] = df_complete_filtered['booleanValue'].apply(lambda x: bool(x) if x != -99 else False)\n",
    "\n",
    "# Convert \"stringValue\", \"status\", \"study_version\" to string using StringDtype\n",
    "df_complete_filtered['stringValue'] = df_complete_filtered['stringValue'].astype('string')\n",
    "df_complete_filtered['status'] = df_complete_filtered['status'].astype('string')\n",
    "df_complete_filtered['study_version'] = df_complete_filtered['study_version'].astype('string')\n",
    "df_complete_filtered['customer'] = df_complete_filtered['customer'].astype('string')\n",
    "df_complete_filtered['type'] = df_complete_filtered['type'].astype('string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_coverage(df, today_day, data_type_groups):\n",
    "    \"\"\"\n",
    "    Calculate the data coverage percentage for each customer, for each specified group of data types.\n",
    "\n",
    "    :param df: DataFrame containing customer data\n",
    "    :param today_day: The current date for calculating potential coverage\n",
    "    :param data_type_groups: A dictionary where keys are group names and values are lists of data types\n",
    "    :param status_col: Column name for the status\n",
    "    :param study_version_col: Column name for the study version\n",
    "    :param ema_base_end_col: Column name for the EMA base end date\n",
    "    :return: DataFrame with additional columns for data coverage percentages\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the date columns are datetime objects\n",
    "    df['startTimestamp_day'] = pd.to_datetime(df['startTimestamp_day'])\n",
    "\n",
    "    df['ema_base_end'] = pd.to_datetime(df['ema_base_end'])\n",
    "\n",
    "    # Find the earliest 'startTimestamp_day' for each customer\n",
    "    earliest_timestamp_per_customer = df.groupby('customer')['startTimestamp_day'].min()\n",
    "\n",
    "    # Map the earliest timestamp back to the original DataFrame\n",
    "    df['earliest_start_day'] = df['customer'].map(earliest_timestamp_per_customer)\n",
    "\n",
    "    # Calculate potential days of coverage from the earliest start day to today\n",
    "    df['potential_days_coverage'] = (today_day - df['earliest_start_day']).dt.days\n",
    "\n",
    "    # Define the condition for adjusting potential days coverage\n",
    "    condition = (\n",
    "        (df['status'] == 'Abgeschlossen') & \n",
    "        (df['study_version'].isin(['Kurz', 'Kurz (Wechsel/Abbruch)']))\n",
    "    )\n",
    "\n",
    "    # Adjust potential days of coverage based on the condition\n",
    "    df['potential_days_coverage'] = np.where(\n",
    "        condition,\n",
    "        (df['ema_base_end'] - df['earliest_start_day']).dt.days,\n",
    "        df['potential_days_coverage']\n",
    "    )\n",
    "\n",
    "    for group_name, data_types in data_type_groups.items():\n",
    "        # Filter for the current group of data types\n",
    "        df_type_group = df[df['type'].isin(data_types)]\n",
    "        \n",
    "        # Count unique days with data for each customer for the current data types\n",
    "        actual_days = df_type_group.groupby('customer')['startTimestamp_day'].nunique()\n",
    "\n",
    "        # Map the actual number of days back to the DataFrame\n",
    "        df[f'{group_name}_actual_days_with_data'] = df['customer'].map(actual_days).fillna(0)\n",
    "\n",
    "        # Calculate data coverage percentage for the current data types\n",
    "        df[f'{group_name}_data_coverage_per'] = (df[f'{group_name}_actual_days_with_data'] / df['potential_days_coverage']) * 100\n",
    "\n",
    "    # Drop intermediary columns if necessary\n",
    "    df.drop(columns=['earliest_start_day'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_groups = {\n",
    "    'GPS': [\"Latitude\"],\n",
    "    # Add more groups as needed\n",
    "    'Activity': [\"Steps\"],\n",
    "    'Sleep': [\"SleepBinary\"],\n",
    "    'Heart_Rate': [\"HeartRate\"]\n",
    "    # Add more groups as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_filtered_cov = calculate_data_coverage(df_complete_filtered, today_day, data_type_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "session = pd.read_csv(datapath1 + \"questionnaireSession.csv\",low_memory=False)\n",
    "answers = pd.read_csv(datapath1 + \"answers.csv\", low_memory=False)\n",
    "choice = pd.read_csv(datapath1 + \"choice.csv\",low_memory=False)\n",
    "questions = pd.read_csv(datapath1 + \"questions.csv\",low_memory=False)\n",
    "questionnaire = pd.read_csv(datapath1 + \"questionnaires.csv\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session data\n",
    "session[\"user\"] = session[\"user\"].str[:4]\n",
    "session.rename(columns = {\"user\":\"customer\",\"completedAt\": \"quest_complete\", \"createdAt\": \"quest_create\", \"expirationTimestamp\": \"quest_expir\"}, inplace=True)\n",
    "session[\"quest_create\"] = (pd.to_datetime(session[\"quest_create\"],unit='ms'))\n",
    "session[\"quest_complete\"] = (pd.to_datetime(session[\"quest_complete\"],unit='ms'))\n",
    "\n",
    "df_sess = session[[\"customer\", \"sessionRun\", \"quest_create\", \"quest_complete\", \"study\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer data \n",
    "answers[\"user\"] = answers[\"user\"].str[:4]\n",
    "answers = answers[[\"user\", \"questionnaireSession\", \"questionnaire\", \"study\", \n",
    "                   \"question\", \"order\",\"element\", \"createdAt\"]]\n",
    "answers[\"createdAt\"] = (pd.to_datetime(answers[\"createdAt\"],unit='ms'))\n",
    "answers.rename(columns={\"user\":\"customer\",\"questionnaireSession\":\"session_unique\", \"createdAt\": \"quest_create\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item description data\n",
    "choice = choice[[\"element\", \"choice_id\", \"text\", \"question\"]]\n",
    "choice.rename(columns={\"text\":\"choice_text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question description data\n",
    "questions = questions[[\"id\", \"title\"]]\n",
    "questions.rename(columns={\"id\":\"question\",\"title\":\"quest_title\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire = questionnaire[[\"id\", \"name\"]]\n",
    "questionnaire.rename(columns={\"id\":\"questionnaire\",\"name\":\"questionnaire_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_merged = pd.merge(answers, choice, on= [\"question\",\"element\"])\n",
    "answer_merged = pd.merge(answer_merged, questions, on= \"question\")\n",
    "answer_merged = pd.merge(answer_merged, questionnaire, on= \"questionnaire\")\n",
    "answer_merged[\"quest_complete_day\"] = answer_merged.quest_create.dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_merged = pd.merge(answer_merged, df_monitoring, on = \"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = pd.merge(df_sess, df_monitoring, on = \"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess[['customer', 'sessionRun', 'quest_create', 'quest_complete', 'study',\n",
    "       'for_id', 'ema_id', 'study_version', 'status', 't20_post',\n",
    "       'ema_base_start', 'ema_base_end','ema_t20_start', 'ema_t20_end',\n",
    "       'ema_post_start', 'ema_post_end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.copy()\n",
    "df_sess[\"quest_create_day\"] = df_sess.quest_create.dt.normalize()\n",
    "df_sess[\"quest_complete_day\"] = df_sess.quest_complete.dt.normalize()\n",
    "\n",
    "df_sess[\"quest_create_hour\"] = df_sess.quest_create.dt.hour\n",
    "df_sess[\"quest_complete_hour\"] = df_sess.quest_complete.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of completed EMA beeps in first phase\n",
    "df_sess1 = df_sess.loc[df_sess.study.isin([24,25])]\n",
    "df_sess1 = df_sess1.copy()\n",
    "\n",
    "df_sess2 = df_sess.loc[df_sess.study.isin([33,34])]\n",
    "df_sess2 = df_sess2.copy()\n",
    "\n",
    "df_sess3 = df_sess.loc[df_sess.study.isin([38,39])]\n",
    "df_sess3 = df_sess3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess1['quest_complete_relative1'] = (df_sess1['quest_complete_day'] - df_sess1['ema_base_start']).dt.days\n",
    "\n",
    "\n",
    "sess_count1 = df_sess1.dropna(subset=[\"quest_complete\"]).groupby(\"customer\")[\"quest_complete\"].size()\\\n",
    ".reset_index()\n",
    "sess_count1 = sess_count1.rename(columns = {\"quest_complete\":\"nquest_EMA1\"})\n",
    "\n",
    "# count number of completed EMA beeps in second phase\n",
    "sess_count2 = df_sess2.dropna(subset=[\"quest_complete\"]).groupby(\"customer\")[\"quest_complete\"].size()\\\n",
    ".reset_index()\n",
    "sess_count2 = sess_count2.rename(columns = {\"quest_complete\":\"nquest_EMA2\"})\n",
    "\n",
    "# count number of completed EMA beeps in second phase\n",
    "sess_count3 = df_sess3.dropna(subset=[\"quest_complete\"]).groupby(\"customer\")[\"quest_complete\"].size()\\\n",
    ".reset_index()\n",
    "sess_count3 = sess_count3.rename(columns = {\"quest_complete\":\"nquest_EMA3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.merge(sess_count1, on=['customer'], how='left')\n",
    "df_sess = df_sess.merge(sess_count2, on=['customer'], how='left')\n",
    "df_sess = df_sess.merge(sess_count3, on=['customer'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_content = answer_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some entries have unexpectedly high absolute day indices:\n",
      "Customers with high absolute day indices:\n",
      "['UfMn']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_ema_content['weekday'] = df_ema_content['quest_create'].dt.day_name()\n",
    "df_ema_content['createdAt_day'] = df_ema_content.quest_create.dt.normalize()\n",
    "# Convert columns to datetime\n",
    "df_ema_content['createdAt_day'] = pd.to_datetime(df_ema_content['createdAt_day'])\n",
    "df_ema_content['ema_base_start'] = pd.to_datetime(df_ema_content['ema_base_start'], dayfirst=True)\n",
    "df_ema_content['ema_t20_start'] = pd.to_datetime(df_ema_content['ema_t20_start'], dayfirst=True)\n",
    "df_ema_content['ema_post_start'] = pd.to_datetime(df_ema_content['ema_post_start'], dayfirst=True)\n",
    "\n",
    "\n",
    "study_mapping = {\n",
    "    24: 0,\n",
    "    25: 0,\n",
    "    33: 1,\n",
    "    34: 1,\n",
    "    38: 2,\n",
    "    39: 2\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'study' column\n",
    "df_ema_content['assess'] = df_ema_content['study'].map(study_mapping)\n",
    "# Replace '_morning' with '' in the 'quest_title' column as we don't need to differenciate\n",
    "df_ema_content['quest_title'] = df_ema_content['quest_title'].str.replace('_morning', '', regex=False)\n",
    "\n",
    "df_ema_content['weekend'] = [1 if day in ['Saturday', 'Sunday'] else 0 for day in df_ema_content['weekday']]\n",
    "\n",
    "df_ema_content['quest_nr'] = df_ema_content['questionnaire_name'].apply(lambda x: int(re.search(r'\\d+', x).group()) \\\n",
    "                                               if re.search(r'\\d+', x) else None)\n",
    "\n",
    "df_ema_content[\"n_quest\"] = df_ema_content.groupby([\"study\", \"customer\", \"createdAt_day\"])[\"questionnaire_name\"].transform(\"nunique\")\n",
    "\n",
    "# Create unique day ID\n",
    "# Create a unique day identifier directly without creating extra columns\n",
    "df_ema_content['unique_day_id'] = df_ema_content['createdAt_day'].dt.strftime('%Y%m%d') + '_' + df_ema_content['quest_nr'].astype(str)\n",
    "\n",
    "df_ema_content['ema_relative_start'] = df_ema_content.groupby(['customer', 'assess'])['createdAt_day'].transform('min')\n",
    "df_ema_content['ema_relative_end'] = df_ema_content.groupby(['customer', 'assess'])['createdAt_day'].transform('max')\n",
    "\n",
    "# Calculate ema_relative_start and ema_relative_end for each phase and customer\n",
    "phase_0 = df_ema_content[df_ema_content['assess'] == 0].groupby(['customer'])['createdAt_day'].agg(ema_relative_start_phase0='min', ema_relative_end_phase0='max').reset_index()\n",
    "phase_1 = df_ema_content[df_ema_content['assess'] == 1].groupby(['customer'])['createdAt_day'].agg(ema_relative_start_phase1='min', ema_relative_end_phase1='max').reset_index()\n",
    "phase_2 = df_ema_content[df_ema_content['assess'] == 2].groupby(['customer'])['createdAt_day'].agg(ema_relative_start_phase2='min', ema_relative_end_phase2='max').reset_index()\n",
    "\n",
    "# Merge these values back into the original DataFrame\n",
    "df_ema_content = df_ema_content.merge(phase_0, on='customer', how='left')\n",
    "df_ema_content = df_ema_content.merge(phase_1, on='customer', how='left')\n",
    "df_ema_content = df_ema_content.merge(phase_2, on='customer', how='left')\n",
    "\n",
    "# Map the correct start date for each phase\n",
    "df_ema_content['ema_relative_start'] = df_ema_content.apply(\n",
    "    lambda row: row['ema_relative_start_phase0'] if row['assess'] == 0 else (\n",
    "                row['ema_relative_start_phase1'] if row['assess'] == 1 else \n",
    "                row['ema_relative_start_phase2']), axis=1)\n",
    "\n",
    "# Now calculate the Absolute Day Index based on the correct start date\n",
    "df_ema_content['absolute_day_index'] = (\n",
    "    df_ema_content['createdAt_day'] - df_ema_content['ema_relative_start']\n",
    ").dt.days + 1\n",
    "\n",
    "high_indices = df_ema_content[df_ema_content['absolute_day_index'] > 180]\n",
    "if not high_indices.empty:\n",
    "    print(\"Warning: Some entries have unexpectedly high absolute day indices:\")\n",
    "    print(\"Customers with high absolute day indices:\")\n",
    "    print(high_indices['customer'].unique())\n",
    "\n",
    "# Calculate the Relative Day Index by counting unique days since ema_relative_start\n",
    "df_ema_content['relative_day_index'] = df_ema_content.groupby(['customer', 'assess'])['createdAt_day'].rank(method='dense').astype(int)\n",
    "\n",
    "# Drop duplicate rows based on 'customer', 'assess', and 'createdAt_day' to ensure only one counter per unique day\n",
    "df_unique = df_ema_content.drop_duplicates(subset=['customer', 'assess', 'unique_day_id']).copy()\n",
    "\n",
    "# Use .loc to avoid SettingWithCopyWarning\n",
    "df_unique['questionnaire_counter'] = df_unique.groupby(['customer', 'assess']).cumcount() + 1\n",
    "df_unique['questionnaire_counter'] = df_unique.questionnaire_counter.astype(int)\n",
    "\n",
    "\n",
    "# Merge this back into the original dataframe to retain other data if needed\n",
    "df_ema_content = pd.merge(df_ema_content, df_unique[['customer', 'assess', 'unique_day_id', 'questionnaire_counter']],\n",
    "                              on=['customer', 'assess', 'unique_day_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test =df_ema_content.loc[df_ema_content.customer =='UfMn' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_criteria = (df_ema_content['customer'] == 'UfMn') & \\\n",
    "                  (df_ema_content['study'] == 25) & \\\n",
    "                  (df_ema_content['quest_create'] > '2024-02-08')\n",
    "\n",
    "# Drop the entries that match the criteria\n",
    "df_ema_content = df_ema_content[~filter_criteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_base = df_ema_content[[\"customer\", 'ema_relative_start_phase0', 'ema_relative_end_phase0', \n",
    "                             'ema_relative_start_phase1', 'ema_relative_end_phase1',\n",
    "                              'ema_relative_start_phase2', 'ema_relative_end_phase2']]\n",
    "df_ema_base = df_ema_base.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_complete_ema = df_complete_filtered_cov.merge(df_ema_base, on = \"customer\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_complete_ema = pd.concat([df_backup, df_complete_ema], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_complete_ema = df_complete_final.merge(df_ema_base, on = \"customer\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate memory usage in bytes\n",
    "memory_usage_bytes = df_complete_ema.memory_usage(deep=True).sum()\n",
    "\n",
    "# Convert to megabytes\n",
    "memory_usage_mb = memory_usage_bytes / (1024 ** 2)\n",
    "\n",
    "# Convert to gigabytes\n",
    "memory_usage_gb = memory_usage_bytes / (1024 ** 3)\n",
    "\n",
    "# Convert to terabytes\n",
    "memory_usage_tb = memory_usage_bytes / (1024 ** 4)\n",
    "\n",
    "print(f\"Memory usage: {memory_usage_bytes} bytes\")\n",
    "print(f\"Memory usage: {memory_usage_mb:.2f} MB\")\n",
    "print(f\"Memory usage: {memory_usage_gb:.2f} GB\")\n",
    "print(f\"Memory usage: {memory_usage_tb:.2f} TB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redcap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_redcap_zert = pd.read_csv(redcap_path + \"/ZERTIFIZIERUNGFOR518-LeonaExport_DATA_2024-08-21_0954.csv\", low_memory=False)\n",
    "df_redcap = pd.read_csv(redcap_path + f\"/FOR5187-LeonaExport_DATA_2024-08-21_1024.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bsi_scales = {\n",
    "    'bsi_somatization': ['bsi_2', 'bsi_7', 'bsi_23', 'bsi_29', 'bsi_30', 'bsi_33', 'bsi_37'],\n",
    "    'bsi_compulsivity': ['bsi_5', 'bsi_15', 'bsi_26', 'bsi_27', 'bsi_32', 'bsi_36'],\n",
    "    'bsi_insecurity': ['bsi_20', 'bsi_21', 'bsi_22', 'bsi_42'],\n",
    "    'bsi_depression': ['bsi_9', 'bsi_16', 'bsi_17', 'bsi_18', 'bsi_35', 'bsi_50'],\n",
    "    'bsi_anxiety': ['bsi_1', 'bsi_12', 'bsi_19', 'bsi_38', 'bsi_45', 'bsi_49'],\n",
    "    'bsi_aggression': ['bsi_6', 'bsi_13', 'bsi_40', 'bsi_41', 'bsi_46'],\n",
    "    'bsi_phobia': ['bsi_8', 'bsi_28', 'bsi_31', 'bsi_43', 'bsi_47'],\n",
    "    'bsi_paranoia': ['bsi_4', 'bsi_10', 'bsi_24', 'bsi_48', 'bsi_51'],\n",
    "    'bsi_psychotizism': ['bsi_3', 'bsi_14', 'bsi_34', 'bsi_44', 'bsi_53'],\n",
    "    'bsi_additional': ['bsi_11', 'bsi_25', 'bsi_39', 'bsi_52']\n",
    "}\n",
    "\n",
    "# Check for missing columns and calculate each scale\n",
    "for scale, columns in bsi_scales.items():\n",
    "    missing_cols = [col for col in columns if col not in df_redcap_zert.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns in DataFrame for {scale}:\", missing_cols)\n",
    "    else:\n",
    "        # Sum the specified columns and create a new column for each scale\n",
    "        df_redcap_zert[scale] = df_redcap_zert[columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate BSI-GS in Certification dataset\n",
    "bsi_columns = [f'bsi_{i}' for i in range(1, 54)]\n",
    "df_redcap_zert['bsi_gs'] = df_redcap_zert[bsi_columns].sum(axis=1)\n",
    "df_redcap_zert['bsi_gsi'] = df_redcap_zert[\"bsi_gs\"]/53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_redcap_zert = df_redcap_zert[['for_id', 'redcap_event_name','ema_start_date','ema_smartphone', 'ema_special_event', 'age', 'gender', 'marital_status', 'partnership', 'graduation','profession','years_of_education',\n",
    " 'employability', 'ses', 'somatic_problems','scid_cv_prim_cat', 'bsi_somatization',\n",
    " 'bsi_compulsivity','bsi_insecurity','bsi_depression','bsi_anxiety','bsi_aggression','bsi_phobia','bsi_paranoia',\n",
    " 'bsi_psychotizism','bsi_additional','bsi_gs','bsi_gsi', 'prior_treatment', 'redcap_event_name']] #ema_wear_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for different assessment times \n",
    "df_redcap_base = df_redcap.loc[df_redcap.redcap_event_name == 'v1_baseline_arm_1']\n",
    "df_redcap_t20 = df_redcap.loc[df_redcap.redcap_event_name == 'v3_t20_arm_1']\n",
    "\n",
    "df_redcap_zert_base = df_redcap.loc[df_redcap.redcap_event_name == 'v1_baseline_arm_1']\n",
    "df_redcap_zert_t20 = df_redcap_zert.loc[df_redcap.redcap_event_name == 'v3_t20_arm_1']\n",
    "\n",
    "# BSI at T20 \n",
    "bsi_list = ['for_id', 'bsi_gsi']\n",
    "df_redcap_t20 = df_redcap_t20[bsi_list]\n",
    "df_redcap_zert_t20 = df_redcap_zert_t20[bsi_list]\n",
    "df_t20 = pd.concat([df_redcap_t20, df_redcap_zert_t20],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_redcap = pd.concat([df_redcap_base, df_redcap_zert_base],ignore_index=True)\n",
    "df_redcap = pd.merge(df_redcap, df_t20,on='for_id', suffixes=('_base', '_t20'), how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_redcap = df_redcap[['for_id', 'ema_start_date','ema_smartphone',\n",
    " 'ema_wear_exp', 'ema_special_event', 'age', 'gender', 'marital_status', 'partnership', 'graduation','profession','years_of_education',\n",
    " 'employability','ses', 'somatic_problems','scid_cv_prim_cat','bsi_somatization', 'prior_treatment',\n",
    " 'bsi_compulsivity','bsi_insecurity','bsi_depression','bsi_anxiety','bsi_aggression','bsi_phobia','bsi_paranoia',\n",
    " 'bsi_psychotizism','bsi_additional','bsi_gs','bsi_gsi_base', 'bsi_gsi_t20']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_redcap = df_redcap.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gender_mapping = {\n",
    "    1: 'male',\n",
    "    2: 'female',\n",
    "    3: 'diverse',\n",
    "    4: 'no gender',\n",
    "    5: 'not specified'\n",
    "}\n",
    "\n",
    "scid_cv_cat_mapping = {\n",
    "    1: 'Depressive Disorder',\n",
    "    2: 'Specific Phobia',\n",
    "    3: 'Social Anxiety Disorder',\n",
    "    4: 'Agoraphobia and/or Panic Disorder',\n",
    "    5: 'Generalized Anxiety Disorder',\n",
    "    6: 'Obsessive-Compulsive Disorder',\n",
    "    7: 'Post-Traumatic Stress Disorder'\n",
    "}\n",
    "\n",
    "marital_status_mapping = {\n",
    "    1: 'single',\n",
    "    2: 'married/registered partnership',\n",
    "    3: 'divorced',\n",
    "    4: 'separated',\n",
    "    5: 'widowed',\n",
    "    6: 'other'\n",
    "}\n",
    "\n",
    "employability_mapping = {\n",
    "    0: 'employable',\n",
    "    1: 'unemployable (on sick leave)',\n",
    "    2: 'on disability pension',\n",
    "    3: 'on retirement pension',\n",
    "    4: 'other'\n",
    "}\n",
    "\n",
    "graduation_mapping = {\n",
    "    0: 'still in school',\n",
    "    1: 'no school degree',\n",
    "    2: 'elementary school degree or equivalent',\n",
    "    3: 'middle school degree or equivalent',\n",
    "    4: 'high school diploma/university entrance qualification',\n",
    "    5: 'other'\n",
    "}\n",
    "\n",
    "profession_mapping = {\n",
    "    0: 'still in training or studies',\n",
    "    1: 'no training degree',\n",
    "    2: 'vocational training, including technical school',\n",
    "    3: 'university or college degree',\n",
    "    4: 'other'\n",
    "}\n",
    "\n",
    "prior_treatment_mapping = {\n",
    "    0: 'no prior treatment',\n",
    "    1: 'outpatient psychotherapy',\n",
    "    2: 'inpatient or partial inpatient treatment/psychotherapy',\n",
    "    3: 'both',\n",
    "    4: 'yes'\n",
    "}\n",
    "\n",
    "ema_smartphone_mapping = {\n",
    "    1: 'iPhone',\n",
    "    0: 'Android'\n",
    "}\n",
    "\n",
    "ema_special_event_mapping = {\n",
    "    0: 'usual',\n",
    "    1: 'special event'\n",
    "}\n",
    "def categorize_age(age):\n",
    "    if 18 <= age <= 24:\n",
    "        return 0\n",
    "    elif 25 <= age <= 34:\n",
    "        return 1\n",
    "    elif 35 <= age <= 44:\n",
    "        return 2\n",
    "    elif 45 <= age <= 54:\n",
    "        return 3\n",
    "    elif 55 <= age <= 64:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply mappings\n",
    "df_redcap['gender_description'] = df_redcap['gender'].map(gender_mapping)\n",
    "df_redcap['scid_cv_description'] = df_redcap['scid_cv_prim_cat'].map(scid_cv_cat_mapping)\n",
    "df_redcap['marital_status_description'] = df_redcap['marital_status'].map(marital_status_mapping)\n",
    "df_redcap['employability_description'] = df_redcap['employability'].map(employability_mapping)\n",
    "df_redcap['graduation_description'] = df_redcap['graduation'].map(graduation_mapping)\n",
    "df_redcap['profession_description'] = df_redcap['profession'].map(profession_mapping)\n",
    "df_redcap['prior_treatment_description'] = df_redcap['prior_treatment'].map(prior_treatment_mapping)\n",
    "df_redcap['ema_smartphone_description'] = df_redcap['ema_smartphone'].map(ema_smartphone_mapping)\n",
    "df_redcap['ema_special_event_description'] = df_redcap['ema_special_event'].map(ema_special_event_mapping)\n",
    "df_redcap['age_description'] = df_redcap['age'].apply(categorize_age)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_monitoring[\"for_id\"] = df_monitoring.for_id.str.strip()\n",
    "df_forid = df_monitoring[[\"for_id\",\"customer\"]]\n",
    "df_redcap = pd.merge(df_forid, df_redcap, on=\"for_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_df = df_redcap.dropna(subset=['ema_start_date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_complete_ema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m backup_path \u001b[38;5;241m=\u001b[39m preprocessed_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackup_data_passive.feather\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df_complete_ema\u001b[38;5;241m.\u001b[39mto_feather(backup_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(preprocessed_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/ema_data.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(df_sess, file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_complete_ema' is not defined"
     ]
    }
   ],
   "source": [
    "backup_path = preprocessed_path + \"backup_data_passive.feather\"\n",
    "df_complete_ema.to_feather(backup_path)\n",
    "\n",
    "with open(preprocessed_path + f'/ema_data.pkl', 'wb') as file:\n",
    "    pickle.dump(df_sess, file)\n",
    "    \n",
    "with open(preprocessed_path + f'/monitoring_data.pkl', 'wb') as file:\n",
    "    pickle.dump(df_monitoring, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(preprocessed_path + f'/redcap_data.pkl', 'wb') as file:\n",
    "    pickle.dump(valid_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "with open(preprocessed_path + f'/ema_content.pkl', 'wb') as file:\n",
    "    pickle.dump(df_ema_content, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
