{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"TIKI: Data Compliance and Quality\"\n",
    "author: \"Leona Hammelrath\"\n",
    "date: \"10 April 2024\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "\n",
    "format: html\n",
    "execute:\n",
    "    eval: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "First Data for Passive Data Collection using Smartwatches and GPS from the PREACT Study. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Treatment personalization is highly discussed to counteract insufficient response rates in psychotherapy. In the quest for criteria allowing informed selection or adaptation, ambulatory assessment data (i.e. EMA, passive sensing)are a key component, as processes happening outside of therapy sessions can be depicted in high temporal and/or spatial resolution.\n",
    "\n",
    "PREACT is a multicenter prospective-longitudinal study investigating different predictors of non-response (i.e. EEG, fMRI) in around 500 patients undergoing cognitive behavioral therapy for internalizing disorders (https://forschungsgruppe5187.de/de). \n",
    "\n",
    "## Methods\n",
    "Patients can enroll for therapy-accompanying ambulatory assessment. They are provided with a customized study app and a state-of-the-art smartwatch collecting passive data like GPS and heart rate for up to 365 days. In parallel, three 14-day EMA phases (pre-, mid- and post-therapy) cover transdiagnostic (i.e. emotion regulation), contextual and therapy-related aspects.  \n",
    "\n",
    "Here, we present first results on data compliance and quality for the passive sensing data as well as EMA assessments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from config import datapath, proj_sheet,preprocessed_path, raw_path\n",
    "\n",
    "today = date.today().strftime(\"%d%m%Y\")\n",
    "today_day = pd.to_datetime('today').normalize()\n",
    "\n",
    "\n",
    "df_monitoring = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{proj_sheet}/export?format=csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = df_monitoring.copy()\n",
    "df_monitoring.rename(columns = {\"Pseudonym\": \"customer\", \"EMA_ID\": \"ema_id\", \"Status\": \"status\",\n",
    "                                \"Studienversion\":\"study_version\", \"FOR_ID\":\"for_id\", \n",
    "                           \"Start EMA Baseline\": \"ema_base_start\", \"Ende EMA Baseline\": \"ema_base_end\", \n",
    "                           \"Freischaltung/ Start EMA T20\": \"ema_t20_start\",\"Ende EMA T20\":\"ema_t20_end\", \n",
    "                               \"Termin 1. Gespr√§ch\": \"first_call_date\", \"Freischaltung/ Start EMA Post\":\"ema_post_start\",\n",
    "                               \"Ende EMA Post\":\"ema_post_end\", \"T20=Post\":\"t20_post\" }, inplace=True)\n",
    "\n",
    "df_monitoring[\"customer\"] = df_monitoring[\"customer\"].str[:4]\n",
    "df_monitoring[\"for_id\"] = df_monitoring.for_id.str.strip()\n",
    "\n",
    "df_monitoring[\"ema_base_start\"] = pd.to_datetime(df_monitoring[\"ema_base_start\"], dayfirst=True)\n",
    "df_monitoring[\"ema_base_end\"] = pd.to_datetime(df_monitoring[\"ema_base_end\"], dayfirst=True)\n",
    "\n",
    "df_monitoring_short = df_monitoring[[\"customer\", \"status\", \"study_version\", \"ema_base_start\",\"ema_base_end\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small backup passive data\n",
    "#file_pattern_back_1 = os.path.join(datapath, 'raw/tiki_backup_files/export_tiki_27052024/\"epoch_part*.csv\"')\n",
    "datapath_back = os.path.join(raw_path, \"tiki_backup_files/export_tiki_21052024/\")\n",
    "file_pattern_back_1 = os.path.join(datapath_back,\"epoch_part*.csv\")  # Adjust the path and extension if needed\n",
    "\n",
    "backup_files = glob.glob(file_pattern_back_1)\n",
    "file_list = glob.glob(file_pattern_back_1)\n",
    "file_list.sort()\n",
    "df_backup_small = pd.concat((pd.read_csv(f, encoding=\"latin-1\", low_memory=False) for f in file_list), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup_small[\"start_end\"] = df_backup_small[\"endTimestamp\"] - df_backup_small[\"startTimestamp\"]\n",
    "df_backup_small[\"startTimestamp\"] = pd.to_datetime(df_backup_small[\"startTimestamp\"],unit='ms')\n",
    "df_backup_small[\"endTimestamp\"] = pd.to_datetime(df_backup_small[\"endTimestamp\"],unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup_small[\"customer\"] = df_backup_small.customer.str.split(\"@\").str.get(0)\n",
    "df_backup_small[\"customer\"] = df_backup_small[\"customer\"].str[:4]\n",
    "\n",
    "df_backup_small['startTimestamp'] = df_backup_small['startTimestamp'] + pd.to_timedelta(df_backup_small['timezoneOffset'], unit='m')\n",
    "df_backup_small['endTimestamp'] = df_backup_small['endTimestamp'] + pd.to_timedelta(df_backup_small['timezoneOffset'], unit='m')\n",
    "\n",
    "df_backup_small[\"startTimestamp_day\"] = df_backup_small.startTimestamp.dt.normalize()\n",
    "df_backup_small[\"startTimestamp_hour\"] = df_backup_small.startTimestamp.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the pattern for big backup passive data files\n",
    "file_pattern_back_2 = os.path.join(raw_path, 'tiki_backup_files/tiki_backup_*.csv')\n",
    "\n",
    "# Use glob to find all matching files\n",
    "big_backup_files = glob.glob(file_pattern_back_2)\n",
    "\n",
    "# Define the dtype for columns that are known to be problematic\n",
    "dtype_spec = {\n",
    "    'startTimestamp': 'str',  # Load as string initially\n",
    "    'endTimestamp': 'str'     # Load as string initially\n",
    "}\n",
    "\n",
    "# Create a list to hold all the dataframes\n",
    "all_dfs = []\n",
    "\n",
    "# Loop over the files and read them with date parsing\n",
    "for filename in big_backup_files:\n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        dtype=dtype_spec,  # Load timestamps as strings first\n",
    "        low_memory=False  # Ensure proper memory handling\n",
    "    )\n",
    "    \n",
    "    # Convert the timestamp columns to datetime, ensuring proper parsing of ISO 8601 format\n",
    "    df['starttimestamp'] = pd.to_datetime(df['startTimestamp'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce')\n",
    "    df['endtimestamp'] = pd.to_datetime(df['endTimestamp'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce')\n",
    "\n",
    "    \n",
    "    # Additional processing (e.g., adding date and index columns)\n",
    "    match = re.match(r'tiki_backup_(\\d{4}-\\d{2}-\\d{2})_(\\d+)\\.csv', os.path.basename(filename))\n",
    "    if match:\n",
    "        date_part = match.group(1)  # Extract the date\n",
    "        index_part = int(match.group(2))  # Extract the index\n",
    "        # Add the date and index as new columns to the dataframe\n",
    "        df['date'] = date_part\n",
    "        df['index'] = index_part\n",
    "\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "df_backup_big = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Sort the merged dataframe by 'date' and 'index'\n",
    "\n",
    "# Optionally, drop the 'date' and 'index' columns if they are no longer needed\n",
    "df_backup_big.drop(columns=['date', 'index'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'startTimestamp' and 'endTimestamp' columns to datetime objects\n",
    "df_backup_big['startTimestamp'] = pd.to_datetime(df_backup_big['startTimestamp'], errors='coerce')\n",
    "df_backup_big['endTimestamp'] = pd.to_datetime(df_backup_big['endTimestamp'], errors='coerce')\n",
    "# Convert the 'startTimestamp' and 'endTimestamp' columns to datetime objects\n",
    "df_backup_big['startTimestamp'] = pd.to_datetime(df_backup_big['startTimestamp'], errors='coerce')\n",
    "df_backup_big['endTimestamp'] = pd.to_datetime(df_backup_big['endTimestamp'], errors='coerce')\n",
    "\n",
    "# Adjust for timezone offset\n",
    "df_backup_big['startTimestamp'] = df_backup_big['startTimestamp'] + pd.to_timedelta(df_backup_big['timezoneOffset'], unit='m')\n",
    "df_backup_big['endTimestamp'] = df_backup_big['endTimestamp'] + pd.to_timedelta(df_backup_big['timezoneOffset'], unit='m')\n",
    "\n",
    "\n",
    "# Format the datetime objects to the desired format\n",
    "df_backup_big['startTimestamp'] = df_backup_big['startTimestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_backup_big['endTimestamp'] = df_backup_big['endTimestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_backup_big['startTimestamp'] = pd.to_datetime(df_backup_big['startTimestamp'], errors='coerce')\n",
    "df_backup_big['endTimestamp'] = pd.to_datetime(df_backup_big['endTimestamp'], errors='coerce')\n",
    "\n",
    "df_backup_big[\"startTimestamp_day\"] = df_backup_big.startTimestamp.dt.normalize()\n",
    "df_backup_big[\"startTimestamp_hour\"] = df_backup_big.startTimestamp.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 2 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_timestamp_big = df_backup_big['startTimestamp'].max()\n",
    "\n",
    "# Filter the second dataframe to include only entries after the latest timestamp\n",
    "df_backup_small_filtered = df_backup_small[df_backup_small['startTimestamp'] > latest_timestamp_big]\n",
    "\n",
    "# Concatenate the first dataframe with the filtered second dataframe\n",
    "result_df_final = pd.concat([df_backup_big, df_backup_small_filtered], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort the original DataFrames by startTimestamp\n",
    "df_backup_big_sorted = df_backup_big.sort_values(by='startTimestamp')\n",
    "df_backup_small_filtered_sorted = df_backup_small_filtered.sort_values(by='startTimestamp')\n",
    "\n",
    "# Concatenate the sorted DataFrames\n",
    "result_df_final = pd.concat([df_backup_big_sorted, df_backup_small_filtered_sorted], ignore_index=True)\n",
    "\n",
    "# Optional: Sort the final DataFrame again to ensure everything is in order\n",
    "result_df_final = result_df_final.sort_values(by='startTimestamp').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final[\"customer\"] = result_df_final.customer.str.split(\"@\").str.get(0)\n",
    "result_df_final[\"customer\"] = result_df_final[\"customer\"].str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum timestamp: 2023-03-27 04:46:00\n",
      "Maximum timestamp: 2024-05-21 03:25:01\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum timestamp:\", result_df_final['startTimestamp'].min())\n",
    "print(\"Maximum timestamp:\", result_df_final['startTimestamp'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final_merged = result_df_final.merge(df_monitoring_short, on=\"customer\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum timestamp: 2023-05-17 18:44:00\n",
      "Maximum timestamp: 2024-05-21 03:25:01\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum timestamp:\", result_df_final_merged['startTimestamp'].min())\n",
    "print(\"Maximum timestamp:\", result_df_final_merged['startTimestamp'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum timestamp: 2023-05-17 18:44:00\n",
      "Maximum timestamp: 2024-05-21 03:25:01\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum timestamp:\", result_df_final_merged['startTimestamp'].min())\n",
    "print(\"Maximum timestamp:\", result_df_final_merged['startTimestamp'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_coverage(df, today_day, data_type_groups):\n",
    "    \"\"\"\n",
    "    Calculate the data coverage percentage for each customer, for each specified group of data types.\n",
    "\n",
    "    :param df: DataFrame containing customer data\n",
    "    :param today_day: The current date for calculating potential coverage\n",
    "    :param data_type_groups: A dictionary where keys are group names and values are lists of data types\n",
    "    :param status_col: Column name for the status\n",
    "    :param study_version_col: Column name for the study version\n",
    "    :param ema_base_end_col: Column name for the EMA base end date\n",
    "    :return: DataFrame with additional columns for data coverage percentages\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the date columns are datetime objects\n",
    "    df['startTimestamp_day'] = pd.to_datetime(df['startTimestamp_day'])\n",
    "    df['ema_base_end'] = pd.to_datetime(df['ema_base_end'])\n",
    "\n",
    "    # Find the earliest 'startTimestamp_day' for each customer\n",
    "    earliest_timestamp_per_customer = df.groupby('customer')['startTimestamp_day'].min()\n",
    "\n",
    "    # Map the earliest timestamp back to the original DataFrame\n",
    "    df['earliest_start_day'] = df['customer'].map(earliest_timestamp_per_customer)\n",
    "\n",
    "    # Calculate potential days of coverage from the earliest start day to today\n",
    "    df['potential_days_coverage'] = (today_day - df['earliest_start_day']).dt.days\n",
    "\n",
    "    # Define the condition for adjusting potential days coverage\n",
    "    condition = (\n",
    "        (df['status'] == 'Abgeschlossen') & \n",
    "        (df['study_version'].isin(['Kurz', 'Kurz (Wechsel/Abbruch)']))\n",
    "    )\n",
    "\n",
    "    # Adjust potential days of coverage based on the condition\n",
    "    df['potential_days_coverage'] = np.where(\n",
    "        condition,\n",
    "        (df['ema_base_end'] - df['earliest_start_day']).dt.days,\n",
    "        df['potential_days_coverage']\n",
    "    )\n",
    "\n",
    "    for group_name, data_types in data_type_groups.items():\n",
    "        # Filter for the current group of data types\n",
    "        df_type_group = df[df['type'].isin(data_types)]\n",
    "        \n",
    "        # Count unique days with data for each customer for the current data types\n",
    "        actual_days = df_type_group.groupby('customer')['startTimestamp_day'].nunique()\n",
    "\n",
    "        # Map the actual number of days back to the DataFrame\n",
    "        df[f'{group_name}_actual_days_with_data'] = df['customer'].map(actual_days).fillna(0)\n",
    "\n",
    "        # Calculate data coverage percentage for the current data types\n",
    "        df[f'{group_name}_data_coverage_per'] = (df[f'{group_name}_actual_days_with_data'] / df['potential_days_coverage']) * 100\n",
    "\n",
    "    # Drop intermediary columns if necessary\n",
    "    df.drop(columns=['earliest_start_day'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_groups = {\n",
    "    'GPS': [\"Latitude\"],\n",
    "    # Add more groups as needed\n",
    "    'Activity': [\"Steps\"],\n",
    "    'Sleep': [\"SleepBinary\"],\n",
    "    'Heart_Rate': [\"HeartRate\"]\n",
    "    # Add more groups as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final_merged = calculate_data_coverage(result_df_final_merged, today_day, data_type_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final_merged = result_df_final_merged.drop(columns=['valueType', 'createdAt', 'source', 'trustworthiness', 'medicalGrade', 'generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = [\"booleanValue\", \"stringValue\", \"status\", \"study_version\", \"customer\", \"type\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with -99 for the specified columns\n",
    "for col in object_cols:\n",
    "    result_df_final_merged[col] = result_df_final_merged[col].fillna(-99)\n",
    "\n",
    "# Convert \"booleanValue\" to boolean\n",
    "result_df_final_merged['booleanValue'] = result_df_final_merged['booleanValue'].apply(lambda x: bool(x) if x != -99 else False)\n",
    "\n",
    "# Convert \"stringValue\", \"status\", \"study_version\" to string using StringDtype\n",
    "result_df_final_merged['stringValue'] = result_df_final_merged['stringValue'].astype('string')\n",
    "result_df_final_merged['status'] = result_df_final_merged['status'].astype('string')\n",
    "result_df_final_merged['study_version'] = result_df_final_merged['study_version'].astype('string')\n",
    "result_df_final_merged['customer'] = result_df_final_merged['customer'].astype('string')\n",
    "result_df_final_merged['type'] = result_df_final_merged['type'].astype('string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19106416138 bytes\n",
      "Memory usage: 18221.30 MB\n",
      "Memory usage: 17.79 GB\n",
      "Memory usage: 0.02 TB\n"
     ]
    }
   ],
   "source": [
    "# Calculate memory usage in bytes\n",
    "memory_usage_bytes = result_df_final.memory_usage(deep=True).sum()\n",
    "\n",
    "# Convert to megabytes\n",
    "memory_usage_mb = memory_usage_bytes / (1024 ** 2)\n",
    "\n",
    "# Convert to gigabytes\n",
    "memory_usage_gb = memory_usage_bytes / (1024 ** 3)\n",
    "\n",
    "# Convert to terabytes\n",
    "memory_usage_tb = memory_usage_bytes / (1024 ** 4)\n",
    "\n",
    "print(f\"Memory usage: {memory_usage_bytes} bytes\")\n",
    "print(f\"Memory usage: {memory_usage_mb:.2f} MB\")\n",
    "print(f\"Memory usage: {memory_usage_gb:.2f} GB\")\n",
    "print(f\"Memory usage: {memory_usage_tb:.2f} TB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_path = preprocessed_path + \"backup_data_passive_general.feather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to HDF5 format\n",
    "result_df_final_merged.to_feather(backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-05-17 18:44:00')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_final_merged.startTimestamp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-05-21 03:25:01')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_final_merged.startTimestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31342749, 30)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_final_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>type</th>\n",
       "      <th>startTimestamp</th>\n",
       "      <th>endTimestamp</th>\n",
       "      <th>doubleValue</th>\n",
       "      <th>longValue</th>\n",
       "      <th>booleanValue</th>\n",
       "      <th>dateValue</th>\n",
       "      <th>stringValue</th>\n",
       "      <th>userReliability</th>\n",
       "      <th>...</th>\n",
       "      <th>ema_base_end</th>\n",
       "      <th>potential_days_coverage</th>\n",
       "      <th>GPS_actual_days_with_data</th>\n",
       "      <th>GPS_data_coverage_per</th>\n",
       "      <th>Activity_actual_days_with_data</th>\n",
       "      <th>Activity_data_coverage_per</th>\n",
       "      <th>Sleep_actual_days_with_data</th>\n",
       "      <th>Sleep_data_coverage_per</th>\n",
       "      <th>Heart_Rate_actual_days_with_data</th>\n",
       "      <th>Heart_Rate_data_coverage_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>Steps</td>\n",
       "      <td>2023-05-17 18:44:00</td>\n",
       "      <td>2023-05-17 18:45:00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>474.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.2827</td>\n",
       "      <td>348.0</td>\n",
       "      <td>73.417722</td>\n",
       "      <td>329.0</td>\n",
       "      <td>69.409283</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>ActiveBurnedCalories</td>\n",
       "      <td>2023-05-17 18:44:00</td>\n",
       "      <td>2023-05-17 18:45:00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>474.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.2827</td>\n",
       "      <td>348.0</td>\n",
       "      <td>73.417722</td>\n",
       "      <td>329.0</td>\n",
       "      <td>69.409283</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>CoveredDistance</td>\n",
       "      <td>2023-05-17 18:44:00</td>\n",
       "      <td>2023-05-17 18:45:00</td>\n",
       "      <td>4.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>474.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.2827</td>\n",
       "      <td>348.0</td>\n",
       "      <td>73.417722</td>\n",
       "      <td>329.0</td>\n",
       "      <td>69.409283</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>HeartRate</td>\n",
       "      <td>2023-05-17 18:58:01</td>\n",
       "      <td>2023-05-17 18:58:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>474.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.2827</td>\n",
       "      <td>348.0</td>\n",
       "      <td>73.417722</td>\n",
       "      <td>329.0</td>\n",
       "      <td>69.409283</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>ActiveBurnedCalories</td>\n",
       "      <td>2023-05-17 19:04:00</td>\n",
       "      <td>2023-05-17 19:05:00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>474.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.2827</td>\n",
       "      <td>348.0</td>\n",
       "      <td>73.417722</td>\n",
       "      <td>329.0</td>\n",
       "      <td>69.409283</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer                  type      startTimestamp        endTimestamp  \\\n",
       "0     4MLe                 Steps 2023-05-17 18:44:00 2023-05-17 18:45:00   \n",
       "1     4MLe  ActiveBurnedCalories 2023-05-17 18:44:00 2023-05-17 18:45:00   \n",
       "2     4MLe       CoveredDistance 2023-05-17 18:44:00 2023-05-17 18:45:00   \n",
       "3     4MLe             HeartRate 2023-05-17 18:58:01 2023-05-17 18:58:38   \n",
       "4     4MLe  ActiveBurnedCalories 2023-05-17 19:04:00 2023-05-17 19:05:00   \n",
       "\n",
       "   doubleValue  longValue  booleanValue  dateValue stringValue  \\\n",
       "0         6.00        NaN         False        NaN         -99   \n",
       "1         0.14        NaN         False        NaN         -99   \n",
       "2         4.62        NaN         False        NaN         -99   \n",
       "3          NaN       74.0         False        NaN         -99   \n",
       "4         0.36        NaN         False        NaN         -99   \n",
       "\n",
       "   userReliability  ...  ema_base_end  potential_days_coverage  \\\n",
       "0              NaN  ...    2023-05-31                    474.0   \n",
       "1              NaN  ...    2023-05-31                    474.0   \n",
       "2              NaN  ...    2023-05-31                    474.0   \n",
       "3              NaN  ...    2023-05-31                    474.0   \n",
       "4              NaN  ...    2023-05-31                    474.0   \n",
       "\n",
       "  GPS_actual_days_with_data GPS_data_coverage_per  \\\n",
       "0                      44.0                9.2827   \n",
       "1                      44.0                9.2827   \n",
       "2                      44.0                9.2827   \n",
       "3                      44.0                9.2827   \n",
       "4                      44.0                9.2827   \n",
       "\n",
       "  Activity_actual_days_with_data  Activity_data_coverage_per  \\\n",
       "0                          348.0                   73.417722   \n",
       "1                          348.0                   73.417722   \n",
       "2                          348.0                   73.417722   \n",
       "3                          348.0                   73.417722   \n",
       "4                          348.0                   73.417722   \n",
       "\n",
       "   Sleep_actual_days_with_data Sleep_data_coverage_per  \\\n",
       "0                        329.0               69.409283   \n",
       "1                        329.0               69.409283   \n",
       "2                        329.0               69.409283   \n",
       "3                        329.0               69.409283   \n",
       "4                        329.0               69.409283   \n",
       "\n",
       "  Heart_Rate_actual_days_with_data Heart_Rate_data_coverage_per  \n",
       "0                            343.0                    72.362869  \n",
       "1                            343.0                    72.362869  \n",
       "2                            343.0                    72.362869  \n",
       "3                            343.0                    72.362869  \n",
       "4                            343.0                    72.362869  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_final_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tiki]",
   "language": "python",
   "name": "conda-env-.conda-tiki-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
