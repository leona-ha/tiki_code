{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"TIKI: Data Compliance and Quality\"\n",
    "author: \"Leona Hammelrath\"\n",
    "date: \"10 April 2024\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "\n",
    "format: html\n",
    "execute:\n",
    "    eval: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "First Data for Passive Data Collection using Smartwatches and GPS from the PREACT Study. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Treatment personalization is highly discussed to counteract insufficient response rates in psychotherapy. In the quest for criteria allowing informed selection or adaptation, ambulatory assessment data (i.e. EMA, passive sensing)are a key component, as processes happening outside of therapy sessions can be depicted in high temporal and/or spatial resolution.\n",
    "\n",
    "PREACT is a multicenter prospective-longitudinal study investigating different predictors of non-response (i.e. EEG, fMRI) in around 500 patients undergoing cognitive behavioral therapy for internalizing disorders (https://forschungsgruppe5187.de/de). \n",
    "\n",
    "## Methods\n",
    "Patients can enroll for therapy-accompanying ambulatory assessment. They are provided with a customized study app and a state-of-the-art smartwatch collecting passive data like GPS and heart rate for up to 365 days. In parallel, three 14-day EMA phases (pre-, mid- and post-therapy) cover transdiagnostic (i.e. emotion regulation), contextual and therapy-related aspects.  \n",
    "\n",
    "Here, we present first results on data compliance and quality for the passive sensing data as well as EMA assessments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from server_config import datapath, proj_sheet,preprocessed_path, raw_path\n",
    "\n",
    "today = date.today().strftime(\"%d%m%Y\")\n",
    "today_day = pd.to_datetime('today').normalize()\n",
    "\n",
    "\n",
    "df_monitoring = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{proj_sheet}/export?format=csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = df_monitoring.copy()\n",
    "df_monitoring.rename(columns = {\"Pseudonym\": \"customer\", \"EMA_ID\": \"ema_id\", \"Status\": \"status\",\n",
    "                                \"Studienversion\":\"study_version\", \"FOR_ID\":\"for_id\", \n",
    "                           \"Start EMA Baseline\": \"ema_base_start\", \"Ende EMA Baseline\": \"ema_base_end\", \n",
    "                           \"Freischaltung/ Start EMA T20\": \"ema_t20_start\",\"Ende EMA T20\":\"ema_t20_end\", \n",
    "                               \"Termin 1. GesprÃ¤ch\": \"first_call_date\", \"Freischaltung/ Start EMA Post\":\"ema_post_start\",\n",
    "                               \"Ende EMA Post\":\"ema_post_end\", \"T20=Post\":\"t20_post\" }, inplace=True)\n",
    "\n",
    "df_monitoring[\"customer\"] = df_monitoring[\"customer\"].str[:4]\n",
    "df_monitoring[\"for_id\"] = df_monitoring.for_id.str.strip()\n",
    "\n",
    "df_monitoring[\"ema_base_start\"] = pd.to_datetime(df_monitoring[\"ema_base_start\"], dayfirst=True)\n",
    "df_monitoring[\"ema_base_end\"] = pd.to_datetime(df_monitoring[\"ema_base_end\"], dayfirst=True)\n",
    "\n",
    "df_monitoring_short = df_monitoring[[\"customer\", \"status\", \"study_version\", \"ema_base_start\",\"ema_base_end\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m file_list \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(file_pattern_back_1)\n\u001b[1;32m      8\u001b[0m file_list\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m----> 9\u001b[0m df_backup_small \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatin-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tiki/lib/python3.13/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.conda/envs/tiki/lib/python3.13/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/.conda/envs/tiki/lib/python3.13/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# small backup passive data\n",
    "#file_pattern_back_1 = os.path.join(datapath, 'raw/tiki_backup_files/export_tiki_27052024/\"epoch_part*.csv\"')\n",
    "datapath_back = os.path.join(raw_path, \"tiki_backup_files/export_tiki_21052024/\")\n",
    "file_pattern_back_1 = os.path.join(datapath_back,\"epoch_part*.csv\")  # Adjust the path and extension if needed\n",
    "\n",
    "backup_files = glob.glob(file_pattern_back_1)\n",
    "file_list = glob.glob(file_pattern_back_1)\n",
    "file_list.sort()\n",
    "df_backup_small = pd.concat((pd.read_csv(f, encoding=\"latin-1\", low_memory=False) for f in file_list), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup_small[\"start_end\"] = df_backup_small[\"endTimestamp\"] - df_backup_small[\"startTimestamp\"]\n",
    "df_backup_small[\"startTimestamp\"] = pd.to_datetime(df_backup_small[\"startTimestamp\"],unit='ms')\n",
    "df_backup_small[\"endTimestamp\"] = pd.to_datetime(df_backup_small[\"endTimestamp\"],unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup_small[\"customer\"] = df_backup_small.customer.str.split(\"@\").str.get(0)\n",
    "df_backup_small[\"customer\"] = df_backup_small[\"customer\"].str[:4]\n",
    "\n",
    "df_backup_small['startTimestamp'] = df_backup_small['startTimestamp'] + pd.to_timedelta(df_backup_small['timezoneOffset'], unit='m')\n",
    "df_backup_small['endTimestamp'] = df_backup_small['endTimestamp'] + pd.to_timedelta(df_backup_small['timezoneOffset'], unit='m')\n",
    "\n",
    "df_backup_small[\"startTimestamp_day\"] = df_backup_small.startTimestamp.dt.normalize()\n",
    "df_backup_small[\"startTimestamp_hour\"] = df_backup_small.startTimestamp.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the pattern for big backup passive data files\n",
    "file_pattern_back_2 = os.path.join(raw_path, 'tiki_backup_files/tiki_backup_*.csv')\n",
    "\n",
    "# Use glob to find all matching files\n",
    "big_backup_files = glob.glob(file_pattern_back_2)\n",
    "\n",
    "# Define the dtype for columns that are known to be problematic\n",
    "dtype_spec = {\n",
    "    'startTimestamp': 'str',  # Load as string initially\n",
    "    'endTimestamp': 'str'     # Load as string initially\n",
    "}\n",
    "\n",
    "# Create a list to hold all the dataframes\n",
    "all_dfs = []\n",
    "\n",
    "# Loop over the files and read them with date parsing\n",
    "for filename in big_backup_files:\n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        dtype=dtype_spec,  # Load timestamps as strings first\n",
    "        low_memory=False  # Ensure proper memory handling\n",
    "    )\n",
    "    \n",
    "    # Convert the timestamp columns to datetime, ensuring proper parsing of ISO 8601 format\n",
    "    df['starttimestamp'] = pd.to_datetime(df['startTimestamp'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce')\n",
    "    df['endtimestamp'] = pd.to_datetime(df['endTimestamp'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce')\n",
    "\n",
    "    \n",
    "    # Additional processing (e.g., adding date and index columns)\n",
    "    match = re.match(r'tiki_backup_(\\d{4}-\\d{2}-\\d{2})_(\\d+)\\.csv', os.path.basename(filename))\n",
    "    if match:\n",
    "        date_part = match.group(1)  # Extract the date\n",
    "        index_part = int(match.group(2))  # Extract the index\n",
    "        # Add the date and index as new columns to the dataframe\n",
    "        df['date'] = date_part\n",
    "        df['index'] = index_part\n",
    "\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "df_backup_big = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Sort the merged dataframe by 'date' and 'index'\n",
    "\n",
    "# Optionally, drop the 'date' and 'index' columns if they are no longer needed\n",
    "df_backup_big.drop(columns=['date', 'index'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'startTimestamp' and 'endTimestamp' columns to datetime objects\n",
    "df_backup_big['startTimestamp'] = pd.to_datetime(df_backup_big['startTimestamp'], errors='coerce')\n",
    "df_backup_big['endTimestamp'] = pd.to_datetime(df_backup_big['endTimestamp'], errors='coerce')\n",
    "# Convert the 'startTimestamp' and 'endTimestamp' columns to datetime objects\n",
    "df_backup_big['startTimestamp'] = pd.to_datetime(df_backup_big['startTimestamp'], errors='coerce')\n",
    "df_backup_big['endTimestamp'] = pd.to_datetime(df_backup_big['endTimestamp'], errors='coerce')\n",
    "\n",
    "# Adjust for timezone offset\n",
    "df_backup_big['startTimestamp'] = df_backup_big['startTimestamp'] + pd.to_timedelta(df_backup_big['timezoneOffset'], unit='m')\n",
    "df_backup_big['endTimestamp'] = df_backup_big['endTimestamp'] + pd.to_timedelta(df_backup_big['timezoneOffset'], unit='m')\n",
    "\n",
    "\n",
    "# Format the datetime objects to the desired format\n",
    "df_backup_big['startTimestamp'] = df_backup_big['startTimestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_backup_big['endTimestamp'] = df_backup_big['endTimestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_backup_big['startTimestamp'] = pd.to_datetime(df_backup_big['startTimestamp'], errors='coerce')\n",
    "df_backup_big['endTimestamp'] = pd.to_datetime(df_backup_big['endTimestamp'], errors='coerce')\n",
    "\n",
    "df_backup_big[\"startTimestamp_day\"] = df_backup_big.startTimestamp.dt.normalize()\n",
    "df_backup_big[\"startTimestamp_hour\"] = df_backup_big.startTimestamp.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 2 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_timestamp_big = df_backup_big['startTimestamp'].max()\n",
    "\n",
    "# Filter the second dataframe to include only entries after the latest timestamp\n",
    "df_backup_small_filtered = df_backup_small[df_backup_small['startTimestamp'] > latest_timestamp_big]\n",
    "\n",
    "# Concatenate the first dataframe with the filtered second dataframe\n",
    "result_df_final = pd.concat([df_backup_big, df_backup_small_filtered], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort the original DataFrames by startTimestamp\n",
    "df_backup_big_sorted = df_backup_big.sort_values(by='startTimestamp')\n",
    "df_backup_small_filtered_sorted = df_backup_small_filtered.sort_values(by='startTimestamp')\n",
    "\n",
    "# Concatenate the sorted DataFrames\n",
    "result_df_final = pd.concat([df_backup_big_sorted, df_backup_small_filtered_sorted], ignore_index=True)\n",
    "\n",
    "# Optional: Sort the final DataFrame again to ensure everything is in order\n",
    "result_df_final = result_df_final.sort_values(by='startTimestamp').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final[\"customer\"] = result_df_final.customer.str.split(\"@\").str.get(0)\n",
    "result_df_final[\"customer\"] = result_df_final[\"customer\"].str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Minimum timestamp:\", result_df_final['startTimestamp'].min())\n",
    "print(\"Maximum timestamp:\", result_df_final['startTimestamp'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final_merged = result_df_final.merge(df_monitoring_short, on=\"customer\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Minimum timestamp:\", result_df_final_merged['startTimestamp'].min())\n",
    "print(\"Maximum timestamp:\", result_df_final_merged['startTimestamp'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Minimum timestamp:\", result_df_final_merged['startTimestamp'].min())\n",
    "print(\"Maximum timestamp:\", result_df_final_merged['startTimestamp'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_coverage(df, today_day, data_type_groups):\n",
    "    \"\"\"\n",
    "    Calculate the data coverage percentage for each customer, for each specified group of data types.\n",
    "\n",
    "    :param df: DataFrame containing customer data\n",
    "    :param today_day: The current date for calculating potential coverage\n",
    "    :param data_type_groups: A dictionary where keys are group names and values are lists of data types\n",
    "    :param status_col: Column name for the status\n",
    "    :param study_version_col: Column name for the study version\n",
    "    :param ema_base_end_col: Column name for the EMA base end date\n",
    "    :return: DataFrame with additional columns for data coverage percentages\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the date columns are datetime objects\n",
    "    df['startTimestamp_day'] = pd.to_datetime(df['startTimestamp_day'])\n",
    "    df['ema_base_end'] = pd.to_datetime(df['ema_base_end'])\n",
    "\n",
    "    # Find the earliest 'startTimestamp_day' for each customer\n",
    "    earliest_timestamp_per_customer = df.groupby('customer')['startTimestamp_day'].min()\n",
    "\n",
    "    # Map the earliest timestamp back to the original DataFrame\n",
    "    df['earliest_start_day'] = df['customer'].map(earliest_timestamp_per_customer)\n",
    "\n",
    "    # Calculate potential days of coverage from the earliest start day to today\n",
    "    df['potential_days_coverage'] = (today_day - df['earliest_start_day']).dt.days\n",
    "\n",
    "    # Define the condition for adjusting potential days coverage\n",
    "    condition = (\n",
    "        (df['status'] == 'Abgeschlossen') & \n",
    "        (df['study_version'].isin(['Kurz', 'Kurz (Wechsel/Abbruch)']))\n",
    "    )\n",
    "\n",
    "    # Adjust potential days of coverage based on the condition\n",
    "    df['potential_days_coverage'] = np.where(\n",
    "        condition,\n",
    "        (df['ema_base_end'] - df['earliest_start_day']).dt.days,\n",
    "        df['potential_days_coverage']\n",
    "    )\n",
    "\n",
    "    for group_name, data_types in data_type_groups.items():\n",
    "        # Filter for the current group of data types\n",
    "        df_type_group = df[df['type'].isin(data_types)]\n",
    "        \n",
    "        # Count unique days with data for each customer for the current data types\n",
    "        actual_days = df_type_group.groupby('customer')['startTimestamp_day'].nunique()\n",
    "\n",
    "        # Map the actual number of days back to the DataFrame\n",
    "        df[f'{group_name}_actual_days_with_data'] = df['customer'].map(actual_days).fillna(0)\n",
    "\n",
    "        # Calculate data coverage percentage for the current data types\n",
    "        df[f'{group_name}_data_coverage_per'] = (df[f'{group_name}_actual_days_with_data'] / df['potential_days_coverage']) * 100\n",
    "\n",
    "    # Drop intermediary columns if necessary\n",
    "    df.drop(columns=['earliest_start_day'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_groups = {\n",
    "    'GPS': [\"Latitude\"],\n",
    "    # Add more groups as needed\n",
    "    'Activity': [\"Steps\"],\n",
    "    'Sleep': [\"SleepBinary\"],\n",
    "    'Heart_Rate': [\"HeartRate\"]\n",
    "    # Add more groups as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final_merged = calculate_data_coverage(result_df_final_merged, today_day, data_type_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final_merged = result_df_final_merged.drop(columns=['valueType', 'createdAt', 'source', 'trustworthiness', 'medicalGrade', 'generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = [\"booleanValue\", \"stringValue\", \"status\", \"study_version\", \"customer\", \"type\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with -99 for the specified columns\n",
    "for col in object_cols:\n",
    "    result_df_final_merged[col] = result_df_final_merged[col].fillna(-99)\n",
    "\n",
    "# Convert \"booleanValue\" to boolean\n",
    "result_df_final_merged['booleanValue'] = result_df_final_merged['booleanValue'].apply(lambda x: bool(x) if x != -99 else False)\n",
    "\n",
    "# Convert \"stringValue\", \"status\", \"study_version\" to string using StringDtype\n",
    "result_df_final_merged['stringValue'] = result_df_final_merged['stringValue'].astype('string')\n",
    "result_df_final_merged['status'] = result_df_final_merged['status'].astype('string')\n",
    "result_df_final_merged['study_version'] = result_df_final_merged['study_version'].astype('string')\n",
    "result_df_final_merged['customer'] = result_df_final_merged['customer'].astype('string')\n",
    "result_df_final_merged['type'] = result_df_final_merged['type'].astype('string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate memory usage in bytes\n",
    "memory_usage_bytes = result_df_final.memory_usage(deep=True).sum()\n",
    "\n",
    "# Convert to megabytes\n",
    "memory_usage_mb = memory_usage_bytes / (1024 ** 2)\n",
    "\n",
    "# Convert to gigabytes\n",
    "memory_usage_gb = memory_usage_bytes / (1024 ** 3)\n",
    "\n",
    "# Convert to terabytes\n",
    "memory_usage_tb = memory_usage_bytes / (1024 ** 4)\n",
    "\n",
    "print(f\"Memory usage: {memory_usage_bytes} bytes\")\n",
    "print(f\"Memory usage: {memory_usage_mb:.2f} MB\")\n",
    "print(f\"Memory usage: {memory_usage_gb:.2f} GB\")\n",
    "print(f\"Memory usage: {memory_usage_tb:.2f} TB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_path = preprocessed_path + \"backup_data_passive_general.feather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to HDF5 format\n",
    "result_df_final_merged.to_feather(backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final_merged.startTimestamp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final_merged.startTimestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tiki]",
   "language": "python",
   "name": "conda-env-.conda-tiki-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
