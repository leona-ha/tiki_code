{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20ac61a",
   "metadata": {},
   "source": [
    "# 4 Detailed Preprocessing of Passive Data\n",
    "\n",
    "This notebook shows the analysis of situational context using EMA and passive sensing data\n",
    "\n",
    "1. **Load Data**: Load necessary data from pickle files.\n",
    "2. **Preprocess EMA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cd56bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import regex as re\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from config import datapath, preprocessed_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import statistics  # Make sure this is imported\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_context(\"notebook\", rc={\"axes.labelsize\": 14, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': True})\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696265f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backup_path = preprocessed_path + \"backup_data_passive_actual.feather\"\n",
    "df_backup = pd.read_feather(backup_path)\n",
    "\n",
    "with open(preprocessed_path + '/ema_data.pkl', 'rb') as file:\n",
    "    df_ema_framework = pickle.load(file)\n",
    "\n",
    "with open(preprocessed_path + '/ema_content.pkl', 'rb') as file:\n",
    "    df_ema_content = pickle.load(file)  \n",
    "\n",
    "with open(preprocessed_path + '/monitoring_data.pkl', 'rb') as file:\n",
    "    df_monitoring = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba14b26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# Check min. amount of EMA data available to map to passive data\n",
    "\n",
    "timedelta_hours = 2\n",
    "assess = 0\n",
    "\n",
    "#GPS data\n",
    "speed_limit = 1.4\n",
    "max_distance = 150 \n",
    "kms_per_radian = 6371000\n",
    "epsilon = 100/kms_per_radian\n",
    "min_samples = 10\n",
    "min_cluster_size = 20\n",
    "min_nights_obs = 4\n",
    "min_f_home = 0.5\n",
    "\n",
    "# EMA\n",
    "assessment_phase = [0] #1,2\n",
    "min_num_daily = 4\n",
    "min_days_data = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d27fda9-0ca4-4ea8-9504-ef00001d8fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema1 = df_ema_content.loc[df_ema_content.study.isin([24,25])] # first assessment phase\n",
    "df_ema1 = df_ema1.loc[df_ema1[\"n_quest\"] >= min_num_daily]\n",
    "df_ema1[\"n_days_min\"] = df_ema1.groupby(\"customer\")['quest_complete_day'].transform(\"nunique\")\n",
    "df_ema1 = df_ema1.loc[df_ema1.n_days_min >= min_days_data]\n",
    "df_ema1_customers = df_ema1.customer.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20265259",
   "metadata": {},
   "source": [
    "## 1. Prepare passive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "973f3003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_pass_act \u001b[38;5;241m=\u001b[39m df_backup\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:6452\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   6342\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   6343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   6344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6345\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[1;32m   6346\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6450\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[1;32m   6451\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6452\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39mdeep)\n\u001b[1;32m   6453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[1;32m   6454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:653\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    651\u001b[0m     new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m--> 653\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m, deep\u001b[38;5;241m=\u001b[39mdeep)\n\u001b[1;32m    654\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py:540\u001b[0m, in \u001b[0;36mBlock.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    538\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 540\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    541\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:545\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.copy\u001b[0;34m(self, order)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: DatetimeLikeArrayT, order: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatetimeLikeArrayT:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;66;03m# error: Unexpected keyword argument \"order\" for \"copy\"\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m     new_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcopy(order\u001b[38;5;241m=\u001b[39morder)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    546\u001b[0m     new_obj\u001b[38;5;241m.\u001b[39m_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_obj\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_pass_act = df_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02478e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only keep data that were collected during the first assessment phase\n",
    "df_pass_act_base = df_pass_act[df_pass_act.startTimestamp <= df_pass_act.ema_base_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cab1c-c515-4365-a3c1-6efbfd465284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pass_act_base = df_pass_act_base.loc[df_pass_act_base.customer.isin(df_ema1_customers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40577ed8-9cdf-4909-a565-847daca726dd",
   "metadata": {},
   "source": [
    "### 1.1 Calculate GPS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead0d9e-b89c-44d1-ba1d-6e1f3b2e17ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act_loc =df_pass_act_base[df_pass_act_base.type.isin([\"Latitude\", \"Longitude\"])][[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8c7eb-9534-456c-900b-2b6aced14f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_loc = df_pass_act_loc.pivot_table(\n",
    "    index=[\"customer\", \"startTimestamp\"],\n",
    "    columns=\"type\",\n",
    "    values=[\"doubleValue\"],\n",
    "    aggfunc='first'  # Using 'first' since each type should theoretically have only one entry per customer and timestamp\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex in columns\n",
    "df_loc.columns = ['_'.join(col).strip() for col in df_loc.columns.values]\n",
    "\n",
    "df_loc = df_loc.rename_axis(None, axis=1).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "df_loc = df_loc.rename(columns={\n",
    "    'doubleValue_Latitude': 'Latitude',\n",
    "    'doubleValue_Longitude': 'Longitude',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0626794-c047-4348-ba02-ecf73961aef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hdbscan\n",
    "import statistics\n",
    "\n",
    "class HomeClusterExtractor:\n",
    "    def __init__(self, df, speed_limit, max_distance, epsilon, min_samples, min_nights_obs, min_f_home, clustering_method='dbscan', normalize_min_samples=False, min_data_points=10):\n",
    "        self.df = df.copy()\n",
    "        self.speed_limit = speed_limit\n",
    "        self.max_distance = max_distance\n",
    "        self.epsilon = epsilon\n",
    "        self.min_samples = min_samples\n",
    "        self.min_nights_obs = min_nights_obs\n",
    "        self.min_f_home = min_f_home\n",
    "        self.clustering_method = clustering_method\n",
    "        self.normalize_min_samples = normalize_min_samples\n",
    "        self.min_data_points = min_data_points  # Minimum data points threshold\n",
    "\n",
    "        self.df['hour_gps'] = self.df['startTimestamp'].dt.hour\n",
    "        self.df['day_gps'] = self.df['startTimestamp'].dt.date\n",
    "\n",
    "    def calculate_distances_and_speeds(self):\n",
    "        \"\"\"Calculate distances and speeds for each customer.\"\"\"\n",
    "        self.df['distance'], self.df['time_diff'], self.df['speed'] = np.nan, np.nan, np.nan\n",
    "\n",
    "        for customer in self.df['customer'].unique():\n",
    "            mask = self.df['customer'] == customer\n",
    "            customer_data = self.df.loc[mask]\n",
    "\n",
    "            distances = self._calculate_distances(customer_data)\n",
    "            time_diffs = customer_data['startTimestamp'].diff().dt.total_seconds().fillna(0)\n",
    "            speeds = distances / time_diffs.replace(0, np.nan)\n",
    "\n",
    "            self.df.loc[mask, 'distance'] = distances\n",
    "            self.df.loc[mask, 'time_diff'] = time_diffs\n",
    "            self.df.loc[mask, 'speed'] = speeds\n",
    "\n",
    "    def calculate_stationary_and_transition(self):\n",
    "        \"\"\"Determine stationary points and transition status based on speed and distance.\"\"\"\n",
    "        self.df = self.df[self.df['speed'] <= 220 * 1000 / 3600]  # Filter out points with speed > 220 km/h\n",
    "        self.df['stationary'] = (self.df['speed'] < self.speed_limit) & (self.df['distance'] < self.max_distance)\n",
    "        self.df['transition'] = np.where(self.df['stationary'], 0, 1)\n",
    "        return self.df\n",
    "\n",
    "    def _calculate_distances(self, df):\n",
    "        \"\"\"Helper method to calculate distances using haversine formula.\"\"\"\n",
    "        coords = df[['Latitude', 'Longitude']].values\n",
    "        distances = np.array([\n",
    "            self._haversine(coords[i-1][1], coords[i-1][0], coords[i][1], coords[i][0])\n",
    "            for i in range(1, len(coords))\n",
    "        ])\n",
    "        return np.append(distances, 0)\n",
    "\n",
    "    def _haversine(self, lon1, lat1, lon2, lat2):\n",
    "        \"\"\"Haversine formula to calculate distance between two lat/lon points in meters.\"\"\"\n",
    "        R = 6371000\n",
    "        lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        return R * c\n",
    "\n",
    "    def apply_clustering(self, df):\n",
    "        \"\"\"Apply clustering based on the selected method.\"\"\"\n",
    "        df_cleaned = df.dropna(subset=['Longitude', 'Latitude'])\n",
    "\n",
    "        return df_cleaned.groupby('customer').apply(self._apply_clustering_method).reset_index(drop=True)\n",
    "\n",
    "    def _apply_clustering_method(self, df):\n",
    "        \"\"\"Helper method to apply the chosen clustering method.\"\"\"\n",
    "        customer_point_count = len(df)\n",
    "\n",
    "        # Skip clustering for customers with too few points\n",
    "        if customer_point_count < self.min_data_points:\n",
    "            print(f\"Customer {df['customer'].iloc[0]} has too few data points ({customer_point_count}). Skipping clustering.\")\n",
    "            return pd.DataFrame({'cluster': [-1] * customer_point_count}, index=df.index)\n",
    "\n",
    "        # Use normalized min_samples or a default value\n",
    "        if self.normalize_min_samples:\n",
    "            min_samples = max(2, int(customer_point_count * 0.02))  # 3% of points, with a minimum of 2\n",
    "        else:\n",
    "            min_samples = self.min_samples\n",
    "\n",
    "        if self.clustering_method == 'dbscan':\n",
    "            clustering_model = DBSCAN(eps=self.epsilon, min_samples=min_samples, metric=\"haversine\")\n",
    "            cluster_labels = clustering_model.fit_predict(df[['Longitude', 'Latitude']].apply(np.radians))\n",
    "            cluster_labels = cluster_labels.astype(int)\n",
    "            return pd.DataFrame({'cluster': cluster_labels}, index=df.index)\n",
    "        elif self.clustering_method == 'hdbscan':\n",
    "            min_cluster_size = max(2, min(min_samples, customer_point_count))\n",
    "            clustering_model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, metric='haversine')\n",
    "            cluster_labels = clustering_model.fit_predict(df[['Longitude', 'Latitude']].apply(np.radians))\n",
    "            cluster_labels = cluster_labels.astype(int)\n",
    "            return pd.DataFrame({'cluster': cluster_labels}, index=df.index)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid clustering method: {self.clustering_method}\")\n",
    "\n",
    "    def data_quality_check(self):\n",
    "        \"\"\"Filter out customers with insufficient data points.\"\"\"\n",
    "        customer_counts = self.df.groupby('customer').size().reset_index(name='point_count')\n",
    "        valid_customers = customer_counts[customer_counts['point_count'] >= self.min_data_points]['customer']\n",
    "        self.df = self.df[self.df['customer'].isin(valid_customers)]\n",
    "        print(f\"Data quality check: {len(valid_customers)} customers with sufficient data retained.\")\n",
    "        \n",
    "    def find_home_cluster(self, geodata_clusters):\n",
    "        \"\"\"Identify the home cluster based on nighttime data, with fallback to the largest cluster from all data points.\"\"\"\n",
    "\n",
    "        # Filter for night hours\n",
    "        geodata_night = geodata_clusters.loc[\n",
    "            (geodata_clusters['hour_gps'] >= 20) | (geodata_clusters['hour_gps'] <= 7)\n",
    "        ].copy()\n",
    "\n",
    "        # Initialize the 'home' column to None\n",
    "        geodata_clusters['home'] = None\n",
    "\n",
    "        # Time-based home cluster assignment: most frequent cluster at night\n",
    "        if not geodata_night.empty:\n",
    "            # Only exclude noise from home assignment during night hours\n",
    "            valid_clusters_night = geodata_night[geodata_night['cluster'] != -1].copy()\n",
    "\n",
    "            if not valid_clusters_night.empty:\n",
    "                # Calculate the most frequent cluster (mode) per customer at night\n",
    "                valid_clusters_night['home'] = valid_clusters_night.groupby('customer')['cluster'].transform(\n",
    "                    lambda x: statistics.mode(x) if len(x) > 0 else None\n",
    "                )\n",
    "\n",
    "                # Calculate the number of unique nights with observations for each customer\n",
    "                valid_clusters_night['nights_with_obs'] = valid_clusters_night.groupby('customer')['day_gps'].transform('nunique')\n",
    "\n",
    "                # Count the number of points in the identified home cluster (most frequent) per customer\n",
    "                valid_clusters_night['n_home'] = valid_clusters_night.groupby(['customer', 'home'])['day_gps'].transform('size')\n",
    "\n",
    "                # Calculate the total number of night-time points for each customer\n",
    "                valid_clusters_night['night_obs'] = valid_clusters_night.groupby('customer')['day_gps'].transform('size')\n",
    "\n",
    "                # Calculate the fraction of night-time points spent at home\n",
    "                valid_clusters_night['f_home'] = valid_clusters_night['n_home'] / valid_clusters_night['night_obs']\n",
    "\n",
    "                # Apply both conditions: Minimum nights observed and minimum fraction of time spent at home\n",
    "                valid_clusters_night['home'] = valid_clusters_night.apply(\n",
    "                    lambda x: x['home'] if (x['nights_with_obs'] >= self.min_nights_obs) and (x['f_home'] >= self.min_f_home) else None, axis=1\n",
    "                )\n",
    "\n",
    "                # Merge the time-based home assignment back into the main dataframe\n",
    "                home_mapping = valid_clusters_night[['customer', 'home']].drop_duplicates(subset=['customer'])\n",
    "                geodata_clusters = pd.merge(geodata_clusters, home_mapping, on='customer', how='left', suffixes=('', '_temp'))\n",
    "                geodata_clusters['home'] = geodata_clusters['home'].combine_first(geodata_clusters['home_temp'])\n",
    "                geodata_clusters.drop(columns=['home_temp'], inplace=True)\n",
    "\n",
    "        # Fallback: Assign the largest cluster per customer from **all data points** if no home is found\n",
    "        no_home_customers = geodata_clusters.loc[geodata_clusters['home'].isna(), 'customer'].unique()\n",
    "        print(f\"Customers with no home after time-based method: {len(no_home_customers)}\")\n",
    "\n",
    "        if len(no_home_customers) > 0:\n",
    "            # Consider all points (not just night-time) for customers with no home cluster\n",
    "            fallback_home_clusters = (\n",
    "                geodata_clusters[geodata_clusters['customer'].isin(no_home_customers) & (geodata_clusters['cluster'] != -1)]\n",
    "                .groupby(['customer', 'cluster'])\n",
    "                .size()\n",
    "                .reset_index(name='cluster_size')\n",
    "            )\n",
    "\n",
    "            if not fallback_home_clusters.empty:\n",
    "                # Take the largest cluster per customer based on **all data points**\n",
    "                fallback_home_clusters = fallback_home_clusters.loc[\n",
    "                    fallback_home_clusters.groupby('customer')['cluster_size'].idxmax()\n",
    "                ]\n",
    "\n",
    "                # Assign the fallback home clusters\n",
    "                fallback_home_clusters['home'] = fallback_home_clusters['cluster']\n",
    "\n",
    "                # Merge fallback home clusters back to the main dataset\n",
    "                fallback_home_mapping = fallback_home_clusters[['customer', 'home']].drop_duplicates()\n",
    "                geodata_clusters = pd.merge(geodata_clusters, fallback_home_mapping, on='customer', how='left', suffixes=('', '_fallback'))\n",
    "\n",
    "                # Fill any remaining NaNs in the 'home' column with the fallback cluster\n",
    "                geodata_clusters['home'] = geodata_clusters['home'].combine_first(geodata_clusters['home_fallback'])\n",
    "                geodata_clusters.drop(columns=['home_fallback'], inplace=True)\n",
    "                print(f\"Fallback home clusters assigned: {len(fallback_home_clusters)}\")\n",
    "\n",
    "        # For customers that still have no home cluster after the fallback\n",
    "        final_no_home = geodata_clusters.loc[geodata_clusters['home'].isna(), 'customer'].unique()\n",
    "        print(f\"Warning: {len(final_no_home)} customers still do not have a home cluster.\")\n",
    "\n",
    "        # Create homeID by combining customer and home cluster\n",
    "        geodata_clusters['homeID'] = geodata_clusters.apply(\n",
    "            lambda x: f\"{x['customer']}00{int(x['home'])}\" if pd.notna(x['home']) else None, axis=1\n",
    "        )\n",
    "\n",
    "        return geodata_clusters\n",
    "\n",
    "\n",
    "\n",
    "    def determine_if_at_home(self, df):\n",
    "        \"\"\"Determine if a person is at home, handling unclustered points (-1) properly.\"\"\"\n",
    "        \n",
    "        # Convert cluster to integer before creating the clusterID and homeID\n",
    "        df['cluster'] = df['cluster'].astype(int)\n",
    "        df['home'] = df['home'].astype(int, errors='ignore')  # Handle NaNs gracefully\n",
    "\n",
    "        # Create clusterID and homeID, ensuring no decimal points\n",
    "        df['clusterID'] = df.apply(lambda x: f\"{x['customer']}00{int(x['cluster'])}\" if x['cluster'] != -1 else None, axis=1)\n",
    "        df['homeID'] = df.apply(lambda x: f\"{x['customer']}00{int(x['home'])}\" if pd.notna(x['home']) else None, axis=1)\n",
    "\n",
    "        # Check if a person is at home (-1 if no valid cluster/home)\n",
    "        df['at_home'] = df.apply(\n",
    "            lambda x: -1 if x['cluster'] == -1 else (1 if x['clusterID'] == x['homeID'] else 0), axis=1\n",
    "        )\n",
    "        return df\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the full extraction process.\"\"\"\n",
    "        self.data_quality_check()\n",
    "        self.calculate_distances_and_speeds()\n",
    "        self.df = self.calculate_stationary_and_transition()\n",
    "\n",
    "        # Apply clustering based on all data (not just stationary points)\n",
    "        geodata_cluster_df = self.apply_clustering(self.df)\n",
    "\n",
    "        # Merge clustering results back to the original dataframe, including transition points\n",
    "        geodata_clusters = pd.concat([self.df.reset_index(drop=True), geodata_cluster_df[['cluster']]], axis=1)\n",
    "        geodata_clusters['cluster'].fillna(-1, inplace=True)\n",
    "\n",
    "        # Find home cluster with fallback\n",
    "        geodata_clusters = self.find_home_cluster(geodata_clusters)\n",
    "\n",
    "        # Determine if the person is at home\n",
    "        geodata_clusters = self.determine_if_at_home(geodata_clusters)\n",
    "\n",
    "        return geodata_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d652d9d-063d-4a6e-a7d4-11843777c840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check: 166 customers with sufficient data retained.\n",
      "Customers with no home after time-based method: 22\n",
      "Fallback home clusters assigned: 22\n",
      "Warning: 0 customers still do not have a home cluster.\n"
     ]
    }
   ],
   "source": [
    "# Example usage with HDBSCAN and normalized min_samples:\n",
    "extractor = HomeClusterExtractor(df_loc, speed_limit=speed_limit, max_distance=max_distance, epsilon=epsilon, min_samples=min_samples, \n",
    "                                 min_nights_obs = min_nights_obs, min_f_home=min_f_home, clustering_method='dbscan', \n",
    "                                 normalize_min_samples=False, min_data_points=50)\n",
    "result = extractor.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3568e76-c2e5-4e29-8712-17ff2e6074b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.861500091068507"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result.loc[result.cluster ==-1][\"customer\"].count()/result.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9895a55-a53d-4ab1-9b07-1159723e50e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home_clusters_red = result[[\"customer\", \"startTimestamp\", \"at_home\",\"transition\", \"distance\", \"time_diff\", \"speed\" ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596a358",
   "metadata": {},
   "source": [
    "## 2. Prepare EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "133d9e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_udi = df_ema_content[[\"customer\", \"createdAt_day\", \"quest_create\", \"unique_day_id\", \"assess\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "43dbd254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by customer and unique_day_id and calculate the minimum quest_create\n",
    "df_min_quest = df_ema_udi.groupby(['customer', 'unique_day_id'])['quest_create'].min().reset_index()\n",
    "\n",
    "# Rename the column to sensor_block_end\n",
    "df_min_quest.rename(columns={'quest_create': 'sensor_block_end'}, inplace=True)\n",
    "\n",
    "# Merge the minimum quest_create back to the original DataFrame\n",
    "df_ema_udi = pd.merge(df_ema_udi, df_min_quest, on=['customer', 'unique_day_id'], how='left')\n",
    "\n",
    "# Create the sensor_block_start column, which is 2 hours before quest_create\n",
    "df_ema_udi.drop(columns=['quest_create'], inplace=True)\n",
    "df_ema_udi = df_ema_udi.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "79939ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_udi['sensor_block_start'] = df_ema_udi['sensor_block_end'] - pd.Timedelta(hours=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "041705f9-ae62-4345-a71d-afc2f06224d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only include first assessment phase\n",
    "df_ema_udi_base = df_ema_udi.loc[df_ema_udi.assess == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4c98e545-b195-4c99-a137-6c508372c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_udi_test = df_ema_udi_base.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6c3f7312-1c8c-41b7-abb9-38e362cce259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act_test = df_pass_act_base.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5c641-d497-4956-96a2-35a09594aa57",
   "metadata": {},
   "source": [
    "## 3. Merge EMA to passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dd79898b-4f3c-42c9-83aa-5c155f842b67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class EMAMapper:\n",
    "    def __init__(self, df_ema, df_data, df_home_clusters=None):\n",
    "        self.df_ema = df_ema.copy()\n",
    "        self.df_data = df_data.copy()\n",
    "\n",
    "        if df_home_clusters is not None:\n",
    "            self.df_home_clusters = df_home_clusters.copy()\n",
    "            self.df_home_clusters['startTimestamp'] = pd.to_datetime(self.df_home_clusters['startTimestamp'])\n",
    "        else:\n",
    "            self.df_home_clusters = None\n",
    "\n",
    "        self.df_ema['sensor_block_start'] = pd.to_datetime(self.df_ema['sensor_block_start'])\n",
    "        self.df_ema['sensor_block_end'] = pd.to_datetime(self.df_ema['sensor_block_end'])\n",
    "        self.df_data['startTimestamp'] = pd.to_datetime(self.df_data['startTimestamp'])\n",
    "\n",
    "    def map_steps_to_ema(self):\n",
    "        \"\"\"Map steps to EMA blocks.\"\"\"\n",
    "        n_steps_values = []\n",
    "\n",
    "        df_steps = self.df_data[self.df_data['type'] == 'Steps']\n",
    "\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            sensor_block_start = ema_row['sensor_block_start']\n",
    "            sensor_block_end = ema_row['sensor_block_end']\n",
    "\n",
    "            # Use df_steps, not df_filtered, for filtering\n",
    "            df_filtered = df_steps[(df_steps['startTimestamp'] < sensor_block_end) & \n",
    "                                   (df_steps['endTimestamp'] > sensor_block_start)]\n",
    "\n",
    "            if df_filtered.empty:\n",
    "                n_steps_values.append(0)\n",
    "            else:\n",
    "                overlap_start = df_filtered['startTimestamp'].combine(sensor_block_start, max)\n",
    "                overlap_end = df_filtered['endTimestamp'].combine(sensor_block_end, min)\n",
    "\n",
    "                overlap_duration = (overlap_end - overlap_start).dt.total_seconds()\n",
    "                step_duration = (df_filtered['endTimestamp'] - df_filtered['startTimestamp']).dt.total_seconds()\n",
    "\n",
    "                proportion = overlap_duration / step_duration\n",
    "                weighted_value = proportion * df_filtered['doubleValue']\n",
    "\n",
    "                n_steps = weighted_value.sum()\n",
    "                n_steps_values.append(round(n_steps))\n",
    "\n",
    "        self.df_ema['n_steps'] = n_steps_values\n",
    "        return self.df_ema\n",
    "\n",
    "    def map_gps_and_transition_to_ema(self):\n",
    "        \"\"\"\n",
    "        Map GPS, stationary, and transition data to EMA blocks in one process, using the predefined 'sensor_block_start' and 'sensor_block_end'.\n",
    "\n",
    "        Returns:\n",
    "        - df_ema with additional columns: 'n_GPS', 'total_distance_km', 'transition', 'transition_minutes', 'at_home_minute', and 'at_home_binary'.\n",
    "        \"\"\"\n",
    "        if self.df_home_clusters is None:\n",
    "            raise ValueError(\"df_home_clusters is not provided during initialization.\")\n",
    "\n",
    "        gps_counts = []\n",
    "        total_distances = []\n",
    "        transition_values = []\n",
    "        transition_minute_values = []\n",
    "        at_home_minute_values = []\n",
    "        at_home_binary_values = []\n",
    "\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            sensor_block_start = ema_row['sensor_block_start']  # Use predefined block start\n",
    "            sensor_block_end = ema_row['sensor_block_end']      # Use predefined block end\n",
    "            customer = ema_row['customer']\n",
    "\n",
    "            # Filter the home clusters for the current customer and time window\n",
    "            df_filtered = self.df_home_clusters[\n",
    "                (self.df_home_clusters['customer'] == customer) &\n",
    "                (self.df_home_clusters['startTimestamp'] >= sensor_block_start) &\n",
    "                (self.df_home_clusters['startTimestamp'] <= sensor_block_end)\n",
    "            ]\n",
    "\n",
    "            if df_filtered.empty:\n",
    "                # No data for the block, set to default values\n",
    "                gps_counts.append(0)\n",
    "                total_distances.append(0)\n",
    "                transition_values.append(-1)  # No data\n",
    "                transition_minute_values.append(0)\n",
    "                at_home_minute_values.append(0)\n",
    "                at_home_binary_values.append(-1)  # No data\n",
    "            else:\n",
    "                # Count GPS points\n",
    "                gps_count = df_filtered.shape[0]\n",
    "                gps_counts.append(gps_count)\n",
    "\n",
    "                # Calculate total distance (sum of distances)\n",
    "                total_distance = df_filtered['distance'].sum() / 1000  # Convert to kilometers\n",
    "                total_distances.append(total_distance)\n",
    "\n",
    "                # Calculate time differences between consecutive GPS points within the block\n",
    "                df_filtered.loc[:, 'time_diff'] = df_filtered['startTimestamp'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "                # Calculate transition minutes and at_home minutes\n",
    "                transition_minutes = df_filtered[df_filtered['transition'] == 1]['time_diff'].sum() / 60  # Convert seconds to minutes\n",
    "                at_home_minutes = df_filtered[df_filtered['at_home'] == 1]['time_diff'].sum() / 60  # Convert seconds to minutes\n",
    "\n",
    "                transition_minute_values.append(transition_minutes)\n",
    "                at_home_minute_values.append(at_home_minutes)\n",
    "\n",
    "                # Calculate at_home_binary\n",
    "                if df_filtered['at_home'].eq(1).any():\n",
    "                    # At least one GPS point at home\n",
    "                    at_home_binary_values.append(1)\n",
    "                else:\n",
    "                    # There is data, but no valid home cluster, or all points are not at home\n",
    "                    at_home_binary_values.append(0)\n",
    "\n",
    "                # Transition status\n",
    "                if transition_minutes > 0:\n",
    "                    transition_status = 1  # Some transition occurred\n",
    "                else:\n",
    "                    transition_status = 0  # No transition occurred\n",
    "                transition_values.append(transition_status)\n",
    "\n",
    "        # Update the df_ema DataFrame with new columns\n",
    "        self.df_ema['n_GPS'] = gps_counts\n",
    "        self.df_ema['total_distance_km'] = total_distances\n",
    "        self.df_ema['transition'] = transition_values\n",
    "        self.df_ema['transition_minutes'] = transition_minute_values\n",
    "        self.df_ema['at_home_minute'] = at_home_minute_values\n",
    "        self.df_ema['at_home_binary'] = at_home_binary_values\n",
    "\n",
    "        return self.df_ema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cbecc8ab-0a6a-471a-abaf-111b726456be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>createdAt_day</th>\n",
       "      <th>unique_day_id</th>\n",
       "      <th>assess</th>\n",
       "      <th>sensor_block_end</th>\n",
       "      <th>sensor_block_start</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>n_GPS</th>\n",
       "      <th>total_distance_km</th>\n",
       "      <th>transition</th>\n",
       "      <th>transition_minutes</th>\n",
       "      <th>at_home_minute</th>\n",
       "      <th>at_home_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>20230919_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-19 07:31:20.352</td>\n",
       "      <td>2023-09-19 05:31:20.352</td>\n",
       "      <td>295</td>\n",
       "      <td>115</td>\n",
       "      <td>0.445619</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>20230919_6</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-19 17:14:33.463</td>\n",
       "      <td>2023-09-19 15:14:33.463</td>\n",
       "      <td>11126</td>\n",
       "      <td>286</td>\n",
       "      <td>16.757672</td>\n",
       "      <td>1</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-28</td>\n",
       "      <td>20230928_6</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-28 18:29:55.737</td>\n",
       "      <td>2023-09-28 16:29:55.737</td>\n",
       "      <td>12097</td>\n",
       "      <td>2</td>\n",
       "      <td>19.321239</td>\n",
       "      <td>1</td>\n",
       "      <td>35.283333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-25</td>\n",
       "      <td>20230925_6</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-25 17:08:34.592</td>\n",
       "      <td>2023-09-25 15:08:34.592</td>\n",
       "      <td>13281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-22</td>\n",
       "      <td>20230922_6</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-22 17:06:14.166</td>\n",
       "      <td>2023-09-22 15:06:14.166</td>\n",
       "      <td>6328</td>\n",
       "      <td>85</td>\n",
       "      <td>6.815990</td>\n",
       "      <td>1</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer createdAt_day unique_day_id  assess        sensor_block_end  \\\n",
       "0      MYAi    2023-09-19    20230919_1       0 2023-09-19 07:31:20.352   \n",
       "32     MYAi    2023-09-19    20230919_6       0 2023-09-19 17:14:33.463   \n",
       "33     MYAi    2023-09-28    20230928_6       0 2023-09-28 18:29:55.737   \n",
       "35     MYAi    2023-09-25    20230925_6       0 2023-09-25 17:08:34.592   \n",
       "39     MYAi    2023-09-22    20230922_6       0 2023-09-22 17:06:14.166   \n",
       "\n",
       "        sensor_block_start  n_steps  n_GPS  total_distance_km  transition  \\\n",
       "0  2023-09-19 05:31:20.352      295    115           0.445619           0   \n",
       "32 2023-09-19 15:14:33.463    11126    286          16.757672           1   \n",
       "33 2023-09-28 16:29:55.737    12097      2          19.321239           1   \n",
       "35 2023-09-25 15:08:34.592    13281      0           0.000000          -1   \n",
       "39 2023-09-22 15:06:14.166     6328     85           6.815990           1   \n",
       "\n",
       "    transition_minutes  at_home_minute  at_home_binary  \n",
       "0             0.000000           118.0               1  \n",
       "32           18.750000             0.0               0  \n",
       "33           35.283333             0.0               0  \n",
       "35            0.000000             0.0              -1  \n",
       "39           12.200000             0.0               0  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 2: Map EMA data\n",
    "ema_mapper = EMAMapper(df_ema_udi_base, df_pass_act_base, df_home_clusters=home_clusters_red)\n",
    "\n",
    "# Step 3: Map steps, GPS data, and transitions\n",
    "df_ema_with_steps = ema_mapper.map_steps_to_ema()\n",
    "df_ema_with_gps_and_transition = ema_mapper.map_gps_and_transition_to_ema()\n",
    "df_ema_with_at_home = ema_mapper.map_gps_and_transition_to_ema()\n",
    "\n",
    "# Final DataFrame with all the mapped data\n",
    "df_ema_with_at_home.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a15b834-d8ce-49f7-bf59-ccb0e05ff54c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "at_home_binary\n",
       "-1    9783\n",
       " 0    2540\n",
       " 1    4879\n",
       "Name: customer, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ema_with_at_home.groupby(\"at_home_binary\")[\"customer\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0ebe768c-5f50-4137-895f-70573ac68024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transition\n",
       "-1    9783\n",
       " 0    3155\n",
       " 1    4264\n",
       "Name: customer, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ema_with_at_home.groupby(\"transition\")[\"customer\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "70bd9bce-1396-4f95-b7d1-87cacb01281c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17202.000000\n",
       "mean         3.662431\n",
       "std         11.599654\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max        116.833333\n",
       "Name: transition_minutes, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ema_with_at_home.transition_minutes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dad818d2-b04d-4cf6-b10e-f7ff7a33e800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(preprocessed_path + '/map_ema_passive.pkl', 'wb') as file:\n",
    "    pickle.dump(df_ema_with_at_home, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a26233-94bb-4e2b-9dd4-f2a6beb422bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
