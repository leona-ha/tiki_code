{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20ac61a",
   "metadata": {},
   "source": [
    "# 04 Detailed Preprocessing of Passive Data\n",
    "\n",
    "This notebook shows the analysis of situational context using EMA and passive sensing data\n",
    "\n",
    "1. **Load Data**: Load necessary data from pickle files.\n",
    "2. **Preprocess EMA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cd56bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import regex as re\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from config import datapath, preprocessed_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import statistics  # Make sure this is imported\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_context(\"notebook\", rc={\"axes.labelsize\": 14, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': True})\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696265f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backup_path = preprocessed_path + \"backup_data_passive.feather\"\n",
    "df_backup = pd.read_feather(backup_path)\n",
    "\n",
    "with open(preprocessed_path + '/ema_data.pkl', 'rb') as file:\n",
    "    df_ema_framework = pickle.load(file)\n",
    "\n",
    "with open(preprocessed_path + '/ema_content.pkl', 'rb') as file:\n",
    "    df_ema_content = pickle.load(file)  \n",
    "\n",
    "with open(preprocessed_path + '/monitoring_data.pkl', 'rb') as file:\n",
    "    df_monitoring = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba14b26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# Check min. amount of EMA data available to map to passive data\n",
    "\n",
    "timedelta_hours = 2\n",
    "assess = 0\n",
    "\n",
    "#GPS data\n",
    "speed_limit = 1.4\n",
    "max_distance = 150 \n",
    "kms_per_radian = 6371.0088\n",
    "epsilon = 0.05/kms_per_radian\n",
    "min_samples = 30\n",
    "min_nights_obs = 5\n",
    "min_f_home = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20265259",
   "metadata": {},
   "source": [
    "## 1. Prepare passive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973f3003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act = df_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc02478e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only keep data that were collected during the first assessment phase\n",
    "df_pass_act_base = df_pass_act[df_pass_act.startTimestamp <= df_pass_act.ema_base_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40577ed8-9cdf-4909-a565-847daca726dd",
   "metadata": {},
   "source": [
    "### 1.1 Calculate GPS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ead0d9e-b89c-44d1-ba1d-6e1f3b2e17ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act_loc =df_pass_act_base[df_pass_act_base.type.isin([\"Latitude\", \"Longitude\"])][[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e8c7eb-9534-456c-900b-2b6aced14f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_loc = df_pass_act_loc.pivot_table(\n",
    "    index=[\"customer\", \"startTimestamp\"],\n",
    "    columns=\"type\",\n",
    "    values=[\"doubleValue\"],\n",
    "    aggfunc='first'  # Using 'first' since each type should theoretically have only one entry per customer and timestamp\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex in columns\n",
    "df_loc.columns = ['_'.join(col).strip() for col in df_loc.columns.values]\n",
    "\n",
    "df_loc = df_loc.rename_axis(None, axis=1).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "df_loc = df_loc.rename(columns={\n",
    "    'doubleValue_Latitude': 'Latitude',\n",
    "    'doubleValue_Longitude': 'Longitude',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aacc6b05-a40a-4e1a-b200-5f4ebc569259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomeClusterExtractor:\n",
    "    def __init__(self, df, speed_limit, max_distance, epsilon, min_samples, min_nights_obs, min_f_home):\n",
    "        self.df = df.copy()\n",
    "        self.speed_limit = speed_limit\n",
    "        self.max_distance = max_distance\n",
    "        self.epsilon = epsilon\n",
    "        self.min_samples = min_samples\n",
    "        self.min_nights_obs = min_nights_obs\n",
    "        self.min_f_home = min_f_home\n",
    "        \n",
    "        self.df['hour_gps'] = self.df['startTimestamp'].dt.hour\n",
    "        self.df['day_gps'] = self.df['startTimestamp'].dt.date\n",
    "\n",
    "    def calculate_distances_and_speeds(self):\n",
    "        \"\"\"Calculate distances and speeds for each customer.\"\"\"\n",
    "        self.df['distance'], self.df['time_diff'], self.df['speed'] = np.nan, np.nan, np.nan\n",
    "\n",
    "        for customer in self.df['customer'].unique():\n",
    "            mask = self.df['customer'] == customer\n",
    "            customer_data = self.df.loc[mask]\n",
    "\n",
    "            # Calculate distances between consecutive points\n",
    "            distances = self._calculate_distances(customer_data)\n",
    "            time_diffs = customer_data['startTimestamp'].diff().dt.total_seconds().fillna(0)\n",
    "            speeds = distances / time_diffs.replace(0, np.nan)\n",
    "\n",
    "            self.df.loc[mask, 'distance'] = distances\n",
    "            self.df.loc[mask, 'time_diff'] = time_diffs\n",
    "            self.df.loc[mask, 'speed'] = speeds\n",
    "\n",
    "    def _calculate_distances(self, df):\n",
    "        \"\"\"Helper method to calculate distances using haversine formula.\"\"\"\n",
    "        coords = df[['Latitude', 'Longitude']].values\n",
    "        distances = np.array([\n",
    "            self._haversine(coords[i-1][1], coords[i-1][0], coords[i][1], coords[i][0])\n",
    "            for i in range(1, len(coords))\n",
    "        ])\n",
    "        return np.append(distances, 0)  # Append 0 for the last point\n",
    "\n",
    "    def _haversine(self, lon1, lat1, lon2, lat2):\n",
    "        \"\"\"Haversine formula to calculate distance between two lat/lon points in meters.\"\"\"\n",
    "        R = 6371000  # Radius of Earth in meters\n",
    "        phi_1 = np.radians(lat1)\n",
    "        phi_2 = np.radians(lat2)\n",
    "        delta_phi = np.radians(lat2 - lat1)\n",
    "        delta_lambda = np.radians(lon2 - lon1)\n",
    "        a = np.sin(delta_phi/2.0)**2 + np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda/2.0)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        meters = R * c  # Output distance in meters\n",
    "        return meters\n",
    "\n",
    "    def extract_stationary_points(self):\n",
    "        \"\"\"Filter stationary points based on speed and distance.\"\"\"\n",
    "        return self.df[(self.df['speed'] < self.speed_limit) & (self.df['distance'] < self.max_distance)]\n",
    "\n",
    "    def apply_clustering(self, df):\n",
    "        \"\"\"Apply DBSCAN clustering on stationary points.\"\"\"\n",
    "        return df.groupby('customer').apply(self._apply_dbscan).reset_index(drop=True)\n",
    "\n",
    "    def _apply_dbscan(self, df):\n",
    "        \"\"\"Helper method to apply DBSCAN clustering.\"\"\"\n",
    "        clustering_model = DBSCAN(eps=self.epsilon, min_samples=self.min_samples, metric=\"haversine\")\n",
    "        cluster_labels = clustering_model.fit_predict(df[['Longitude', 'Latitude']].apply(np.radians))\n",
    "        return pd.DataFrame({'cluster': cluster_labels}, index=df.index)\n",
    "\n",
    "    def find_home_cluster(self, geodata_clusters):\n",
    "        \"\"\"Identify the home cluster based on nighttime data.\"\"\"\n",
    "        # Filter for night hours\n",
    "        geodata_night = geodata_clusters.loc[\n",
    "            (geodata_clusters['hour_gps'] >= 20) | (geodata_clusters['hour_gps'] <= 7)\n",
    "        ].copy()  # Make an explicit copy to avoid SettingWithCopyWarning\n",
    "\n",
    "        # Filter out any rows with a clusterID of -1\n",
    "        geodata_night = geodata_night[geodata_night['clusterID'] != 'None']\n",
    "\n",
    "        # Calculate the mode of clusterID per user during night hours\n",
    "        geodata_night['home'] = geodata_night.groupby('customer')['clusterID'].transform(\n",
    "            lambda x: statistics.mode(x)\n",
    "        )\n",
    "\n",
    "        # Calculate various metrics to validate the home cluster\n",
    "        geodata_night['nights_with_obs'] = geodata_night.groupby('customer')['day_gps'].transform('nunique')\n",
    "        geodata_night['night_obs'] = geodata_night.groupby('customer')['day_gps'].transform('size')\n",
    "        geodata_night['n_home'] = geodata_night.groupby('customer')['home'].transform(lambda x: x.value_counts().iloc[0])\n",
    "        geodata_night['f_home'] = geodata_night['n_home'] / geodata_night['night_obs']\n",
    "\n",
    "        # Update the 'home' label based on conditions\n",
    "        geodata_night['home'] = geodata_night.apply(\n",
    "            lambda x: x['home'] if x['nights_with_obs'] >= self.min_nights_obs and x['f_home'] > self.min_f_home else None, axis=1)\n",
    "\n",
    "        # Extract a mapping of userID to home cluster\n",
    "        user_home_mapping = geodata_night[['customer', 'home']].drop_duplicates()\n",
    "\n",
    "        # Merging back to the full dataset\n",
    "        return pd.merge(geodata_clusters, user_home_mapping, on='customer', how='left')\n",
    "    \n",
    "    def determine_if_at_home(self, df):\n",
    "        \"\"\"Determine if a person is at home.\"\"\"\n",
    "        # Create a column 'at_home' that compares 'clusterID' with 'home'\n",
    "        df['at_home'] = df.apply(\n",
    "            lambda x: 1 if x['clusterID'] == x['home'] else (0 if pd.notna(x['home']) else -1), axis=1\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the full extraction process.\"\"\"\n",
    "        self.calculate_distances_and_speeds()\n",
    "        stationary_df = self.extract_stationary_points()\n",
    "        geodata_cluster_df = self.apply_clustering(stationary_df)\n",
    "\n",
    "        geodata_clusters = pd.concat([stationary_df.reset_index(drop=True), geodata_cluster_df['cluster']], axis=1)\n",
    "        geodata_clusters = geodata_clusters[geodata_clusters['cluster'] != -1]\n",
    "        geodata_clusters['clusterID'] = geodata_clusters['customer'].astype(str) + '00' + geodata_clusters['cluster'].astype(str)\n",
    "\n",
    "        geodata_clusters = self.find_home_cluster(geodata_clusters)\n",
    "        geodata_clusters = self.determine_if_at_home(geodata_clusters)\n",
    "\n",
    "        return geodata_clusters\n",
    "\n",
    "extractor = HomeClusterExtractor(df_loc, speed_limit=speed_limit, max_distance=max_distance, epsilon=epsilon, min_samples=min_samples, \n",
    "                                 min_nights_obs=min_nights_obs, min_f_home=min_f_home)\n",
    "home_clusters = extractor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9895a55-a53d-4ab1-9b07-1159723e50e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home_clusters_red = home_clusters[[\"customer\", \"startTimestamp\", \"at_home\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596a358",
   "metadata": {},
   "source": [
    "## 2. Prepare EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "133d9e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_udi = df_ema_content[[\"customer\", \"createdAt_day\", \"quest_create\", \"unique_day_id\", \"assess\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43dbd254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by customer and unique_day_id and calculate the minimum quest_create\n",
    "df_min_quest = df_ema_udi.groupby(['customer', 'unique_day_id'])['quest_create'].min().reset_index()\n",
    "\n",
    "# Rename the column to sensor_block_end\n",
    "df_min_quest.rename(columns={'quest_create': 'sensor_block_end'}, inplace=True)\n",
    "\n",
    "# Merge the minimum quest_create back to the original DataFrame\n",
    "df_ema_udi = pd.merge(df_ema_udi, df_min_quest, on=['customer', 'unique_day_id'], how='left')\n",
    "\n",
    "# Create the sensor_block_start column, which is 2 hours before quest_create\n",
    "df_ema_udi.drop(columns=['quest_create'], inplace=True)\n",
    "df_ema_udi = df_ema_udi.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79939ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_udi['sensor_block_start'] = df_ema_udi['sensor_block_end'] - pd.Timedelta(hours=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "041705f9-ae62-4345-a71d-afc2f06224d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only include first assessment phase\n",
    "df_ema_udi_base = df_ema_udi.loc[df_ema_udi.assess == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5c641-d497-4956-96a2-35a09594aa57",
   "metadata": {},
   "source": [
    "## 3. Merge EMA to passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4645a937-492e-4517-afee-2fb30ecbe9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class EMAMapper:\n",
    "    def __init__(self, df_ema, df_data, df_home_clusters=None):\n",
    "        self.df_ema = df_ema.copy()\n",
    "        self.df_data = df_data.copy()\n",
    "\n",
    "        if df_home_clusters is not None:\n",
    "            self.df_home_clusters = df_home_clusters.copy()\n",
    "            self.df_home_clusters['startTimestamp'] = pd.to_datetime(self.df_home_clusters['startTimestamp'])\n",
    "        else:\n",
    "            self.df_home_clusters = None\n",
    "\n",
    "        self.df_ema['sensor_block_start'] = pd.to_datetime(self.df_ema['sensor_block_start'])\n",
    "        self.df_ema['sensor_block_end'] = pd.to_datetime(self.df_ema['sensor_block_end'])\n",
    "        self.df_data['startTimestamp'] = pd.to_datetime(self.df_data['startTimestamp'])\n",
    "        if 'endTimestamp' in self.df_data.columns:\n",
    "            self.df_data['endTimestamp'] = pd.to_datetime(self.df_data['endTimestamp'])\n",
    "\n",
    "        self.outlier_distances = []  # To collect filtered outlier distances\n",
    "\n",
    "    def _haversine(self, lon1, lat1, lon2, lat2):\n",
    "        lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "\n",
    "        a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        r = 6371  # Radius of Earth in kilometers\n",
    "        distance = c * r\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def count_gps_rows_within_blocks_and_calculate_distances(self, max_distance_km=50):\n",
    "        gps_counts = []\n",
    "        total_distances = []\n",
    "\n",
    "        df_gps = self.df_data[self.df_data['type'].isin(['Latitude', 'Longitude'])]\n",
    "        df_gps = df_gps.pivot_table(index=[\"customer\", \"startTimestamp\"], columns=\"type\", values=\"doubleValue\", aggfunc='first').reset_index()\n",
    "\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            sensor_block_start = ema_row['sensor_block_start']\n",
    "            sensor_block_end = ema_row['sensor_block_end']\n",
    "\n",
    "            df_filtered = df_gps[(df_gps['startTimestamp'] >= sensor_block_start) & \n",
    "                                 (df_gps['startTimestamp'] <= sensor_block_end)]\n",
    "\n",
    "            if df_filtered.empty:\n",
    "                gps_counts.append(0)\n",
    "                total_distances.append(0)\n",
    "            else:\n",
    "                df_filtered = df_filtered[(df_filtered['Latitude'] != 0) & (df_filtered['Longitude'] != 0)]\n",
    "                df_filtered = df_filtered.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "                gps_count = df_filtered.shape[0]\n",
    "                gps_counts.append(gps_count)\n",
    "\n",
    "                latitudes = df_filtered['Latitude'].values\n",
    "                longitudes = df_filtered['Longitude'].values\n",
    "\n",
    "                distances = []\n",
    "                for i in range(1, len(latitudes)):\n",
    "                    distance = self._haversine(longitudes[i-1], latitudes[i-1], longitudes[i], latitudes[i])\n",
    "\n",
    "                    # Apply max distance outlier detection\n",
    "                    if distance > max_distance_km:\n",
    "                        self.outlier_distances.append({\n",
    "                            'customer': df_filtered.iloc[i]['customer'],\n",
    "                            'unique_day_id': ema_row['unique_day_id'],\n",
    "                            'segment': i,\n",
    "                            'distance_km': distance,\n",
    "                            'reason': 'Exceeds max distance'\n",
    "                        })\n",
    "                        distances.append(0)  # Set outlier distances to 0\n",
    "                    else:\n",
    "                        distances.append(distance)\n",
    "\n",
    "                total_distance = np.sum(distances)\n",
    "                total_distances.append(total_distance)\n",
    "\n",
    "        self.df_ema['n_GPS'] = gps_counts\n",
    "        self.df_ema['total_distance_km'] = total_distances\n",
    "\n",
    "        return self.df_ema\n",
    "\n",
    "    def map_steps_to_ema(self):\n",
    "        n_steps_values = []\n",
    "\n",
    "        df_steps = self.df_data[self.df_data['type'] == 'Steps']\n",
    "\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            sensor_block_start = ema_row['sensor_block_start']\n",
    "            sensor_block_end = ema_row['sensor_block_end']\n",
    "\n",
    "            df_filtered = df_steps[(df_steps['startTimestamp'] < sensor_block_end) & \n",
    "                                   (df_steps['endTimestamp'] > sensor_block_start)]\n",
    "\n",
    "            if df_filtered.empty:\n",
    "                n_steps_values.append(0)\n",
    "            else:\n",
    "                overlap_start = df_filtered['startTimestamp'].combine(sensor_block_start, max)\n",
    "                overlap_end = df_filtered['endTimestamp'].combine(sensor_block_end, min)\n",
    "\n",
    "                overlap_duration = (overlap_end - overlap_start).dt.total_seconds()\n",
    "                step_duration = (df_filtered['endTimestamp'] - df_filtered['startTimestamp']).dt.total_seconds()\n",
    "\n",
    "                proportion = overlap_duration / step_duration\n",
    "                weighted_value = proportion * df_filtered['doubleValue']\n",
    "\n",
    "                n_steps = weighted_value.sum()\n",
    "                n_steps_values.append(round(n_steps))\n",
    "\n",
    "        self.df_ema['n_steps'] = n_steps_values\n",
    "\n",
    "        return self.df_ema\n",
    "\n",
    "    def map_at_home_to_ema(self):\n",
    "        \"\"\"\n",
    "        Maps the 'at_home' variable to EMA blocks based on whether the startTimestamp from the df_home_clusters falls within the blocks.\n",
    "        \n",
    "        Returns:\n",
    "        - df_ema with an additional column 'at_home' containing the proportion of time spent at home within each block.\n",
    "        \"\"\"\n",
    "        if self.df_home_clusters is None:\n",
    "            raise ValueError(\"df_home_clusters is not provided during initialization.\")\n",
    "\n",
    "        at_home_values = []\n",
    "\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            sensor_block_start = ema_row['sensor_block_start']\n",
    "            sensor_block_end = ema_row['sensor_block_end']\n",
    "            customer = ema_row['customer']\n",
    "\n",
    "            df_filtered = self.df_home_clusters[\n",
    "                (self.df_home_clusters['startTimestamp'] >= sensor_block_start) & \n",
    "                (self.df_home_clusters['startTimestamp'] <= sensor_block_end) & \n",
    "                (self.df_home_clusters['customer'] == customer)\n",
    "            ]\n",
    "\n",
    "            # Calculate the proportion of time at home within the block\n",
    "            if df_filtered.empty:\n",
    "                at_home_values.append(0)\n",
    "            else:\n",
    "                at_home = df_filtered['at_home'].mean()\n",
    "                at_home_values.append(at_home)\n",
    "\n",
    "        self.df_ema['at_home'] = at_home_values\n",
    "\n",
    "        return self.df_ema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbecc8ab-0a6a-471a-abaf-111b726456be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming df_ema, df_data, and df_home_clusters_red are already defined DataFrames\n",
    "ema_mapper = EMAMapper(df_ema_udi_base, df_pass_act_base, df_home_clusters=home_clusters_red)\n",
    "\n",
    "# Mapping steps and GPS data\n",
    "df_ema_with_mapped_values = ema_mapper.map_steps_to_ema()\n",
    "df_ema_with_gps_counts_distances = ema_mapper.count_gps_rows_within_blocks_and_calculate_distances(max_distance_km=50)\n",
    "\n",
    "# Mapping the 'at_home' variable\n",
    "df_ema_with_at_home = ema_mapper.map_at_home_to_ema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dad818d2-b04d-4cf6-b10e-f7ff7a33e800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(preprocessed_path + f'/map_ema_passive.pkl', 'wb') as file:\n",
    "    pickle.dump(df_ema_with_at_home, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a26233-94bb-4e2b-9dd4-f2a6beb422bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
