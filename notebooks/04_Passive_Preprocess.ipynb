{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20ac61a",
   "metadata": {},
   "source": [
    "# 4 Detailed Preprocessing of Passive Data\n",
    "\n",
    "This notebook shows the analysis of situational context using EMA and passive sensing data\n",
    "\n",
    "1. **Load Data**: Load necessary data from pickle files.\n",
    "2. **Preprocess EMA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cd56bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from server_config import datapath, preprocessed_path_freezed, redcap_path, preprocessed_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e875ed9f-525f-4fc9-9c70-6ec815b5f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as mpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5dff914-69d4-4453-90dd-9a43781385a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import EMA_Mapper\n",
    "import gps_features\n",
    "from missing_data import summarize_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3db9590-8d35-49f1-bb1b-643a7bc61e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import statistics  # Make sure this is imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc49635-a266-4367-968a-d6079a67aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress only SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696265f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(preprocessed_path_freezed + '/ema_data.pkl', 'rb') as file:\n",
    "    df_ema_framework = pickle.load(file)\n",
    "\n",
    "with open(preprocessed_path_freezed + '/ema_content.pkl', 'rb') as file:\n",
    "    df_ema_content = pickle.load(file)  \n",
    "\n",
    "with open(preprocessed_path_freezed + '/monitoring_data.pkl', 'rb') as file:\n",
    "    df_monitoring = pickle.load(file)\n",
    "    \n",
    "with open(preprocessed_path_freezed + '/redcap_data.pkl', 'rb') as file:\n",
    "    df_redcap = pickle.load(file)\n",
    "\n",
    "sp1_path = redcap_path + \"/baseline_T5_data_incl_ns_freezed_241120.sav\"\n",
    "df_sp1 = pd.read_spss(sp1_path)\n",
    "freezed_ids = df_sp1.for_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb38507-3245-4c06-ad53-ffebd351c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_path = preprocessed_path_freezed + \"/backup_data_passive_actual.feather\"\n",
    "df_backup = pd.read_feather(backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c61df-fc49-487d-ac3f-05cc83b6ac69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc28df9a-956c-4944-a93f-688159c7609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup_path = preprocessed_path+ \"/backup_data_passive_actual.feather\"\n",
    "#df_backup = pd.read_feather(backup_path)\n",
    "\n",
    "#with open(preprocessed_path + '/ema_data.pkl', 'rb') as file:\n",
    "#    df_ema_framework = pickle.load(file)\n",
    "\n",
    "#with open(preprocessed_path + '/ema_content.pkl', 'rb') as file:\n",
    "#    df_ema_content = pickle.load(file)  \n",
    "\n",
    "#with open(preprocessed_path + '/monitoring_data.pkl', 'rb') as file:\n",
    "#    df_monitoring = pickle.load(file)\n",
    "    \n",
    "#with open(preprocessed_path + '/redcap_data.pkl', 'rb') as file:\n",
    "#    df_redcap = pickle.load(file)\n",
    "\n",
    "#sp1_path = redcap_path + \"/baseline_T5_data_incl_ns_freezed_241120.sav\"\n",
    "#df_sp1 = pd.read_spss(sp1_path)\n",
    "#freezed_ids = df_sp1.for_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba14b26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# Check min. amount of EMA data available to map to passive data\n",
    "\n",
    "#GPS data\n",
    "speed_limit = 1.4\n",
    "max_distance = 150 \n",
    "kms_per_radian = 6371000\n",
    "epsilon = 100/kms_per_radian\n",
    "min_samples = 10\n",
    "min_cluster_size = 20\n",
    "min_nights_obs = 4\n",
    "min_f_home = 0.5\n",
    "\n",
    "# EMA\n",
    "assessment_phase = [0] #1,2\n",
    "min_num_daily = 4\n",
    "min_days_data = 7\n",
    "\n",
    "\n",
    "#Passive to EMA matching\n",
    "timedelta_hours = 2\n",
    "assess = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb0b98-e053-4c56-8035-063d805fa1ee",
   "metadata": {},
   "source": [
    "## Filter for participants with sufficient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e84307a-48e9-4237-a174-ca4aea690c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first assessment phase finished\n",
    "df_ema = df_ema_content.loc[df_ema_content.status.isin([\"Abgeschlossen\", \"Post_Erhebung_1\",\n",
    "                                                             \"Erhebung_2_aktiv\",\"Post_Erhebung_2\", \"Erhebung_3_aktiv\", \"Dropout\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da31c439-6072-4c86-80a5-1531eaa1f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema[\"quest_create_day\"] = df_ema.quest_create.dt.normalize()\n",
    "df_ema[\"quest_create_hour\"] = df_ema.quest_create.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa27524a-88f2-4a4e-bd84-cd2133071ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_cols = [\"assess\", \"study\", \"quest_create\", \"weekend\", \"quest_nr\", \"weekday\", \"season\", \"time_of_day\", \"quest_create_day\", \"quest_create_hour\"]\n",
    "\n",
    "aggregated_info = df_ema.groupby([\"customer\", \"unique_day_id\"])[extra_cols].first().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04638442-f380-4101-bb53-95f1dd8b7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_panas = df_ema.loc[df_ema.quest_title.isin(['panas_fear1', 'panas_fear2', 'panas_guilt1', \n",
    "            'panas_guilt2', 'panas_hostility1', 'panas_hostility2', \n",
    "             'panas_sadness1', 'panas_sadness2'])]\n",
    "\n",
    "# Pivot the table as specified\n",
    "df_piv = df_ema_panas.pivot_table(\n",
    "    index=[\"customer\", \"unique_day_id\", \"assess\"],\n",
    "    columns=\"quest_title\",\n",
    "    values=\"choice_text\",\n",
    "    aggfunc='first'  # Using 'first' since each entry should theoretically be unique per group\n",
    ")\n",
    "\n",
    "# The columns are now a single level Index with just the quest_title values since 'values' is not a list anymore\n",
    "df_piv.columns = [col for col in df_piv.columns.values]\n",
    "\n",
    "# Reset the index to turn the MultiIndex into columns\n",
    "df_piv = df_piv.reset_index()\n",
    "df_piv = df_piv.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "389ab7bf-1318-437e-bc7a-831917b1e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_TAI = df_ema.loc[df_ema.quest_title.isin(['ta_behavioral_2',\n",
    "       'ta_kognitiv', 'ta_kognitiv_2', 'ta_behavioral', 'panas_selfassurance', 'panas_joviality2', 'panas_fatigue',\n",
    "       'panas_joviality1', 'panas_fear1', 'panas_hostility2',\n",
    "       'panas_serenity2', 'panas_shyness', 'panas_hostility1',\n",
    "       'panas_guilt1', 'panas_fear2', 'panas_sadness1', 'panas_guilt2',\n",
    "       'panas_loneliness', 'panas_serenity1', 'panas_sadness2',\n",
    "       'panas_attentiveness', 'er_intensity', 'er_control',\n",
    "       'er_distraction', 'er_reappraisal', 'er_rumination',\n",
    "       'er_relaxation', 'er_suppression', 'er_acceptance'])]\n",
    "\n",
    "# Pivot the table as specified\n",
    "df_piv_tai = df_ema_TAI.pivot_table(\n",
    "    index=[\"customer\", \"unique_day_id\", \"assess\"],\n",
    "    columns=\"quest_title\",\n",
    "    values=\"choice_text\",\n",
    "    aggfunc='first'  # Using 'first' since each entry should theoretically be unique per group\n",
    ")\n",
    "\n",
    "# The columns are now a single level Index with just the quest_title values since 'values' is not a list anymore\n",
    "df_piv_tai.columns = [col for col in df_piv_tai.columns.values]\n",
    "\n",
    "# Reset the index to turn the MultiIndex into columns\n",
    "df_piv_tai = df_piv_tai.reset_index()\n",
    "df_piv_tai = df_piv_tai.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb339672-4426-4112-81a3-772a1aa3a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_piv_tai = pd.merge(aggregated_info, df_piv_tai, on=[\"customer\",\"assess\", \"unique_day_id\"])\n",
    "df_piv = pd.merge(aggregated_info, df_piv, on=[\"customer\",\"assess\", \"unique_day_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d1404c0-9fe6-4378-9b88-3ee3c4e83aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_piv_tai_csv_path = preprocessed_path + '/ema_tai_benni.csv'\n",
    "df_piv_tai.to_csv(df_piv_tai_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b09f59bf-87c1-4e56-ae4e-a91c580e0a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_piv = df_piv.loc[df_piv.study.isin([24,25])] # first assessment phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05f8aed5-4d32-46d6-ae46-0f53ee2e086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "na_scale = ['panas_fear1', 'panas_fear2', 'panas_guilt1', \n",
    "            'panas_guilt2', 'panas_hostility1', 'panas_hostility2', \n",
    "             'panas_sadness1', 'panas_sadness2']\n",
    "\n",
    "# Step 1: Ensure the columns in pa_scale and na_scale are numeric\n",
    "df_piv[na_scale] = df_piv[na_scale].apply(pd.to_numeric, errors='coerce')\n",
    "# Drop rows where any of the na_scale columns have NaN\n",
    "df_piv_clean = df_piv.dropna(subset=na_scale, how='any')\n",
    "\n",
    "# Step 2: Calculate the mean for PA and NA scales per unique_day_id\n",
    "df_piv['mean_na'] = df_piv.groupby(['customer', 'unique_day_id'])[na_scale].transform('mean').mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e918981f-7f63-4c3c-bf76-ea4312810550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique 'unique_day_id' per 'customer' and 'quest_complete_day'\n",
    "df_piv['n_quest'] = df_piv.groupby(['customer', 'quest_create_day'])['unique_day_id'].transform('nunique')\n",
    "\n",
    "# 1) For each customer, count how many UNIQUE days they have n_quest >= min_num_daily\n",
    "df_days_count = (df_piv[df_piv['n_quest'] >= min_num_daily]\n",
    "                 .groupby('customer')['quest_create_day'].nunique()\n",
    "                 .reset_index(name='n_days_min'))\n",
    "\n",
    "# 2) Identify valid customers\n",
    "valid_customers = df_days_count.loc[df_days_count['n_days_min'] >= min_days_data, 'customer']\n",
    "\n",
    "# 3) Filter the original df_piv to keep all rows from valid customers\n",
    "df_piv_filtered = df_piv[df_piv['customer'].isin(valid_customers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab958503-4cc8-48c4-a9ff-4cf047579cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_piv_filtered[\"n_quest_sum\"] = df_piv_filtered['total_unique_ids_per_customer'] = (\n",
    "    df_piv.groupby('customer')['unique_day_id'].transform('nunique')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa6861b1-d04e-4016-a279-42b59676d2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    16614.000000\n",
       "mean        86.022270\n",
       "std         17.866264\n",
       "min         45.000000\n",
       "25%         73.000000\n",
       "50%         88.000000\n",
       "75%        101.000000\n",
       "max        116.000000\n",
       "Name: n_quest_sum, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_piv_filtered[\"n_quest_sum\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20265259",
   "metadata": {},
   "source": [
    "## 1. Prepare passive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "973f3003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act = df_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86831ba8-eb56-441e-abfb-309935f0773e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-12-30 01:15:12.060000')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pass_act.startTimestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc02478e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only keep data that were collected during the first assessment phase\n",
    "df_pass_act_base = df_pass_act[df_pass_act.startTimestamp <= (df_pass_act.ema_base_end + pd.Timedelta(days=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef0cab1c-c515-4365-a3c1-6efbfd465284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pass_act_base = df_pass_act_base.loc[df_pass_act_base.customer.isin(valid_customers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06e8a105-1433-4a86-b475-b3e666184ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pass_act_base.customer.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40577ed8-9cdf-4909-a565-847daca726dd",
   "metadata": {},
   "source": [
    "### 1.1 Calculate GPS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ead0d9e-b89c-44d1-ba1d-6e1f3b2e17ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act_loc =df_pass_act_base[df_pass_act_base.type.isin([\"Latitude\", \"Longitude\"])][[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4e8c7eb-9534-456c-900b-2b6aced14f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_loc = df_pass_act_loc.pivot_table(\n",
    "    index=[\"customer\", \"startTimestamp\"],\n",
    "    columns=\"type\",\n",
    "    values=[\"doubleValue\"],\n",
    "    aggfunc='first'  # Using 'first' since each type should theoretically have only one entry per customer and timestamp\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex in columns\n",
    "df_loc.columns = ['_'.join(col).strip() for col in df_loc.columns.values]\n",
    "\n",
    "df_loc = df_loc.rename_axis(None, axis=1).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "df_loc = df_loc.rename(columns={\n",
    "    'doubleValue_Latitude': 'Latitude',\n",
    "    'doubleValue_Longitude': 'Longitude',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8d3d176-fd67-49df-8ce9-9d9b32784d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loc.customer.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d652d9d-063d-4a6e-a7d4-11843777c840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data quality check: 183 customers with sufficient data retained.\n",
      "INFO:root:Customers with no home after time-based method: 22\n",
      "INFO:root:Fallback home clusters assigned: 22\n",
      "WARNING:root:0 customers still do not have a home cluster.\n"
     ]
    }
   ],
   "source": [
    "# Example usage with HDBSCAN and normalized min_samples:\n",
    "extractor = gps_features.HomeClusterExtractor(df_loc, speed_limit=speed_limit, max_distance=max_distance, epsilon=epsilon, min_samples=min_samples, \n",
    "                                 min_nights_obs = min_nights_obs, min_f_home=min_f_home, clustering_method='dbscan', \n",
    "                                 normalize_min_samples=False, min_data_points=50)\n",
    "result = extractor.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8834db62-be07-4fc3-882e-dfb100d78605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 customers not enough GPS data (i.e. less than 50 data points, so that no home cluster could be computed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9895a55-a53d-4ab1-9b07-1159723e50e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home_clusters_red = result[[\"customer\", \"startTimestamp\", \"at_home\",\"transition\", \"distance\", \"stationary\",\"time_diff\", \"speed\", \"clusterID\", \"homeID\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d21bb74-7c7b-4972-93e4-2b19de543a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_customer_list = home_clusters_red.customer.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596a358",
   "metadata": {},
   "source": [
    "## 2. Prepare EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75454ad6-61e3-483b-a824-032cfc2d8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_udi = df_piv_filtered[[\"customer\", \"quest_create_day\",\"quest_create\", \"unique_day_id\", \"assess\",  \"quest_create_hour\", \"weekday\", \n",
    "                     \"weekend\",\"season\", \"time_of_day\",\"n_quest\",\"mean_na\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edff90ae-e728-4485-9aed-400da4d35455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_udi = df_ema_udi.loc[df_ema_udi.customer.isin(gps_customer_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43dbd254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by customer and unique_day_id and calculate the minimum quest_create\n",
    "df_min_quest = df_ema_udi.groupby(['customer', 'unique_day_id'])['quest_create'].min().reset_index()\n",
    "\n",
    "# Rename the column to sensor_block_end\n",
    "df_min_quest.rename(columns={'quest_create': 'sensor_block_end'}, inplace=True)\n",
    "\n",
    "# Merge the minimum quest_create back to the original DataFrame\n",
    "df_ema_udi = pd.merge(df_ema_udi, df_min_quest, on=['customer', 'unique_day_id'], how='left')\n",
    "\n",
    "# Create the sensor_block_start column, which is 2 hours before quest_create\n",
    "df_ema_udi.drop(columns=['quest_create'], inplace=True)\n",
    "df_ema_udi = df_ema_udi.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79939ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare mapping of passing data by creating blocks\n",
    "\n",
    "df_ema_udi['sensor_block_start'] = df_ema_udi['sensor_block_end'] - pd.Timedelta(hours=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "041705f9-ae62-4345-a71d-afc2f06224d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only include first assessment phase\n",
    "\n",
    "df_ema_udi_base = df_ema_udi.loc[df_ema_udi.assess == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba09797e-588f-411c-949b-f70acb554ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_udi_base = df_ema_udi_base.copy()\n",
    "df_ema_udi_base[\"unique_blocks\"] = df_ema_udi_base.customer + df_ema_udi_base.unique_day_id\n",
    "df_ema_udi_base = df_ema_udi_base.drop_duplicates(subset = [\"customer\", \"unique_blocks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26d54df1-cca4-4821-bcbe-12a935de8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_udi_merged = pd.merge(df_ema_udi_base, df_redcap, on=\"customer\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6adf57d-f203-4120-a1a7-7fc051220210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_udi_merged = df_ema_udi_merged.drop_duplicates(subset = [\"customer\", \"unique_blocks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5c641-d497-4956-96a2-35a09594aa57",
   "metadata": {},
   "source": [
    "## 3. Merge EMA to passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f2b8668-24ae-49e8-add1-e714e167dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'customer' columns are strings and stripped of whitespace\n",
    "df_ema_udi_merged['customer'] = df_ema_udi_merged['customer'].astype(str).str.strip()\n",
    "df_pass_act_base['customer'] = df_pass_act_base['customer'].astype(str).str.strip()\n",
    "home_clusters_red['customer'] = home_clusters_red['customer'].astype(str).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75c91bf6-890b-4822-abaf-9270980be15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_udi_merged_mapper = df_ema_udi_merged[['customer','sensor_block_end', 'sensor_block_start','unique_blocks']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5592e45e-4a08-4bf8-8786-6ee6198b44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pass_act_base_mapper = df_pass_act_base[['customer', 'type', 'startTimestamp', 'endTimestamp', 'doubleValue',\n",
    "       'longValue', 'booleanValue', 'dateValue', 'stringValue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64e57c12-20c1-45af-b112-e4c4ef9d2ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart Rate Data Cleaning Summary:\n",
      "Initial entries: 566882\n",
      "Removed due to non-numeric values: 0\n",
      "Removed due to thresholds: 0\n",
      "Total entries removed: 0\n",
      "Remaining entries: 566882\n",
      "Processing user batch 1/10...\n",
      "Processing user batch 2/10...\n",
      "Processing user batch 3/10...\n",
      "Processing user batch 4/10...\n",
      "Processing user batch 5/10...\n",
      "Processing user batch 6/10...\n",
      "Processing user batch 7/10...\n",
      "Processing user batch 8/10...\n",
      "Processing user batch 9/10...\n",
      "Processing user batch 10/10...\n",
      "Step Data Cleaning Summary:\n",
      "Initial entries: 575100\n",
      "Removed due to non-numeric values: 0\n",
      "Removed due to negative steps: 0\n",
      "Removed due to zero or negative duration: 0\n",
      "Removed due to exceeding steps per minute threshold: 374\n",
      "Remaining entries: 574726\n",
      "Processing user batch 1/10...\n",
      "Processing user batch 2/10...\n",
      "Processing user batch 3/10...\n",
      "Processing user batch 4/10...\n",
      "Processing user batch 5/10...\n",
      "Processing user batch 6/10...\n",
      "Processing user batch 7/10...\n",
      "Processing user batch 8/10...\n",
      "Processing user batch 9/10...\n",
      "Processing user batch 10/10...\n",
      "Processing user batch 1/10...\n",
      "Batch 1 joined shape: (547915, 25)\n",
      "Processing user batch 2/10...\n",
      "Batch 2 joined shape: (582038, 25)\n",
      "Processing user batch 3/10...\n",
      "Batch 3 joined shape: (516690, 25)\n",
      "Processing user batch 4/10...\n",
      "Batch 4 joined shape: (564311, 25)\n",
      "Processing user batch 5/10...\n",
      "Batch 5 joined shape: (475844, 25)\n",
      "Processing user batch 6/10...\n",
      "Batch 6 joined shape: (549325, 25)\n",
      "Processing user batch 7/10...\n",
      "Batch 7 joined shape: (472584, 25)\n",
      "Processing user batch 8/10...\n",
      "Batch 8 joined shape: (494620, 25)\n",
      "Processing user batch 9/10...\n",
      "Batch 9 joined shape: (618678, 25)\n",
      "Processing user batch 10/10...\n",
      "Batch 10 joined shape: (62716, 25)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the EMAMapper\n",
    "ema_mapper = EMA_Mapper.EMAMapper(df_ema_udi_merged_mapper, df_pass_act_base_mapper, home_clusters_red)\n",
    "\n",
    "# Run all mappings\n",
    "ema_mapper.run_mappings()\n",
    "# Retrieve the enriched EMA DataFrame\n",
    "df_ema_enriched = ema_mapper.get_result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98933117-258d-4edf-9e9a-bacc78cb1ed0",
   "metadata": {},
   "source": [
    "### Include weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a10865e-7835-418b-b8a6-730aaacde41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates 52.5483283996582°N 13.407821655273438°E\n",
      "Elevation 46.0 m asl\n",
      "Timezone b'Europe/Berlin' b'GMT+1'\n",
      "Timezone difference to GMT+0 3600 s\n"
     ]
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "\t\"latitude\": 52.5244,\n",
    "\t\"longitude\": 13.4105,\n",
    "\t\"start_date\": \"2023-05-01\",\n",
    "\t\"end_date\": \"2024-12-31\",\n",
    "\t\"daily\": [\"temperature_2m_max\", \"temperature_2m_min\", \"temperature_2m_mean\", \"apparent_temperature_max\", \"apparent_temperature_min\", \"apparent_temperature_mean\", \"sunshine_duration\", \"precipitation_sum\", \"precipitation_hours\"],\n",
    "\t\"timezone\": \"Europe/Berlin\"\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop fo*+r multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"Elevation {response.Elevation()} m asl\")\n",
    "print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "# Process daily data. The order of variables needs to be the same as requested.\n",
    "daily = response.Daily()\n",
    "daily_temperature_2m_max = daily.Variables(0).ValuesAsNumpy()\n",
    "daily_temperature_2m_min = daily.Variables(1).ValuesAsNumpy()\n",
    "daily_temperature_2m_mean = daily.Variables(2).ValuesAsNumpy()\n",
    "daily_apparent_temperature_max = daily.Variables(3).ValuesAsNumpy()\n",
    "daily_apparent_temperature_min = daily.Variables(4).ValuesAsNumpy()\n",
    "daily_apparent_temperature_mean = daily.Variables(5).ValuesAsNumpy()\n",
    "daily_sunshine_duration = daily.Variables(6).ValuesAsNumpy()\n",
    "daily_precipitation_sum = daily.Variables(7).ValuesAsNumpy()\n",
    "daily_precipitation_hours = daily.Variables(8).ValuesAsNumpy()\n",
    "\n",
    "daily_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(daily.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(daily.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = daily.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "\n",
    "daily_data[\"apparent_temperature_mean\"] = daily_apparent_temperature_mean\n",
    "daily_data[\"sunshine_duration\"] = daily_sunshine_duration/3600\n",
    "daily_data[\"precipitation_hours\"] = daily_precipitation_hours\n",
    "\n",
    "daily_dataframe = pd.DataFrame(data = daily_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "787a8a7a-c8e6-4a69-9c92-71e49e7729ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_udi_merged_simple = df_ema_udi_merged[['customer', 'quest_create_day','unique_day_id', 'assess',\n",
    "       'quest_create_hour', 'weekday', 'weekend', 'season', 'time_of_day',\n",
    "       'n_quest', 'mean_na', 'unique_blocks', 'for_id', 'ema_watch','age', 'gender',\n",
    "       'scid_cv_prim_cat', 'marital_status', 'partnership', 'graduation',\n",
    "       'profession', 'ema_start_date', 'years_of_education', 'employability',\n",
    "       'ses', 'ema_smartphone', 'ema_sleep', 'prior_treatment',\n",
    "       'ema_special_event', 'psychotropic', 'somatic_problems',\n",
    "       'gender_description', 'scid_cv_description',\n",
    "       'marital_status_description', 'employability_description',\n",
    "       'employability_description_simple',\n",
    "       'prior_treatment_description_simple', 'graduation_description',\n",
    "       'profession_description', 'prior_treatment_description',\n",
    "       'ema_smartphone_description', 'ema_special_event_description',\n",
    "       'age_description', 'somatic_description', 'psychotropic_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2783207-7a20-46fd-a087-19e9543ed3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_enriched = df_ema_enriched.merge(df_ema_udi_merged_simple, on= [\"customer\", \"unique_blocks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc90716d-eedc-4552-99ce-8965c72d133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_dataframe['date'] = pd.to_datetime(daily_dataframe['date'], utc=True)\n",
    "daily_dataframe['assessment_day'] = daily_dataframe['date'].dt.date\n",
    "df_ema_enriched[\"quest_create_day\"] = pd.to_datetime(df_ema_enriched[\"quest_create_day\"]).dt.date\n",
    "\n",
    "df_ema_weather = pd.merge(\n",
    "    df_ema_enriched,\n",
    "    daily_dataframe,\n",
    "    left_on=\"quest_create_day\",\n",
    "    right_on='assessment_day',\n",
    "    how='left'  # Use 'left' to keep all records from df_ema_enriched\n",
    ")\n",
    "\n",
    "# Drop redundant columns if necessary\n",
    "df_ema_weather.drop(['date', 'assessment_day'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965003ab-539c-47a7-a751-6dd9b08cfcf8",
   "metadata": {},
   "source": [
    "### Analyze missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5f8a709-0d9c-4941-84ce-ee24814685ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customers with missing person-static information\n",
    "\n",
    "missing_static = df_ema_weather[df_ema_weather.age.isna()][[\"customer\",\"for_id\",\"age\"]].customer.unique().tolist()\n",
    "df_ema_weather = df_ema_weather[~df_ema_weather.customer.isin(missing_static)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dbae6da0-fd2e-4fe0-acff-adf0ea0a37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_pa = [\n",
    "    'activity_102_minutes',\n",
    "       'activity_103_minutes', 'activity_104_minutes', 'activity_105_minutes',\n",
    "       'activity_106_minutes', 'activity_107_minutes'\n",
    "]\n",
    "feature_group_hr = ['hr_mean', 'hr_min', 'hr_max', 'hr_std', 'hr_zone_resting', 'hr_zone_moderate',\n",
    "       'hr_zone_vigorous']\n",
    "\n",
    "feature_group_gps = [ 'n_GPS', 'total_distance_km', 'at_home_minute',\n",
    "       'time_in_transition_minutes', 'time_stationary_minutes']\n",
    "\n",
    "feature_group_weather = [\"apparent_temperature_mean\", \"sunshine_duration\", \"precipitation_hours\"]\n",
    "\n",
    "feature_group_person_static = [\n",
    "    'age',\n",
    "    'somatic_description',\n",
    "    'psychotropic_description',\n",
    "    'employability_description_simple',\n",
    "    'prior_treatment_description_simple',\n",
    "    'ema_smartphone_description'\n",
    "]\n",
    "\n",
    "columns_to_check = ['activity_102_minutes',\n",
    "       'activity_103_minutes', 'activity_104_minutes', 'activity_105_minutes',\n",
    "       'activity_106_minutes', 'activity_107_minutes', 'hr_mean', 'hr_min', 'hr_max', 'hr_std', 'hr_median', 'range_heartrate',\n",
    "       'iqr_heartrate', 'skewness_heartrate', 'kurtosis_heartrate','hr_peak_counts', 'hr_zone_resting', 'hr_zone_moderate','hr_zone_vigorous', \n",
    "                    'n_GPS', 'total_distance_km', 'at_home_minute','time_in_transition_minutes', 'time_stationary_minutes', \"apparent_temperature_mean\", \n",
    "                    \"sunshine_duration\", \"precipitation_hours\",'n_steps', 'calories_burned', 'age','somatic_description','psychotropic_description',\n",
    "                    'employability_description_simple', 'prior_treatment_description_simple', 'ema_smartphone_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e279b60-18ff-4cb7-8f50-cc8ea1136bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (14789, 69)\n",
      "Cleaned DataFrame shape: (13243, 69)\n",
      "Number of removed entries: 1546\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to check for missing values\n",
    "cols_to_check = [\n",
    "    'activity_102_minutes', 'activity_103_minutes', 'activity_104_minutes',\n",
    "    'activity_105_minutes', 'activity_106_minutes', 'activity_107_minutes',\n",
    "    'hr_mean', 'hr_min', 'hr_max', 'hr_std', 'hr_zone_resting', 'hr_zone_moderate', 'hr_zone_vigorous',\n",
    "    'n_GPS', 'total_distance_km', 'at_home_minute',\n",
    "    'time_in_transition_minutes', 'time_stationary_minutes'\n",
    "]\n",
    "\n",
    "# Check rows where all the specified columns have the value -1\n",
    "all_negative = (df_ema_weather[cols_to_check] == -1).all(axis=1)\n",
    "\n",
    "# Calculate the number of rows to be removed\n",
    "num_removed = all_negative.sum()\n",
    "\n",
    "# Remove rows where all the specified columns are -1\n",
    "df_ema_weather_cleaned = df_ema_weather[~all_negative].reset_index(drop=True)\n",
    "\n",
    "# Print the shape of the cleaned DataFrame and number of removed entries\n",
    "print(\"Original DataFrame shape:\", df_ema_weather.shape)\n",
    "print(\"Cleaned DataFrame shape:\", df_ema_weather_cleaned.shape)\n",
    "print(\"Number of removed entries:\", num_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24d2e24a-c962-4feb-9405-927872047c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total beeps and condition beeps per participant\n",
    "beep_counts = df_ema_weather_cleaned.groupby('customer').agg(\n",
    "    total_beeps=('unique_blocks', 'count'),\n",
    "    condition_beeps=('at_home_minute', lambda x: ((x == -1) & (df_ema_weather_cleaned.loc[x.index, 'n_steps'] > 625)).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the ratio of condition beeps to total beeps\n",
    "beep_counts['condition_beeps_ratio'] = beep_counts['condition_beeps'] / beep_counts['total_beeps']\n",
    "\n",
    "# Identify participants where condition beeps >50% of total beeps\n",
    "high_condition_participants = beep_counts[beep_counts['condition_beeps_ratio'] > 0.50]['customer']\n",
    "\n",
    "# Remove those participants \n",
    "df_ema_enriched_filtered = df_ema_weather_cleaned[~df_ema_weather_cleaned['customer'].isin(high_condition_participants)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "230f029f-694e-4558-8599-a24da93aad9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ema_enriched_filtered['hr_mean'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f512003c-60ad-46f8-9f96-93d1ef5ce91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing Data Analysis ===\n",
      "\n",
      "---- Person-Static Features Missingness (Based on Unique Customers) ----\n",
      "\n",
      "  age: 0 missing [ 0.00% of unique customers ]\n",
      "  somatic_description: 0 missing [ 0.00% of unique customers ]\n",
      "  psychotropic_description: 0 missing [ 0.00% of unique customers ]\n",
      "  employability_description_simple: 0 missing [ 0.00% of unique customers ]\n",
      "  prior_treatment_description_simple: 0 missing [ 0.00% of unique customers ]\n",
      "  ema_smartphone_description: 0 missing [ 0.00% of unique customers ]\n",
      "\n",
      "---- Group-wise Missing Data Summary ----\n",
      "\n",
      "Group: PA (contains 6 column(s))\n",
      "--------------------------------------------------\n",
      "  activity_102_minutes: 6291 missing (-1)  [49.32% of total rows]\n",
      "  activity_103_minutes: 6291 missing (-1)  [49.32% of total rows]\n",
      "  activity_104_minutes: 6291 missing (-1)  [49.32% of total rows]\n",
      "  activity_105_minutes: 6291 missing (-1)  [49.32% of total rows]\n",
      "  activity_106_minutes: 6291 missing (-1)  [49.32% of total rows]\n",
      "  activity_107_minutes: 6291 missing (-1)  [49.32% of total rows]\n",
      "\n",
      "Group: GPS (contains 5 column(s))\n",
      "--------------------------------------------------\n",
      "  n_GPS: 4043 missing (-1)  [31.69% of total rows]\n",
      "  total_distance_km: 4043 missing (-1)  [31.69% of total rows]\n",
      "  at_home_minute: 4043 missing (-1)  [31.69% of total rows]\n",
      "  time_in_transition_minutes: 4043 missing (-1)  [31.69% of total rows]\n",
      "  time_stationary_minutes: 4043 missing (-1)  [31.69% of total rows]\n",
      "\n",
      "Group: HR (contains 7 column(s))\n",
      "--------------------------------------------------\n",
      "  hr_mean: 462 missing (-1)  [3.62% of total rows]\n",
      "  hr_min: 462 missing (-1)  [3.62% of total rows]\n",
      "  hr_max: 462 missing (-1)  [3.62% of total rows]\n",
      "  hr_std: 462 missing (-1)  [3.62% of total rows]\n",
      "  hr_zone_resting: 462 missing (-1)  [3.62% of total rows]\n",
      "  hr_zone_moderate: 462 missing (-1)  [3.62% of total rows]\n",
      "  hr_zone_vigorous: 462 missing (-1)  [3.62% of total rows]\n",
      "\n",
      "Group: Weather (contains 3 column(s))\n",
      "--------------------------------------------------\n",
      "  apparent_temperature_mean: 0 missing (-1)  [0.00% of total rows]\n",
      "  sunshine_duration: 0 missing (-1)  [0.00% of total rows]\n",
      "  precipitation_hours: 0 missing (-1)  [0.00% of total rows]\n",
      "\n",
      "Group: Person_Static (contains 1 column(s))\n",
      "--------------------------------------------------\n",
      "  Person_Static: 0 missing (any person-static feature)  [0.00% of unique customers]\n",
      "\n",
      "Group: n_steps (contains 1 column(s))\n",
      "--------------------------------------------------\n",
      "  n_steps: 2323 missing (-1)  [18.21% of total rows]\n",
      "\n",
      "=== End of Missing Data Analysis ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_missing_df = summarize_missing_data(\n",
    "    df=df_ema_enriched_filtered,\n",
    "    feature_group_pa=feature_group_pa,\n",
    "    feature_group_gps=feature_group_gps,\n",
    "    feature_group_hr=feature_group_hr,\n",
    "    feature_group_weather = feature_group_weather,\n",
    "    feature_group_person_static = feature_group_person_static,\n",
    "    columns_to_check=columns_to_check,\n",
    "    customer_id_col = \"customer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96e9a7-98ef-4124-a035-9e267c426b71",
   "metadata": {},
   "source": [
    "### GPS condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b15e5a8-83d8-4965-90d5-0e05b80b68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conditions\n",
    "conditions_home = [\n",
    "    (df_ema_enriched_filtered['at_home_minute'] == -1) & (df_ema_enriched_filtered['n_steps'] > 625),\n",
    "    (df_ema_enriched_filtered['at_home_minute'] == -1) & (df_ema_enriched_filtered['n_steps'] <= 625),\n",
    "    (df_ema_enriched_filtered['at_home_minute'] != -1)\n",
    "]\n",
    "\n",
    "conditions = [\n",
    "    (df_ema_enriched_filtered['n_GPS'] == -1) & (df_ema_enriched_filtered['n_steps'] > 625),\n",
    "    (df_ema_enriched_filtered['n_GPS'] == -1) & (df_ema_enriched_filtered['n_steps'] <= 625),\n",
    "    (df_ema_enriched_filtered['n_GPS'] != -1)\n",
    "]\n",
    "# Define the corresponding choices\n",
    "choices = [\n",
    "    'Steps>625',\n",
    "    'Steps<=625',\n",
    "    'GPS_present'\n",
    "]\n",
    "\n",
    "# Create the categorical column\n",
    "df_ema_enriched_filtered['missing_GPS_home'] = np.select(conditions_home, choices, default='Unknown')\n",
    "df_ema_enriched_filtered['missing_GPS'] = np.select(conditions, choices, default='Unknown')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e902da3-6c77-491f-9306-718d73e6237e",
   "metadata": {},
   "source": [
    "### Steps condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d6b7415-7340-4be1-aded-b351ae30fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the condition for all specified columns being -1\n",
    "cols_to_check = [\n",
    "    'hr_mean', \n",
    "    'activity_102_minutes', \n",
    "    'activity_103_minutes', \n",
    "    'activity_104_minutes', \n",
    "    'activity_105_minutes', \n",
    "    'activity_106_minutes', \n",
    "    'activity_107_minutes'\n",
    "]\n",
    "\n",
    "all_negative = (df_ema_enriched_filtered[cols_to_check] == -1).all(axis=1)\n",
    "\n",
    "# Define the conditions for the new \"missing_steps\" column\n",
    "conditions_steps = [\n",
    "    # Condition: n_steps is -1 and all other specified columns are -1\n",
    "    (df_ema_enriched_filtered['n_steps'] == -1) & all_negative,\n",
    "    # Condition: n_steps is -1 but at least one of the specified columns is not -1\n",
    "    (df_ema_enriched_filtered['n_steps'] == -1) & (~all_negative),\n",
    "    # Condition: n_steps is not -1\n",
    "    (df_ema_enriched_filtered['n_steps'] != -1)\n",
    "]\n",
    "\n",
    "# Define the corresponding choices\n",
    "choices_steps = ['step_missing', 'step_zero', 'not_missing']\n",
    "\n",
    "# Create the \"missing_steps\" column\n",
    "df_ema_enriched_filtered['missing_steps'] = np.select(conditions_steps, choices_steps, default='Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30b7602b-d1cf-4dd1-87ee-003ee1fa14a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "missing_steps\n",
       "not_missing     10433\n",
       "step_missing      263\n",
       "step_zero        2060\n",
       "Name: customer, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ema_enriched_filtered.groupby('missing_steps')['customer'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f983d6-7784-4bc5-9bc9-e8d53a839b37",
   "metadata": {},
   "source": [
    "### PA condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4aaf8983-90fb-4b81-b792-5b37ee21f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conditions for the new \"missing_pa\" column\n",
    "\n",
    "conditions_pa = [\n",
    "    # 1. If 'activity_102_minutes' is NOT -1, mark as \"not_missing\"\n",
    "    (df_ema_enriched_filtered['activity_102_minutes'] != -1),\n",
    "    \n",
    "    # 2. If 'activity_102_minutes' is -1 and both 'n_steps' and 'hr_mean' are -1, mark as \"pa_missing\"\n",
    "    (df_ema_enriched_filtered['activity_102_minutes'] == -1) &\n",
    "    ((df_ema_enriched_filtered['n_steps'] == -1) & (df_ema_enriched_filtered['hr_mean'] == -1)),\n",
    "    \n",
    "    # 3. If 'activity_102_minutes' is -1 and at least one of 'n_steps' or 'hr_mean' is not -1, mark as \"pa_zero\"\n",
    "    (df_ema_enriched_filtered['activity_102_minutes'] == -1) &\n",
    "    (((df_ema_enriched_filtered['n_steps'] != -1) | (df_ema_enriched_filtered['hr_mean'] != -1)))\n",
    "]\n",
    "\n",
    "# Define the corresponding choices for each condition\n",
    "choices_pa = ['not_missing', 'pa_missing', 'pa_zero']\n",
    "\n",
    "# Create the \"missing_pa\" column using np.select\n",
    "df_ema_enriched_filtered['missing_pa'] = np.select(conditions_pa, choices_pa, default='Unknown')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8487df7-5b30-4ccb-b138-065abf0e9ebb",
   "metadata": {},
   "source": [
    "### HR condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f02b9e1-054e-4eed-9576-76ec8f0d713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the condition for all specified columns being -1\n",
    "cols_to_check = [\n",
    "    'n_steps', \n",
    "    'activity_102_minutes', \n",
    "    'activity_103_minutes', \n",
    "    'activity_104_minutes', \n",
    "    'activity_105_minutes', \n",
    "    'activity_106_minutes', \n",
    "    'activity_107_minutes'\n",
    "]\n",
    "\n",
    "all_negative = (df_ema_enriched_filtered[cols_to_check] == -1).all(axis=1)\n",
    "\n",
    "# Define the conditions for the new \"missing_steps\" column\n",
    "conditions_steps = [\n",
    "    # Condition: hr is -1 and all other specified columns are -1\n",
    "    (df_ema_enriched_filtered['hr_mean'] == -1) & all_negative,\n",
    "    # Condition: hr is -1 but at least one of the specified columns is not -1\n",
    "    (df_ema_enriched_filtered['hr_mean'] == -1) & (~all_negative),\n",
    "    # Condition: hr is not -1\n",
    "    (df_ema_enriched_filtered['hr_mean'] != -1)\n",
    "]\n",
    "\n",
    "# Define the corresponding choices\n",
    "choices_steps = ['hr_missing', 'hr_missing', 'not_missing']\n",
    "\n",
    "# Create the \"missing_steps\" column\n",
    "df_ema_enriched_filtered['missing_hr'] = np.select(conditions_steps, choices_steps, default='Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03ff5456-d362-48a7-ba19-61fd5a3dc7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer', 'sensor_block_end', 'sensor_block_start', 'unique_blocks',\n",
       "       'hr_mean', 'hr_min', 'hr_max', 'hr_std', 'hr_zone_resting',\n",
       "       'hr_zone_moderate', 'hr_zone_vigorous', 'n_steps', 'n_GPS',\n",
       "       'total_distance_km', 'at_home_minute', 'time_in_transition_minutes',\n",
       "       'time_stationary_minutes', 'activity_102_minutes',\n",
       "       'activity_103_minutes', 'activity_104_minutes', 'activity_105_minutes',\n",
       "       'activity_106_minutes', 'activity_107_minutes', 'quest_create_day',\n",
       "       'unique_day_id', 'assess', 'quest_create_hour', 'weekday', 'weekend',\n",
       "       'season', 'time_of_day', 'n_quest', 'mean_na', 'for_id', 'ema_watch',\n",
       "       'age', 'gender', 'scid_cv_prim_cat', 'marital_status', 'partnership',\n",
       "       'graduation', 'profession', 'ema_start_date', 'years_of_education',\n",
       "       'employability', 'ses', 'ema_smartphone', 'ema_sleep',\n",
       "       'prior_treatment', 'ema_special_event', 'psychotropic',\n",
       "       'somatic_problems', 'gender_description', 'scid_cv_description',\n",
       "       'marital_status_description', 'employability_description',\n",
       "       'employability_description_simple',\n",
       "       'prior_treatment_description_simple', 'graduation_description',\n",
       "       'profession_description', 'prior_treatment_description',\n",
       "       'ema_smartphone_description', 'ema_special_event_description',\n",
       "       'age_description', 'somatic_description', 'psychotropic_description',\n",
       "       'apparent_temperature_mean', 'sunshine_duration', 'precipitation_hours',\n",
       "       'Person_Static', 'missing_GPS_home', 'missing_GPS', 'missing_steps',\n",
       "       'missing_pa', 'missing_hr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ema_enriched_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c94eb2b3-fcc8-48da-8d1f-2cd84f74cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique 'unique_day_id' per 'customer' and 'quest_complete_day'\n",
    "df_ema_enriched_filtered['n_quest'] = df_ema_enriched_filtered.groupby(['customer', 'quest_create_day'])['unique_day_id'].transform('nunique')\n",
    "\n",
    "# 1) For each customer, count how many UNIQUE days they have n_quest >= min_num_daily\n",
    "df_days_count = (df_ema_enriched_filtered[df_ema_enriched_filtered['n_quest'] >= min_num_daily]\n",
    "                 .groupby('customer')['quest_create_day'].nunique()\n",
    "                 .reset_index(name='n_days_min'))\n",
    "\n",
    "# 2) Identify valid customers\n",
    "valid_customers = df_days_count.loc[df_days_count['n_days_min'] >= min_days_data, 'customer']\n",
    "\n",
    "# 3) Filter the original df_piv to keep all rows from valid customers\n",
    "df_ema_enriched_filtered = df_ema_enriched_filtered[df_ema_enriched_filtered['customer'].isin(valid_customers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d48a4f4-7b6c-47c5-8c6f-9f48c0d7e56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ema_enriched_filtered.customer.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c08e105-0342-4091-8737-c2c95d8bc834",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(preprocessed_path_freezed + f'/map_ema_passive.pkl', 'wb') as file:\n",
    "    pickle.dump(df_ema_enriched_filtered, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5392dc5-f496-4fa0-8adb-c1a37355c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(preprocessed_path + f'/map_ema_passive.pkl', 'wb') as file:\n",
    "    pickle.dump(df_ema_enriched_filtered, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c2d644-266c-4e59-b67f-31d85f6e558c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39-env)",
   "language": "python",
   "name": "py39-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
