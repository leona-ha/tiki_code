{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20ac61a",
   "metadata": {},
   "source": [
    "# 04 Detailed Preprocessing of Passive Data\n",
    "\n",
    "This notebook shows the analysis of situational context using EMA and passive sensing data\n",
    "\n",
    "1. **Load Data**: Load necessary data from pickle files.\n",
    "2. **Preprocess EMA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cd56bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import regex as re\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from config import datapath, preprocessed_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import statistics  # Make sure this is imported\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_context(\"notebook\", rc={\"axes.labelsize\": 14, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': True})\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696265f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/Leonas_SSD/tiki_data/preprocessed/backup_data_passive.feather'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m backup_path \u001b[38;5;241m=\u001b[39m preprocessed_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackup_data_passive.feather\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df_backup \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_feather(backup_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(preprocessed_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/ema_data.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     df_ema_framework \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/feather_format.py:144\u001b[0m, in \u001b[0;36mread_feather\u001b[0;34m(path, columns, use_threads, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feather\n\u001b[1;32m    142\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    145\u001b[0m     path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    146\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m feather\u001b[38;5;241m.\u001b[39mread_feather(\n\u001b[1;32m    149\u001b[0m             handles\u001b[38;5;241m.\u001b[39mhandle, columns\u001b[38;5;241m=\u001b[39mcolumns, use_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(use_threads)\n\u001b[1;32m    150\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/Leonas_SSD/tiki_data/preprocessed/backup_data_passive.feather'"
     ]
    }
   ],
   "source": [
    "backup_path = preprocessed_path + \"backup_data_passive.feather\"\n",
    "df_backup = pd.read_feather(backup_path)\n",
    "\n",
    "with open(preprocessed_path + '/ema_data.pkl', 'rb') as file:\n",
    "    df_ema_framework = pickle.load(file)\n",
    "\n",
    "with open(preprocessed_path + '/ema_content.pkl', 'rb') as file:\n",
    "    df_ema_content = pickle.load(file)  \n",
    "\n",
    "with open(preprocessed_path + '/monitoring_data.pkl', 'rb') as file:\n",
    "    df_monitoring = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14b26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# Check min. amount of EMA data available to map to passive data\n",
    "\n",
    "timedelta_hours = 2\n",
    "assess = 0\n",
    "\n",
    "#GPS data\n",
    "speed_limit = 1.4\n",
    "max_distance = 150 \n",
    "kms_per_radian = 6371.0088\n",
    "epsilon = 0.03/kms_per_radian\n",
    "min_samples = 30\n",
    "min_nights_obs = 7\n",
    "min_f_home = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e10f3-e00f-4fe3-82ce-80cb460f3d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_backup.startTimestamp.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20265259",
   "metadata": {},
   "source": [
    "## 1. Prepare passive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f3003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act = df_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02478e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only keep data that were collected during the first assessment phase\n",
    "df_pass_act_base = df_pass_act[df_pass_act.startTimestamp <= df_pass_act.ema_base_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5533c5-a4ad-4e80-b734-7508e0ec4bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act.loc[df_pass_act.customer == \"4MLe\"][\"startTimestamp\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40577ed8-9cdf-4909-a565-847daca726dd",
   "metadata": {},
   "source": [
    "### 1.1 Calculate GPS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead0d9e-b89c-44d1-ba1d-6e1f3b2e17ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act_loc =df_pass_act_base[df_pass_act_base.type.isin([\"Latitude\", \"Longitude\"])][[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8c7eb-9534-456c-900b-2b6aced14f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_loc = df_pass_act_loc.pivot_table(\n",
    "    index=[\"customer\", \"startTimestamp\"],\n",
    "    columns=\"type\",\n",
    "    values=[\"doubleValue\"],\n",
    "    aggfunc='first'  # Using 'first' since each type should theoretically have only one entry per customer and timestamp\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex in columns\n",
    "df_loc.columns = ['_'.join(col).strip() for col in df_loc.columns.values]\n",
    "\n",
    "df_loc = df_loc.rename_axis(None, axis=1).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "df_loc = df_loc.rename(columns={\n",
    "    'doubleValue_Latitude': 'Latitude',\n",
    "    'doubleValue_Longitude': 'Longitude',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc6b05-a40a-4e1a-b200-5f4ebc569259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomeClusterExtractor:\n",
    "    def __init__(self, df, speed_limit, max_distance, epsilon, min_samples, min_nights_obs, min_f_home):\n",
    "        self.df = df.copy()\n",
    "        self.speed_limit = speed_limit\n",
    "        self.max_distance = max_distance\n",
    "        self.epsilon = epsilon\n",
    "        self.min_samples = min_samples\n",
    "        self.min_nights_obs = min_nights_obs\n",
    "        self.min_f_home = min_f_home\n",
    "        \n",
    "        self.df['hour_gps'] = self.df['startTimestamp'].dt.hour\n",
    "        self.df['day_gps'] = self.df['startTimestamp'].dt.date\n",
    "\n",
    "    def calculate_distances_and_speeds(self):\n",
    "        \"\"\"Calculate distances and speeds for each customer.\"\"\"\n",
    "        self.df['distance'], self.df['time_diff'], self.df['speed'] = np.nan, np.nan, np.nan\n",
    "\n",
    "        for customer in self.df['customer'].unique():\n",
    "            mask = self.df['customer'] == customer\n",
    "            customer_data = self.df.loc[mask]\n",
    "\n",
    "            # Calculate distances between consecutive points\n",
    "            distances = self._calculate_distances(customer_data)\n",
    "            time_diffs = customer_data['startTimestamp'].diff().dt.total_seconds().fillna(0)\n",
    "            speeds = distances / time_diffs.replace(0, np.nan)\n",
    "\n",
    "            self.df.loc[mask, 'distance'] = distances\n",
    "            self.df.loc[mask, 'time_diff'] = time_diffs\n",
    "            self.df.loc[mask, 'speed'] = speeds\n",
    "\n",
    "    def _calculate_distances(self, df):\n",
    "        \"\"\"Helper method to calculate distances using haversine formula.\"\"\"\n",
    "        coords = df[['Latitude', 'Longitude']].values\n",
    "        distances = np.array([\n",
    "            self._haversine(coords[i-1][1], coords[i-1][0], coords[i][1], coords[i][0])\n",
    "            for i in range(1, len(coords))\n",
    "        ])\n",
    "        return np.append(distances, 0)  # Append 0 for the last point\n",
    "\n",
    "    def _haversine(self, lon1, lat1, lon2, lat2):\n",
    "        \"\"\"Haversine formula to calculate distance between two lat/lon points in meters.\"\"\"\n",
    "        R = 6371000  # Radius of Earth in meters\n",
    "        phi_1 = np.radians(lat1)\n",
    "        phi_2 = np.radians(lat2)\n",
    "        delta_phi = np.radians(lat2 - lat1)\n",
    "        delta_lambda = np.radians(lon2 - lon1)\n",
    "        a = np.sin(delta_phi/2.0)**2 + np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda/2.0)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        meters = R * c  # Output distance in meters\n",
    "        return meters\n",
    "\n",
    "    def extract_stationary_points(self):\n",
    "        \"\"\"Filter stationary points based on speed and distance.\"\"\"\n",
    "        return self.df[(self.df['speed'] < self.speed_limit) & (self.df['distance'] < self.max_distance)]\n",
    "\n",
    "    def apply_clustering(self, df):\n",
    "        \"\"\"Apply DBSCAN clustering on stationary points.\"\"\"\n",
    "        return df.groupby('customer').apply(self._apply_dbscan).reset_index(drop=True)\n",
    "\n",
    "    def _apply_dbscan(self, df):\n",
    "        \"\"\"Helper method to apply DBSCAN clustering.\"\"\"\n",
    "        clustering_model = DBSCAN(eps=self.epsilon, min_samples=self.min_samples, metric=\"haversine\")\n",
    "        cluster_labels = clustering_model.fit_predict(df[['Longitude', 'Latitude']].apply(np.radians))\n",
    "        return pd.DataFrame({'cluster_100m': cluster_labels}, index=df.index)\n",
    "\n",
    "    def find_home_cluster(self, geodata_clusters):\n",
    "        \"\"\"Identify the home cluster based on nighttime data.\"\"\"\n",
    "        # Filter for night hours\n",
    "        geodata_night = geodata_clusters.loc[\n",
    "            (geodata_clusters['hour_gps'] >= 22) | (geodata_clusters['hour_gps'] <= 6)\n",
    "        ].copy()  # Make an explicit copy to avoid SettingWithCopyWarning\n",
    "\n",
    "        # Calculate the mode of clusterID per user during night hours\n",
    "        geodata_night['home'] = geodata_night.groupby('customer')['clusterID'].transform(\n",
    "            lambda x: statistics.mode(x)\n",
    "        )\n",
    "\n",
    "        # Calculate various metrics to validate the home cluster\n",
    "        geodata_night['nights_with_obs'] = geodata_night.groupby('customer')['day_gps'].transform('nunique')\n",
    "        geodata_night['night_obs'] = geodata_night.groupby('customer')['day_gps'].transform('size')\n",
    "        geodata_night['n_home'] = geodata_night.groupby('customer')['home'].transform(lambda x: x.value_counts().iloc[0])\n",
    "        geodata_night['f_home'] = geodata_night['n_home'] / geodata_night['night_obs']\n",
    "\n",
    "        # Update the 'home' label based on conditions\n",
    "        geodata_night['home'] = geodata_night.apply(\n",
    "            lambda x: x['home'] if x['nights_with_obs'] >= self.min_nights_obs and x['f_home'] > self.min_f_home else None, axis=1\n",
    "        )\n",
    "\n",
    "        # Extract a mapping of userID to home cluster\n",
    "        user_home_mapping = geodata_night[['customer', 'home']].drop_duplicates()\n",
    "\n",
    "        # Merging back to the full dataset\n",
    "        return pd.merge(geodata_clusters, user_home_mapping, on='customer', how='left')\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the full extraction process.\"\"\"\n",
    "        self.calculate_distances_and_speeds()\n",
    "        stationary_df = self.extract_stationary_points()\n",
    "        geodata_cluster_df = self.apply_clustering(stationary_df)\n",
    "\n",
    "        geodata_clusters = pd.concat([stationary_df.reset_index(drop=True), geodata_cluster_df['cluster_100m']], axis=1)\n",
    "        geodata_clusters['clusterID'] = geodata_clusters['customer'].astype(str) + '00' + geodata_clusters['cluster_100m'].astype(str)\n",
    "\n",
    "        return self.find_home_cluster(geodata_clusters)\n",
    "\n",
    "# Example usage:\n",
    "extractor = HomeClusterExtractor(df_loc, speed_limit=speed_limit, max_distance=max_distance, epsilon=epsilon, min_samples=min_samples, \n",
    "                                 min_nights_obs=min_nights_obs, min_f_home=min_f_home)\n",
    "home_clusters = extractor.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c186c1e-11f5-4841-ac81-07a8e4ce468d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home_clusters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596a358",
   "metadata": {},
   "source": [
    "## 2. Prepare EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d9e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_udi = df_ema_content[[\"customer\", \"createdAt_day\", \"quest_create\", \"unique_day_id\", \"assess\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbd254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by customer and unique_day_id and calculate the minimum quest_create\n",
    "df_min_quest = df_ema_udi.groupby(['customer', 'unique_day_id'])['quest_create'].min().reset_index()\n",
    "\n",
    "# Rename the column to sensor_block_end\n",
    "df_min_quest.rename(columns={'quest_create': 'sensor_block_end'}, inplace=True)\n",
    "\n",
    "# Merge the minimum quest_create back to the original DataFrame\n",
    "df_ema_udi = pd.merge(df_ema_udi, df_min_quest, on=['customer', 'unique_day_id'], how='left')\n",
    "\n",
    "# Create the sensor_block_start column, which is 2 hours before quest_create\n",
    "df_ema_udi.drop(columns=['quest_create'], inplace=True)\n",
    "df_ema_udi = df_ema_udi.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79939ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_udi['sensor_block_start'] = df_ema_udi['sensor_block_end'] - pd.Timedelta(hours=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041705f9-ae62-4345-a71d-afc2f06224d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only include first assessment phase\n",
    "df_ema_udi_base = df_ema_udi.loc[df_ema_udi.assess == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5c641-d497-4956-96a2-35a09594aa57",
   "metadata": {},
   "source": [
    "## 3. Merge EMA to passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466277b1-25a0-476a-bc71-d6cb09690a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class EMAMapper:\n",
    "    def __init__(self, df_ema, df_data):\n",
    "        self.df_ema = df_ema.copy()\n",
    "        self.df_data = df_data.copy()\n",
    "\n",
    "        self.df_ema['sensor_block_start'] = pd.to_datetime(self.df_ema['sensor_block_start'])\n",
    "        self.df_ema['sensor_block_end'] = pd.to_datetime(self.df_ema['sensor_block_end'])\n",
    "        self.df_data['startTimestamp'] = pd.to_datetime(self.df_data['startTimestamp'])\n",
    "        if 'endTimestamp' in self.df_data.columns:\n",
    "            self.df_data['endTimestamp'] = pd.to_datetime(self.df_data['endTimestamp'])\n",
    "\n",
    "        self.outlier_distances = []  # To collect filtered outlier distances\n",
    "\n",
    "    def _haversine(self, lon1, lat1, lon2, lat2):\n",
    "        lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "\n",
    "        a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        r = 6371  # Radius of Earth in kilometers\n",
    "        distance = c * r\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def count_gps_rows_within_blocks(self, max_distance_km=50):\n",
    "        \"\"\"\n",
    "        Counts the number of GPS rows (n_GPS) where startTimestamp lies between sensor_block_start and sensor_block_end.\n",
    "        Also calculates the total distance within each block using the Haversine formula and the average speed.\n",
    "        Filters out unrealistic distances.\n",
    "\n",
    "        Parameters:\n",
    "        - max_distance_km: Maximum realistic distance between two consecutive GPS points. Distances above this value will be excluded.\n",
    "\n",
    "        Returns:\n",
    "        - df_ema with additional columns 'n_GPS', 'total_distance_km', and 'avg_speed_kmph' containing the count of GPS rows, total distance, and average speed within each block.\n",
    "        \"\"\"\n",
    "        gps_counts = []\n",
    "        total_distances = []\n",
    "        avg_speeds = []\n",
    "\n",
    "        df_gps = self.df_data[self.df_data['type'].isin(['Latitude', 'Longitude'])]\n",
    "        df_gps = df_gps.pivot_table(index=[\"customer\", \"startTimestamp\"], columns=\"type\", values=\"doubleValue\", aggfunc='first').reset_index()\n",
    "\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            sensor_block_start = ema_row['sensor_block_start']\n",
    "            sensor_block_end = ema_row['sensor_block_end']\n",
    "\n",
    "            df_filtered = df_gps[(df_gps['startTimestamp'] >= sensor_block_start) & \n",
    "                                 (df_gps['startTimestamp'] <= sensor_block_end)]\n",
    "\n",
    "            if df_filtered.empty:\n",
    "                gps_counts.append(0)\n",
    "                total_distances.append(0)\n",
    "                avg_speeds.append(0)\n",
    "            else:\n",
    "                df_filtered = df_filtered[(df_filtered['Latitude'] != 0) & (df_filtered['Longitude'] != 0)]\n",
    "                df_filtered = df_filtered.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "                gps_count = df_filtered.shape[0]\n",
    "                gps_counts.append(gps_count)\n",
    "\n",
    "                latitudes = df_filtered['Latitude'].values\n",
    "                longitudes = df_filtered['Longitude'].values\n",
    "                timestamps = df_filtered['startTimestamp'].values\n",
    "\n",
    "                distances = []\n",
    "                speeds = []\n",
    "                for i in range(1, len(latitudes)):\n",
    "                    distance = self._haversine(longitudes[i-1], latitudes[i-1], longitudes[i], latitudes[i])\n",
    "\n",
    "                    # Calculate time difference in hours between consecutive points\n",
    "                    time_diff_hours = (timestamps[i] - timestamps[i-1]).astype('timedelta64[s]').astype(float) / 3600\n",
    "\n",
    "                    if distance > max_distance_km:\n",
    "                        self.outlier_distances.append({\n",
    "                            'customer': df_filtered.iloc[i]['customer'],\n",
    "                            'unique_day_id': ema_row['unique_day_id'],\n",
    "                            'segment': i,\n",
    "                            'distance_km': distance\n",
    "                        })\n",
    "                        distances.append(0)  # Set outlier distances to 0\n",
    "                        speeds.append(0)  # Set outlier speeds to 0\n",
    "                    else:\n",
    "                        distances.append(distance)\n",
    "                        if time_diff_hours > 0:\n",
    "                            speeds.append(distance / time_diff_hours)\n",
    "                        else:\n",
    "                            speeds.append(0)\n",
    "\n",
    "                total_distance = np.sum(distances)\n",
    "                avg_speed = np.mean(speeds) if len(speeds) > 0 else 0\n",
    "\n",
    "                total_distances.append(total_distance)\n",
    "                avg_speeds.append(avg_speed)\n",
    "\n",
    "        self.df_ema['n_GPS'] = gps_counts\n",
    "        self.df_ema['total_distance_km'] = total_distances\n",
    "        self.df_ema['avg_speed_kmph'] = avg_speeds\n",
    "\n",
    "        return self.df_ema\n",
    "\n",
    "    def map_steps_to_ema(self, max_steps_km=50):\n",
    "        \"\"\"\n",
    "        Maps Steps data to EMA blocks based on time overlap and calculates n_steps.\n",
    "        Rounds n_steps to the nearest whole number and fills with 0 if no Steps data is found within the block.\n",
    "        Filters out unrealistic step values.\n",
    "\n",
    "        Parameters:\n",
    "        - max_steps_km: Maximum realistic distance that steps data should correspond to. Values above this will be considered outliers and excluded.\n",
    "\n",
    "        Returns:\n",
    "        - df_ema with an additional column 'n_steps' containing the summed and rounded Steps values.\n",
    "        \"\"\"\n",
    "        n_steps_values = []\n",
    "\n",
    "        df_steps = self.df_data[self.df_data['type'] == 'Steps']\n",
    "\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            sensor_block_start = ema_row['sensor_block_start']\n",
    "            sensor_block_end = ema_row['sensor_block_end']\n",
    "\n",
    "            df_filtered = df_steps[(df_steps['startTimestamp'] < sensor_block_end) & \n",
    "                                   (df_steps['endTimestamp'] > sensor_block_start)]\n",
    "\n",
    "            if df_filtered.empty:\n",
    "                n_steps_values.append(0)\n",
    "            else:\n",
    "                overlap_start = df_filtered['startTimestamp'].combine(sensor_block_start, max)\n",
    "                overlap_end = df_filtered['endTimestamp'].combine(sensor_block_end, min)\n",
    "\n",
    "                overlap_duration = (overlap_end - overlap_start).dt.total_seconds()\n",
    "                step_duration = (df_filtered['endTimestamp'] - df_filtered['startTimestamp']).dt.total_seconds()\n",
    "\n",
    "                proportion = overlap_duration / step_duration\n",
    "                weighted_value = proportion * df_filtered['doubleValue']\n",
    "\n",
    "                n_steps = weighted_value.sum()\n",
    "                n_steps_values.append(round(n_steps))\n",
    "\n",
    "        self.df_ema['n_steps'] = n_steps_values\n",
    "\n",
    "        return self.df_ema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c38f06e-8784-4a4f-8df8-f60edb043040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the EMAMapper class\n",
    "ema_mapper = EMAMapper(df_ema_udi_base, df_pass_act_base)\n",
    "\n",
    "# Map passive steps to EMA blocks\n",
    "df_ema_with_mapped_values = ema_mapper.map_steps_to_ema()\n",
    "\n",
    "# Count GPS rows within EMA blocks\n",
    "df_ema_with_gps_counts = ema_mapper.count_gps_rows_within_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec280165-9e2a-4140-a946-54a34ff11f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_empty= df_ema_with_gps_counts.loc[(df_ema_with_gps_counts.n_GPS == 0) &(df_ema_with_gps_counts.n_steps == 0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ac7f5-277b-477a-b7be-6d2164ac5926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
