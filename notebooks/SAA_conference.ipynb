{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f142476",
   "metadata": {},
   "source": [
    "# SAA 2024: GPS Data Preprocessing and Reliability in PREACT\n",
    "\n",
    "This notebook preprocesses GPS data in accordance with Mueller et al. (2021) and performs analyses by splitting the data into two weekly segments.\n",
    "\n",
    "1. **Load Data**: Load necessary data from pickle files.\n",
    "2. **Preprocess Data**: Filter and transform the data for analysis.\n",
    "3. **Split Data**: Divide the data into two parts based on the `ema_base_start` variable.\n",
    "4. **Analyze Data**: Perform analyses separately for the two parts.\n",
    "5. **Calculate Internal Consistency**: Evaluate the internal consistency of features between the first and second weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36823522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from config import datapath\n",
    "\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "library_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'library'))\n",
    "sys.path.append(library_path)\n",
    "\n",
    "from gps_features import haversine, apply_clustering, identify_home,calculate_metrics, calculate_transition_time, calculate_intraclass_coefficient, calculate_retest_reliability\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import regex as re\n",
    "import pingouin as pg  # Ensure pingouin is installed\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import statistics \n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr\n",
    "from math import radians, cos, sin, asin, sqrt, log\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns \n",
    "sns.set_context(\"notebook\", rc={\"axes.labelsize\": 14, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': True})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54511dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = dt.date.today().strftime(\"%d%m%Y\")\n",
    "today_day = pd.to_datetime('today').normalize()\n",
    "today = \"21052024\"\n",
    "\n",
    "with open(datapath + f'ema_data.pkl', 'rb') as file:\n",
    "    df_active = pickle.load(file)\n",
    "    \n",
    "with open(datapath + f'ema_content.pkl', 'rb') as file:\n",
    "    df_ema = pickle.load(file)\n",
    "    \n",
    "with open(datapath + f'gps_data.pkl', 'rb') as file:\n",
    "    df_gps = pickle.load(file)\n",
    "    \n",
    "with open(datapath + f'passive_data.pkl', 'rb') as file:\n",
    "    df_passive = pickle.load(file)\n",
    "\n",
    "with open(datapath + f'monitoring_data.pkl', 'rb') as file:\n",
    "    df_monitoring = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76bd4c",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4dbac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hour_daily = 8\n",
    "min_days_data = 12\n",
    "\n",
    "#stationary filtering\n",
    "max_distance = 150 \n",
    "speed_limit = 1.4  # Max allowed speed in m/s\n",
    "\n",
    "# DBSCAN\n",
    "kms_per_radian = 6371.0088 # equitorial radius of the earth = 6,371.1 \n",
    "epsilon = 0.03/kms_per_radian\n",
    "min_samples = 10\n",
    "\n",
    "# Kmeans\n",
    "DKmeans = 500\n",
    "\n",
    "#home featurenight\n",
    "min_nights_obs = 4\n",
    "min_f_home = 0.5 \n",
    "\n",
    "# EMA\n",
    "min_num_daily = 4\n",
    "min_days_data = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4488c8",
   "metadata": {},
   "source": [
    "## EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3196b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema = df_ema[['customer','study', 'createdAt', 'choice_id', 'choice_text',\n",
    "       'quest_title', 'questionnaire_name', 'ema_start_date', 'status',\n",
    "       'study_version']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59eed7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema = df_ema.copy()\n",
    "df_ema['weekday'] = df_ema['createdAt'].dt.day_name()\n",
    "df_ema['createdAt_day'] = df_ema.createdAt.dt.normalize()\n",
    "\n",
    "df_ema['quest_nr'] = df_ema['questionnaire_name'].apply(lambda x: int(re.search(r'\\d+', x).group()) \\\n",
    "                                               if re.search(r'\\d+', x) else None)\n",
    "\n",
    "df_ema[\"n_quest\"] = df_ema.groupby([\"study\", \"customer\", \"createdAt_day\"])[\"questionnaire_name\"].transform(\"nunique\")\n",
    "\n",
    "# Create unique day ID\n",
    "# Create a unique day identifier directly without creating extra columns\n",
    "df_ema['unique_day_id'] = df_ema['createdAt_day'].dt.strftime('%Y%m%d') + '_' + df_ema['quest_nr'].astype(str)\n",
    "\n",
    "# Now df_ema has the 'unique_day_id' column directly\n",
    "\n",
    "study_mapping = {\n",
    "    24: 0,\n",
    "    25: 0,\n",
    "    33: 1,\n",
    "    34: 1\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'study' column\n",
    "df_ema['assess'] = df_ema['study'].map(study_mapping)\n",
    "# Replace '_morning' with '' in the 'quest_title' column as we don't need to differenciate\n",
    "df_ema['quest_title'] = df_ema['quest_title'].str.replace('_morning', '', regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d7a39",
   "metadata": {},
   "source": [
    "### Include only patients with finished assessments and enough data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834e9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema = df_ema.loc[df_ema.status.isin([\"Abgeschlossen\", \"Post_Erhebung_1\",\n",
    "                                                             \"Erhebung_2_aktiv\",\"Post_Erhebung_2\"])]\n",
    "df_ema = df_ema.loc[df_ema.study.isin([24,25])] # first assessment phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49fb930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema = df_ema.loc[df_ema[\"n_quest\"] >= min_num_daily]\n",
    "df_ema[\"n_days_4\"] = df_ema.groupby(\"customer\")[\"createdAt_day\"].transform(\"nunique\")\n",
    "df_ema = df_ema.loc[df_ema.n_days_4 >= min_days_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7a9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time boundaries for the first and second week\n",
    "df_ema['first_week_end'] = df_ema['ema_start_date'] + pd.Timedelta(days=8)\n",
    "df_ema['second_week_end'] = df_ema['ema_start_date'] + pd.Timedelta(days=15)\n",
    "\n",
    "# Filter data for the first and second week\n",
    "first_week_df = df_ema[(df_ema['createdAt_day'] >= df_ema['ema_start_date']) & \n",
    "                       (df_ema['createdAt_day'] < df_ema['first_week_end'])]\n",
    "\n",
    "second_week_df = df_ema[(df_ema['createdAt_day'] >= df_ema['first_week_end'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d4d8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table as specified\n",
    "df_piv_first = first_week_df.pivot_table(\n",
    "    index=[\"customer\", \"unique_day_id\"],\n",
    "    columns=\"quest_title\",\n",
    "    values=\"choice_text\",\n",
    "    aggfunc='first'  # Using 'first' since each entry should theoretically be unique per group\n",
    ")\n",
    "\n",
    "# The columns are now a single level Index with just the quest_title values since 'values' is not a list anymore\n",
    "df_piv_first.columns = [col for col in df_piv_first.columns.values]\n",
    "\n",
    "# Reset the index to turn the MultiIndex into columns\n",
    "df_piv_first = df_piv_first.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35ab272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table as specified\n",
    "df_piv_second = second_week_df.pivot_table(\n",
    "    index=[\"customer\", \"unique_day_id\"],\n",
    "    columns=\"quest_title\",\n",
    "    values=\"choice_text\",\n",
    "    aggfunc='first'  # Using 'first' since each entry should theoretically be unique per group\n",
    ")\n",
    "\n",
    "# The columns are now a single level Index with just the quest_title values since 'values' is not a list anymore\n",
    "df_piv_second.columns = [col for col in df_piv_second.columns.values]\n",
    "\n",
    "# Reset the index to turn the MultiIndex into columns\n",
    "df_piv_second = df_piv_second.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ed0ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['ecg_control', 'er_acceptance',\n",
    "       'er_control', 'er_distraction', 'er_intensity', 'er_reappraisal',\n",
    "       'er_relaxation', 'er_rumination', 'er_suppression', 'event_general',\n",
    "       'event_social1', 'event_social2', 'event_social3',\n",
    "       'panas_attentiveness', 'panas_fatigue', 'panas_fear1', 'panas_fear2',\n",
    "       'panas_guilt1', 'panas_guilt2', 'panas_hostility1', 'panas_hostility2',\n",
    "       'panas_joviality1', 'panas_joviality2', 'panas_loneliness',\n",
    "       'panas_sadness1', 'panas_sadness2', 'panas_selfassurance',\n",
    "       'panas_serenity1', 'panas_serenity2', 'panas_shyness',\n",
    "       'physical_health', 'situation2', 'ta_behavioral',\n",
    "       'ta_behavioral_2', 'ta_kognitiv', 'ta_kognitiv_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e28284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_piv_first[columns_to_convert] = df_piv_first[columns_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b56c1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_piv_second[columns_to_convert] = df_piv_second[columns_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8157ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_scale = ['panas_attentiveness','panas_joviality1','panas_joviality2', 'panas_selfassurance','panas_serenity1',\n",
    " 'panas_serenity2']\n",
    "na_scale = ['panas_fatigue','panas_fear1','panas_fear2','panas_guilt1','panas_guilt2','panas_hostility1',\n",
    "            'panas_hostility2','panas_loneliness','panas_sadness1','panas_sadness2','panas_shyness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4723612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICC results for PA:\n",
      "    Type              Description       ICC          F  df1  df2  \\\n",
      "0   ICC1   Single raters absolute  0.858505  13.134790  110  111   \n",
      "1   ICC2     Single random raters  0.858646  13.321869  110  110   \n",
      "2   ICC3      Single fixed raters  0.860353  13.321869  110  110   \n",
      "3  ICC1k  Average raters absolute  0.923866  13.134790  110  111   \n",
      "4  ICC2k    Average random raters  0.923948  13.321869  110  110   \n",
      "5  ICC3k     Average fixed raters  0.924935  13.321869  110  110   \n",
      "\n",
      "           pval         CI95%  \n",
      "0  3.150269e-34    [0.8, 0.9]  \n",
      "1  2.802954e-34    [0.8, 0.9]  \n",
      "2  2.802954e-34    [0.8, 0.9]  \n",
      "3  3.150269e-34  [0.89, 0.95]  \n",
      "4  2.802954e-34  [0.89, 0.95]  \n",
      "5  2.802954e-34  [0.89, 0.95]  \n",
      "ICC results for NA:\n",
      "    Type              Description       ICC          F  df1  df2  \\\n",
      "0   ICC1   Single raters absolute  0.849272  12.268962  110  111   \n",
      "1   ICC2     Single random raters  0.849193  12.184168  110  110   \n",
      "2   ICC3      Single fixed raters  0.848303  12.184168  110  110   \n",
      "3  ICC1k  Average raters absolute  0.918494  12.268962  110  111   \n",
      "4  ICC2k    Average random raters  0.918447  12.184168  110  110   \n",
      "5  ICC3k     Average fixed raters  0.917926  12.184168  110  110   \n",
      "\n",
      "           pval         CI95%  \n",
      "0  8.046115e-33  [0.79, 0.89]  \n",
      "1  1.885601e-32  [0.79, 0.89]  \n",
      "2  1.885601e-32  [0.79, 0.89]  \n",
      "3  8.046115e-33  [0.88, 0.94]  \n",
      "4  1.885601e-32  [0.88, 0.94]  \n",
      "5  1.885601e-32  [0.88, 0.94]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming pa_scale is defined\n",
    "pa_scale = ['panas_attentiveness', 'panas_joviality1', 'panas_joviality2', \n",
    "            'panas_selfassurance', 'panas_serenity1', 'panas_serenity2']\n",
    "\n",
    "# Calculate the mean PA value per customer for each week\n",
    "mean_pa_first = df_piv_first.groupby('customer')[pa_scale].mean().mean(axis=1).reset_index(name='mean_pa_first')\n",
    "mean_pa_second = df_piv_second.groupby('customer')[pa_scale].mean().mean(axis=1).reset_index(name='mean_pa_second')\n",
    "\n",
    "# Merge the DataFrames for PA\n",
    "merged_pa = mean_pa_first.merge(mean_pa_second, on='customer')\n",
    "\n",
    "# Reshape the data for PA\n",
    "pa_melted = pd.melt(merged_pa, id_vars='customer', \n",
    "                    value_vars=['mean_pa_first', 'mean_pa_second'],\n",
    "                    var_name='week', value_name='mean_pa')\n",
    "\n",
    "# Calculate ICC for PA\n",
    "icc_pa = pg.intraclass_corr(data=pa_melted, targets='customer', raters='week', ratings='mean_pa')\n",
    "print(\"ICC results for PA:\")\n",
    "print(icc_pa)\n",
    "\n",
    "# Similarly, for NA scales, you would follow the same process\n",
    "na_scale = ['panas_fatigue','panas_fear1','panas_fear2','panas_guilt1','panas_guilt2','panas_hostility1',\n",
    "            'panas_hostility2','panas_loneliness','panas_sadness1','panas_sadness2','panas_shyness']  # replace with actual NA features\n",
    "\n",
    "# Calculate the mean NA value per customer for each week\n",
    "mean_na_first = df_piv_first.groupby('customer')[na_scale].mean().mean(axis=1).reset_index(name='mean_na_first')\n",
    "mean_na_second = df_piv_second.groupby('customer')[na_scale].mean().mean(axis=1).reset_index(name='mean_na_second')\n",
    "\n",
    "# Merge the DataFrames for NA\n",
    "merged_na = mean_na_first.merge(mean_na_second, on='customer')\n",
    "\n",
    "# Reshape the data for NA\n",
    "na_melted = pd.melt(merged_na, id_vars='customer', \n",
    "                    value_vars=['mean_na_first', 'mean_na_second'],\n",
    "                    var_name='week', value_name='mean_na')\n",
    "\n",
    "# Calculate ICC for NA\n",
    "icc_na = pg.intraclass_corr(data=na_melted, targets='customer', raters='week', ratings='mean_na')\n",
    "print(\"ICC results for NA:\")\n",
    "print(icc_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2566a0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation for mean_na between first and second week: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Assume geodata_cluster_merged is your merged dataframe\n",
    "# Calculate the retest reliability between the first and second week\n",
    "features = [\"mean_na\"]\n",
    "correlation = calculate_retest_reliability(merged_na, features)\n",
    "\n",
    "# Print the results\n",
    "for feature, correlation in correlation.items():\n",
    "    print(f\"Correlation for {feature} between first and second week: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9a0d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation for mean_pa between first and second week: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Assume geodata_cluster_merged is your merged dataframe\n",
    "# Calculate the retest reliability between the first and second week\n",
    "features = [\"mean_pa\"]\n",
    "correlation = calculate_retest_reliability(merged_pa, features)\n",
    "\n",
    "# Print the results\n",
    "for feature, correlation in correlation.items():\n",
    "    print(f\"Correlation for {feature} between first and second week: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc8dda",
   "metadata": {},
   "source": [
    "### Correlate aggregates for emotion regulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "er_columns = ['er_acceptance','er_control', 'er_distraction', 'er_intensity', 'er_reappraisal',\n",
    "       'er_relaxation', 'er_rumination', 'er_suppression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6cdeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_er_first = df_piv_first.groupby('customer')[er_columns].mean().reset_index()\n",
    "mean_er_second = df_piv_second.groupby('customer')[er_columns].mean().reset_index()\n",
    "\n",
    "merged_er = pd.merge(mean_er_first, mean_er_second, on='customer', suffixes=('_first', '_second'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff61fd6",
   "metadata": {},
   "source": [
    "### Create retest-reliabiliy using ICC and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55ee405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICC result for feature er_acceptance:\n",
      "    Type              Description       ICC         F  df1  df2          pval  \\\n",
      "0   ICC1   Single raters absolute  0.777285  7.980090  110  111  2.451632e-24   \n",
      "1   ICC2     Single random raters  0.778376  8.349136  110  110  5.013216e-25   \n",
      "2   ICC3      Single fixed raters  0.786076  8.349136  110  110  5.013216e-25   \n",
      "3  ICC1k  Average raters absolute  0.874688  7.980090  110  111  2.451632e-24   \n",
      "4  ICC2k    Average random raters  0.875378  8.349136  110  110  5.013216e-25   \n",
      "5  ICC3k     Average fixed raters  0.880227  8.349136  110  110  5.013216e-25   \n",
      "\n",
      "          CI95%  \n",
      "0  [0.69, 0.84]  \n",
      "1  [0.69, 0.84]  \n",
      "2   [0.7, 0.85]  \n",
      "3  [0.82, 0.91]  \n",
      "4  [0.82, 0.92]  \n",
      "5  [0.83, 0.92]  \n",
      "ICC result for feature er_control:\n",
      "    Type              Description       ICC          F  df1  df2  \\\n",
      "0   ICC1   Single raters absolute  0.889008  17.019375  110  111   \n",
      "1   ICC2     Single random raters  0.888955  16.874200  110  110   \n",
      "2   ICC3      Single fixed raters  0.888107  16.874200  110  110   \n",
      "3  ICC1k  Average raters absolute  0.941243  17.019375  110  111   \n",
      "4  ICC2k    Average random raters  0.941214  16.874200  110  110   \n",
      "5  ICC3k     Average fixed raters  0.940738  16.874200  110  110   \n",
      "\n",
      "           pval         CI95%  \n",
      "0  1.062882e-39  [0.84, 0.92]  \n",
      "1  3.128789e-39  [0.84, 0.92]  \n",
      "2  3.128789e-39  [0.84, 0.92]  \n",
      "3  1.062882e-39  [0.91, 0.96]  \n",
      "4  3.128789e-39  [0.91, 0.96]  \n",
      "5  3.128789e-39  [0.91, 0.96]  \n",
      "ICC result for feature er_distraction:\n",
      "    Type              Description       ICC         F  df1  df2          pval  \\\n",
      "0   ICC1   Single raters absolute  0.773838  7.843232  110  111  5.180648e-24   \n",
      "1   ICC2     Single random raters  0.774290  7.984563  110  110  3.455637e-24   \n",
      "2   ICC3      Single fixed raters  0.777396  7.984563  110  110  3.455637e-24   \n",
      "3  ICC1k  Average raters absolute  0.872502  7.843232  110  111  5.180648e-24   \n",
      "4  ICC2k    Average random raters  0.872789  7.984563  110  110  3.455637e-24   \n",
      "5  ICC3k     Average fixed raters  0.874758  7.984563  110  110  3.455637e-24   \n",
      "\n",
      "          CI95%  \n",
      "0  [0.69, 0.84]  \n",
      "1  [0.69, 0.84]  \n",
      "2  [0.69, 0.84]  \n",
      "3  [0.81, 0.91]  \n",
      "4  [0.81, 0.91]  \n",
      "5  [0.82, 0.91]  \n",
      "ICC result for feature er_intensity:\n",
      "    Type              Description       ICC         F  df1  df2          pval  \\\n",
      "0   ICC1   Single raters absolute  0.720098  6.145357  110  111  1.370283e-19   \n",
      "1   ICC2     Single random raters  0.721426  6.361993  110  110  4.518142e-20   \n",
      "2   ICC3      Single fixed raters  0.728334  6.361993  110  110  4.518142e-20   \n",
      "3  ICC1k  Average raters absolute  0.837276  6.145357  110  111  1.370283e-19   \n",
      "4  ICC2k    Average random raters  0.838172  6.361993  110  110  4.518142e-20   \n",
      "5  ICC3k     Average fixed raters  0.842817  6.361993  110  110  4.518142e-20   \n",
      "\n",
      "          CI95%  \n",
      "0   [0.62, 0.8]  \n",
      "1   [0.62, 0.8]  \n",
      "2  [0.63, 0.81]  \n",
      "3  [0.76, 0.89]  \n",
      "4  [0.76, 0.89]  \n",
      "5  [0.77, 0.89]  \n",
      "ICC result for feature er_reappraisal:\n",
      "    Type              Description       ICC          F  df1  df2  \\\n",
      "0   ICC1   Single raters absolute  0.826596  10.533786  110  111   \n",
      "1   ICC2     Single random raters  0.827404  11.134332  110  110   \n",
      "2   ICC3      Single fixed raters  0.835178  11.134332  110  110   \n",
      "3  ICC1k  Average raters absolute  0.905067  10.533786  110  111   \n",
      "4  ICC2k    Average random raters  0.905551  11.134332  110  110   \n",
      "5  ICC3k     Average fixed raters  0.910188  11.134332  110  110   \n",
      "\n",
      "           pval         CI95%  \n",
      "0  9.895899e-30  [0.76, 0.88]  \n",
      "1  1.241263e-30  [0.75, 0.88]  \n",
      "2  1.241263e-30  [0.77, 0.88]  \n",
      "3  9.895899e-30  [0.86, 0.93]  \n",
      "4  1.241263e-30  [0.86, 0.94]  \n",
      "5  1.241263e-30  [0.87, 0.94]  \n",
      "ICC result for feature er_relaxation:\n",
      "    Type              Description       ICC          F  df1  df2  \\\n",
      "0   ICC1   Single raters absolute  0.871324  14.542937  110  111   \n",
      "1   ICC2     Single random raters  0.871563  14.975859  110  110   \n",
      "2   ICC3      Single fixed raters  0.874811  14.975859  110  110   \n",
      "3  ICC1k  Average raters absolute  0.931238  14.542937  110  111   \n",
      "4  ICC2k    Average random raters  0.931375  14.975859  110  110   \n",
      "5  ICC3k     Average fixed raters  0.933226  14.975859  110  110   \n",
      "\n",
      "           pval         CI95%  \n",
      "0  2.344193e-36  [0.82, 0.91]  \n",
      "1  1.034932e-36  [0.82, 0.91]  \n",
      "2  1.034932e-36  [0.82, 0.91]  \n",
      "3  2.344193e-36   [0.9, 0.95]  \n",
      "4  1.034932e-36   [0.9, 0.95]  \n",
      "5  1.034932e-36   [0.9, 0.95]  \n",
      "ICC result for feature er_rumination:\n",
      "    Type              Description       ICC         F  df1  df2          pval  \\\n",
      "0   ICC1   Single raters absolute  0.782133  8.179902  110  111  8.365818e-25   \n",
      "1   ICC2     Single random raters  0.782296  8.236441  110  110  9.042004e-25   \n",
      "2   ICC3      Single fixed raters  0.783466  8.236441  110  110  9.042004e-25   \n",
      "3  ICC1k  Average raters absolute  0.877749  8.179902  110  111  8.365818e-25   \n",
      "4  ICC2k    Average random raters  0.877852  8.236441  110  110  9.042004e-25   \n",
      "5  ICC3k     Average fixed raters  0.878588  8.236441  110  110  9.042004e-25   \n",
      "\n",
      "          CI95%  \n",
      "0   [0.7, 0.84]  \n",
      "1   [0.7, 0.85]  \n",
      "2   [0.7, 0.85]  \n",
      "3  [0.82, 0.92]  \n",
      "4  [0.82, 0.92]  \n",
      "5  [0.82, 0.92]  \n",
      "ICC result for feature er_suppression:\n",
      "    Type              Description       ICC          F  df1  df2  \\\n",
      "0   ICC1   Single raters absolute  0.885778  16.509727  110  111   \n",
      "1   ICC2     Single random raters  0.885732  16.394955  110  110   \n",
      "2   ICC3      Single fixed raters  0.885024  16.394955  110  110   \n",
      "3  ICC1k  Average raters absolute  0.939430  16.509727  110  111   \n",
      "4  ICC2k    Average random raters  0.939404  16.394955  110  110   \n",
      "5  ICC3k     Average fixed raters  0.939006  16.394955  110  110   \n",
      "\n",
      "           pval         CI95%  \n",
      "0  4.764271e-39  [0.84, 0.92]  \n",
      "1  1.279390e-38  [0.84, 0.92]  \n",
      "2  1.279390e-38  [0.84, 0.92]  \n",
      "3  4.764271e-39  [0.91, 0.96]  \n",
      "4  1.279390e-38  [0.91, 0.96]  \n",
      "5  1.279390e-38  [0.91, 0.96]  \n"
     ]
    }
   ],
   "source": [
    "import pingouin as pg\n",
    "# List of features without the '_first' or '_second' suffixes\n",
    "# List of features without the '_first' or '_second' suffixes\n",
    "features = ['er_acceptance','er_control', 'er_distraction', 'er_intensity', 'er_reappraisal',\n",
    "       'er_relaxation', 'er_rumination', 'er_suppression']\n",
    "\n",
    "# Dictionary to store ICC results\n",
    "icc_results = {}\n",
    "\n",
    "# Calculate ICC for each pair of columns\n",
    "for feature in features:\n",
    "    feature_first = feature + '_first'\n",
    "    feature_second = feature + '_second'\n",
    "    \n",
    "    if feature_first in merged_er.columns and feature_second in merged_er.columns:\n",
    "        # Drop rows with NaN values in the relevant columns\n",
    "        clean_data = merged_er[[feature_first, feature_second]].dropna()\n",
    "        \n",
    "        if len(clean_data) >= 5:\n",
    "            # Create a DataFrame suitable for pingouin\n",
    "            data = pd.DataFrame({\n",
    "                'subject': clean_data.index,\n",
    "                'first': clean_data[feature_first],\n",
    "                'second': clean_data[feature_second]\n",
    "            })\n",
    "            \n",
    "            # Melt the DataFrame to long format\n",
    "            data_long = data.melt(id_vars='subject', var_name='rater', value_name='rating')\n",
    "            \n",
    "            # Calculate ICC(2,1) for single measurements\n",
    "            icc = pg.intraclass_corr(data=data_long, targets='subject', raters='rater', ratings='rating', nan_policy='omit')\n",
    "            print(f\"ICC result for feature {feature}:\")\n",
    "            print(icc)\n",
    "            if 'ICC2' in icc['Type'].values:\n",
    "                icc_value = icc[icc['Type'] == 'ICC2']['ICC'].values[0]\n",
    "                icc_results[feature] = icc_value\n",
    "            else:\n",
    "                print(f\"ICC2 not found for feature {feature}\")\n",
    "                icc_results[feature] = np.nan\n",
    "        else:\n",
    "            print(f\"Not enough non-missing values for feature {feature}.\")\n",
    "            icc_results[feature] = np.nan\n",
    "    else:\n",
    "        print(f\"Feature columns for {feature} are missing in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2201350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation for er_acceptance between first and second week: 0.84\n",
      "Correlation for er_control between first and second week: 0.91\n",
      "Correlation for er_distraction between first and second week: 0.82\n",
      "Correlation for er_intensity between first and second week: 0.71\n",
      "Correlation for er_reappraisal between first and second week: 0.87\n",
      "Correlation for er_relaxation between first and second week: 0.89\n",
      "Correlation for er_rumination between first and second week: 0.80\n",
      "Correlation for er_suppression between first and second week: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Assume geodata_cluster_merged is your merged dataframe\n",
    "# Calculate the retest reliability between the first and second week\n",
    "\n",
    "correlation = calculate_retest_reliability(merged_er, features)\n",
    "\n",
    "# Print the results\n",
    "for feature, correlation in correlation.items():\n",
    "    print(f\"Correlation for {feature} between first and second week: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8c624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
