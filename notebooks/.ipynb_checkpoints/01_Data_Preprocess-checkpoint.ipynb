{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"TIKI: Data Compliance and Quality\"\n",
    "author: \"Leona Hammelrath\"\n",
    "date: \"10 April 2024\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "\n",
    "format: html\n",
    "execute:\n",
    "    eval: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "First Data for Passive Data Collection using Smartwatches and GPS from the PREACT Study. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Treatment personalization is highly discussed to counteract insufficient response rates in psychotherapy. In the quest for criteria allowing informed selection or adaptation, ambulatory assessment data (i.e. EMA, passive sensing)are a key component, as processes happening outside of therapy sessions can be depicted in high temporal and/or spatial resolution.\n",
    "\n",
    "PREACT is a multicenter prospective-longitudinal study investigating different predictors of non-response (i.e. EEG, fMRI) in around 500 patients undergoing cognitive behavioral therapy for internalizing disorders (https://forschungsgruppe5187.de/de). \n",
    "\n",
    "## Methods\n",
    "Patients can enroll for therapy-accompanying ambulatory assessment. They are provided with a customized study app and a state-of-the-art smartwatch collecting passive data like GPS and heart rate for up to 365 days. In parallel, three 14-day EMA phases (pre-, mid- and post-therapy) cover transdiagnostic (i.e. emotion regulation), contextual and therapy-related aspects.  \n",
    "\n",
    "Here, we present first results on data compliance and quality for the passive sensing data as well as EMA assessments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import regex as re\n",
    "\n",
    "\n",
    "from server_config import datapath, proj_sheet,preprocessed_path, raw_path, redcap_path, preprocessed_path_freezed\n",
    "\n",
    "today = date.today().strftime(\"%d%m%Y\")\n",
    "today_day = pd.to_datetime('today').normalize()\n",
    "today = \"24032025\"\n",
    "\n",
    "df_monitoring = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{proj_sheet}/export?format=csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual passive + ema_data\n",
    "datapath1 = raw_path + f\"/export_tiki_{today}/\"\n",
    "file_pattern = os.path.join(datapath1, \"epoch_part*.csv\")\n",
    "file_list = glob.glob(file_pattern)\n",
    "file_list.sort()\n",
    "df_complete = pd.concat((pd.read_csv(f, encoding=\"latin-1\", low_memory=False) for f in file_list), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1 Import epoch level passive + GPS data\n",
    "\n",
    "# Extract customer identifier\n",
    "df_complete[\"customer\"] = df_complete.customer.str.split(\"@\").str.get(0)\n",
    "df_complete[\"customer\"] = df_complete[\"customer\"].str[:4]\n",
    "\n",
    "# Convert timestamps from milliseconds since epoch to datetime\n",
    "df_complete[\"startTimestamp\"] = pd.to_datetime(df_complete[\"startTimestamp\"], unit='ms')\n",
    "df_complete[\"endTimestamp\"] = pd.to_datetime(df_complete[\"endTimestamp\"], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Timezone offset adjustment\n",
    "\n",
    "# Fill NaN timezoneOffsets with 0\n",
    "df_complete['timezoneOffset_filled'] = df_complete['timezoneOffset'].fillna(0)\n",
    "\n",
    "# Convert timezoneOffset to timedelta\n",
    "df_complete['timezoneOffset_timedelta'] = pd.to_timedelta(df_complete['timezoneOffset_filled'], unit='ms')\n",
    "\n",
    "# Adjust the timestamps\n",
    "df_complete['startTimestamp'] = df_complete['startTimestamp'] + df_complete['timezoneOffset_timedelta']\n",
    "df_complete['endTimestamp'] = df_complete['endTimestamp'] + df_complete['timezoneOffset_timedelta']\n",
    "\n",
    "# Calculate the duration after adjusting timestamps\n",
    "df_complete[\"start_end\"] = df_complete[\"endTimestamp\"] - df_complete[\"startTimestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Timedelta to total seconds (float)\n",
    "df_complete['start_end'] = df_complete['start_end'].dt.total_seconds()\n",
    "\n",
    "# Extract date and hour\n",
    "df_complete[\"startTimestamp_day\"] = df_complete.startTimestamp.dt.normalize()\n",
    "df_complete[\"startTimestamp_hour\"] = df_complete.startTimestamp.dt.hour\n",
    "\n",
    "# Drop temporary columns if not needed\n",
    "df_complete.drop(columns=['timezoneOffset_filled', 'timezoneOffset_timedelta'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with backup data\n",
    "backup_path = preprocessed_path + \"/backup_data_passive_actual.feather\"\n",
    "\n",
    "df_backup = pd.read_feather(backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_timestamp = df_backup['startTimestamp'].max()\n",
    "\n",
    "# Filter the second dataframe to include only entries after the latest timestamp\n",
    "df_complete_filtered = df_complete[df_complete['startTimestamp'] > latest_timestamp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_filtered = df_complete_filtered.drop(columns=['valueType', 'createdAt', 'source', \n",
    "                                                              'trustworthiness', 'medicalGrade', 'generation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = df_monitoring.copy()\n",
    "df_monitoring.rename(columns = {\"Pseudonym\": \"customer\", \"EMA_ID\": \"ema_id\", \"Status\": \"status\",\n",
    "                                \"Studienversion\":\"study_version\", \"FOR_ID\":\"for_id\", \n",
    "                           \"Start EMA Baseline\": \"ema_base_start\", \"Ende EMA Baseline\": \"ema_base_end\", \n",
    "                           \"Freischaltung/ Start EMA T20\": \"ema_t20_start\",\"Ende EMA T20\":\"ema_t20_end\", \n",
    "                                \"Freischaltung/ Start EMA Post\":\"ema_post_start\",\n",
    "                               \"Ende EMA Post\":\"ema_post_end\", \"T20=Post\":\"t20_post\" }, inplace=True)\n",
    "\n",
    "df_monitoring = df_monitoring[['for_id', 'ema_id', 'customer', 'study_version', 'status',\n",
    "       't20_post', 'ema_base_start', 'ema_base_end', 'ema_t20_start', 'ema_t20_end',\n",
    "       'ema_post_start', 'ema_post_end']]\n",
    "\n",
    "df_monitoring[\"customer\"] = df_monitoring[\"customer\"].str[:4]\n",
    "df_monitoring[\"for_id\"] = df_monitoring.for_id.str.strip()\n",
    "\n",
    "df_monitoring[\"ema_base_start\"] = pd.to_datetime(df_monitoring[\"ema_base_start\"], dayfirst=True)\n",
    "df_monitoring[\"ema_base_end\"] = pd.to_datetime(df_monitoring[\"ema_base_end\"], dayfirst=True)\n",
    "\n",
    "df_monitoring_short = df_monitoring[[\"customer\", \"for_id\",\"ema_id\",\"status\", \"study_version\", \"ema_base_start\",\"ema_base_end\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_filtered= df_complete_filtered.merge(df_monitoring_short, on=\"customer\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1643653/1002762743.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_complete_filtered[col] = df_complete_filtered[col].fillna(-99)\n"
     ]
    }
   ],
   "source": [
    "object_cols = [\"booleanValue\", \"stringValue\",\"customer\", \"type\", \"status\", \"study_version\"] \n",
    "\n",
    "# Fill NaN values with -99 for the specified columns\n",
    "for col in object_cols:\n",
    "    df_complete_filtered[col] = df_complete_filtered[col].fillna(-99)\n",
    "\n",
    "# Convert \"booleanValue\" to boolean\n",
    "df_complete_filtered['booleanValue'] = df_complete_filtered['booleanValue'].apply(lambda x: bool(x) if x != -99 else False)\n",
    "\n",
    "# Convert \"stringValue\", \"status\", \"study_version\" to string using StringDtype\n",
    "df_complete_filtered['stringValue'] = df_complete_filtered['stringValue'].astype('string')\n",
    "df_complete_filtered['status'] = df_complete_filtered['status'].astype('string')\n",
    "df_complete_filtered['study_version'] = df_complete_filtered['study_version'].astype('string')\n",
    "df_complete_filtered['customer'] = df_complete_filtered['customer'].astype('string')\n",
    "df_complete_filtered['type'] = df_complete_filtered['type'].astype('string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_groups = {\n",
    "    'GPS': [\"Latitude\"],\n",
    "    # Add more groups as needed\n",
    "    'Activity': [\"Steps\"],\n",
    "    'Sleep': [\"SleepBinary\"],\n",
    "    'Heart_Rate': [\"HeartRate\"]\n",
    "    # Add more groups as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load and match relevant data from separate .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from separate csv files\n",
    "session = pd.read_csv(datapath1 + \"questionnaireSession.csv\",low_memory=False)\n",
    "answers = pd.read_csv(datapath1 + \"answers.csv\", low_memory=False)\n",
    "choice = pd.read_csv(datapath1 + \"choice.csv\",low_memory=False)\n",
    "questions = pd.read_csv(datapath1 + \"questions.csv\",low_memory=False)\n",
    "questionnaire = pd.read_csv(datapath1 + \"questionnaires.csv\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session data\n",
    "session[\"user\"] = session[\"user\"].str[:4]\n",
    "session.rename(columns = {\"user\":\"customer\",\"completedAt\": \"quest_complete\", \"createdAt\": \"quest_create\", \"expirationTimestamp\": \"quest_expir\"}, inplace=True)\n",
    "session[\"quest_create\"] = (pd.to_datetime(session[\"quest_create\"],unit='ms'))\n",
    "session[\"quest_complete\"] = (pd.to_datetime(session[\"quest_complete\"],unit='ms'))\n",
    "\n",
    "df_sess = session[[\"customer\", \"sessionRun\", \"quest_create\", \"quest_complete\", \"study\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer data \n",
    "answers[\"user\"] = answers[\"user\"].str[:4]\n",
    "answers = answers[[\"user\", \"questionnaire\", \"study\", \"question\",\"element\", \"createdAt\"]]\n",
    "answers[\"createdAt\"] = (pd.to_datetime(answers[\"createdAt\"],unit='ms'))\n",
    "answers.rename(columns={\"user\":\"customer\", \"createdAt\": \"quest_create\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item description data\n",
    "choice = choice[[\"element\", \"choice_id\", \"text\", \"question\"]]\n",
    "choice.rename(columns={\"text\":\"choice_text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question description data\n",
    "questions = questions[[\"id\", \"title\"]]\n",
    "questions.rename(columns={\"id\":\"question\",\"title\":\"quest_title\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire = questionnaire[[\"id\", \"name\"]]\n",
    "questionnaire.rename(columns={\"id\":\"questionnaire\",\"name\":\"questionnaire_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_merged = pd.merge(answers, choice, on= [\"question\",\"element\"])\n",
    "answer_merged = pd.merge(answer_merged, questions, on= \"question\")\n",
    "answer_merged = pd.merge(answer_merged, questionnaire, on= \"questionnaire\")\n",
    "answer_merged[\"quest_create_day\"] = answer_merged.quest_create.dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>questionnaire</th>\n",
       "      <th>study</th>\n",
       "      <th>question</th>\n",
       "      <th>element</th>\n",
       "      <th>quest_create</th>\n",
       "      <th>choice_id</th>\n",
       "      <th>choice_text</th>\n",
       "      <th>quest_title</th>\n",
       "      <th>questionnaire_name</th>\n",
       "      <th>quest_create_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APbN</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>315</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>2023-04-27 08:07:14.748</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>panas_selfassurance</td>\n",
       "      <td>TIKI_1A_E1</td>\n",
       "      <td>2023-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APbN</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>316</td>\n",
       "      <td>1720.0</td>\n",
       "      <td>2023-04-27 08:07:15.748</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>panas_joviality2</td>\n",
       "      <td>TIKI_1A_E1</td>\n",
       "      <td>2023-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APbN</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>317</td>\n",
       "      <td>1726.0</td>\n",
       "      <td>2023-04-27 08:07:16.645</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>panas_fatigue</td>\n",
       "      <td>TIKI_1A_E1</td>\n",
       "      <td>2023-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APbN</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>318</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>2023-04-27 08:07:17.559</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>panas_joviality1</td>\n",
       "      <td>TIKI_1A_E1</td>\n",
       "      <td>2023-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>APbN</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>319</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>2023-04-27 08:07:18.516</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>panas_fear1</td>\n",
       "      <td>TIKI_1A_E1</td>\n",
       "      <td>2023-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152831</th>\n",
       "      <td>M7TE</td>\n",
       "      <td>123</td>\n",
       "      <td>38</td>\n",
       "      <td>358</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>2025-03-23 22:51:06.659</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ta_behavioral_2</td>\n",
       "      <td>TIKI_8_E2_S2</td>\n",
       "      <td>2025-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152832</th>\n",
       "      <td>M7TE</td>\n",
       "      <td>123</td>\n",
       "      <td>38</td>\n",
       "      <td>359</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>2025-03-23 22:51:10.600</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>ta_kognitiv</td>\n",
       "      <td>TIKI_8_E2_S2</td>\n",
       "      <td>2025-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152833</th>\n",
       "      <td>M7TE</td>\n",
       "      <td>123</td>\n",
       "      <td>38</td>\n",
       "      <td>360</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>2025-03-23 22:51:14.085</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>ta_kognitiv_2</td>\n",
       "      <td>TIKI_8_E2_S2</td>\n",
       "      <td>2025-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152834</th>\n",
       "      <td>M7TE</td>\n",
       "      <td>123</td>\n",
       "      <td>38</td>\n",
       "      <td>357</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>2025-03-23 22:51:16.327</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ta_behavioral</td>\n",
       "      <td>TIKI_8_E2_S2</td>\n",
       "      <td>2025-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152835</th>\n",
       "      <td>M7TE</td>\n",
       "      <td>123</td>\n",
       "      <td>38</td>\n",
       "      <td>361</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>2025-03-23 22:51:18.250</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>physical_health</td>\n",
       "      <td>TIKI_8_E2_S2</td>\n",
       "      <td>2025-03-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152836 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        customer  questionnaire  study  question  element  \\\n",
       "0           APbN             56     25       315   1707.0   \n",
       "1           APbN             56     25       316   1720.0   \n",
       "2           APbN             56     25       317   1726.0   \n",
       "3           APbN             56     25       318   1734.0   \n",
       "4           APbN             56     25       319   1740.0   \n",
       "...          ...            ...    ...       ...      ...   \n",
       "1152831     M7TE            123     38       358   1968.0   \n",
       "1152832     M7TE            123     38       359   1976.0   \n",
       "1152833     M7TE            123     38       360   1983.0   \n",
       "1152834     M7TE            123     38       357   1961.0   \n",
       "1152835     M7TE            123     38       361   1988.0   \n",
       "\n",
       "                   quest_create  choice_id choice_text          quest_title  \\\n",
       "0       2023-04-27 08:07:14.748          1           1  panas_selfassurance   \n",
       "1       2023-04-27 08:07:15.748          7           7     panas_joviality2   \n",
       "2       2023-04-27 08:07:16.645          6           6        panas_fatigue   \n",
       "3       2023-04-27 08:07:17.559          7           7     panas_joviality1   \n",
       "4       2023-04-27 08:07:18.516          6           6          panas_fear1   \n",
       "...                         ...        ...         ...                  ...   \n",
       "1152831 2025-03-23 22:51:06.659          4           4      ta_behavioral_2   \n",
       "1152832 2025-03-23 22:51:10.600          5           5          ta_kognitiv   \n",
       "1152833 2025-03-23 22:51:14.085          5           5        ta_kognitiv_2   \n",
       "1152834 2025-03-23 22:51:16.327          4           4        ta_behavioral   \n",
       "1152835 2025-03-23 22:51:18.250          2           0      physical_health   \n",
       "\n",
       "        questionnaire_name quest_create_day  \n",
       "0               TIKI_1A_E1       2023-04-27  \n",
       "1               TIKI_1A_E1       2023-04-27  \n",
       "2               TIKI_1A_E1       2023-04-27  \n",
       "3               TIKI_1A_E1       2023-04-27  \n",
       "4               TIKI_1A_E1       2023-04-27  \n",
       "...                    ...              ...  \n",
       "1152831       TIKI_8_E2_S2       2025-03-23  \n",
       "1152832       TIKI_8_E2_S2       2025-03-23  \n",
       "1152833       TIKI_8_E2_S2       2025-03-23  \n",
       "1152834       TIKI_8_E2_S2       2025-03-23  \n",
       "1152835       TIKI_8_E2_S2       2025-03-23  \n",
       "\n",
       "[1152836 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_merged = pd.merge(answer_merged, df_monitoring, on = \"customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Calculate EMA coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = pd.merge(df_sess, df_monitoring, on = \"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess[['customer', 'sessionRun', 'quest_create', 'quest_complete', 'study',\n",
    "       'for_id', 'ema_id', 'study_version', 'status', 't20_post',\n",
    "       'ema_base_start', 'ema_base_end','ema_t20_start', 'ema_t20_end',\n",
    "       'ema_post_start', 'ema_post_end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.copy()\n",
    "df_sess[\"quest_create_day\"] = df_sess.quest_create.dt.normalize()\n",
    "df_sess[\"quest_complete_day\"] = df_sess.quest_complete.dt.normalize()\n",
    "\n",
    "df_sess[\"quest_create_hour\"] = df_sess.quest_create.dt.hour\n",
    "df_sess[\"quest_complete_hour\"] = df_sess.quest_complete.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of completed EMA beeps in first phase\n",
    "df_sess1 = df_sess.loc[df_sess.study.isin([24,25])]\n",
    "df_sess1 = df_sess1.copy()\n",
    "\n",
    "df_sess2 = df_sess.loc[df_sess.study.isin([33,34])]\n",
    "df_sess2 = df_sess2.copy()\n",
    "\n",
    "df_sess3 = df_sess.loc[df_sess.study.isin([38,39])]\n",
    "df_sess3 = df_sess3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess1['quest_complete_relative1'] = (df_sess1['quest_complete_day'] - df_sess1['ema_base_start']).dt.days\n",
    "\n",
    "\n",
    "sess_count1 = df_sess1.dropna(subset=[\"quest_create\"]).groupby(\"customer\")[\"quest_create\"].size()\\\n",
    ".reset_index()\n",
    "sess_count1 = sess_count1.rename(columns = {\"quest_create\":\"nquest_EMA1\"})\n",
    "\n",
    "# count number of completed EMA beeps in second phase\n",
    "sess_count2 = df_sess2.dropna(subset=[\"quest_create\"]).groupby(\"customer\")[\"quest_create\"].size()\\\n",
    ".reset_index()\n",
    "sess_count2 = sess_count2.rename(columns = {\"quest_create\":\"nquest_EMA2\"})\n",
    "\n",
    "# count number of completed EMA beeps in second phase\n",
    "sess_count3 = df_sess3.dropna(subset=[\"quest_create\"]).groupby(\"customer\")[\"quest_create\"].size()\\\n",
    ".reset_index()\n",
    "sess_count3 = sess_count3.rename(columns = {\"quest_create\":\"nquest_EMA3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.merge(sess_count1, on=['customer'], how='left')\n",
    "df_sess = df_sess.merge(sess_count2, on=['customer'], how='left')\n",
    "df_sess = df_sess.merge(sess_count3, on=['customer'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate auxiliary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_content = answer_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All entries have absolute_day_index <= 16.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_ema_content is your DataFrame and is already loaded\n",
    "\n",
    "### 1. Date and Time Manipulations\n",
    "\n",
    "df_ema_content['weekday'] = df_ema_content['quest_create'].dt.day_name()\n",
    "df_ema_content['createdAt_day'] = df_ema_content['quest_create'].dt.floor('D')\n",
    "\n",
    "date_cols = ['ema_base_start', 'ema_t20_start', 'ema_post_start']\n",
    "for col in date_cols:\n",
    "    df_ema_content[col] = pd.to_datetime(df_ema_content[col], dayfirst=True, errors='coerce')\n",
    "\n",
    "# **Additions Start Here**\n",
    "\n",
    "### 1a. Calculate \"Season\"\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "df_ema_content['season'] = df_ema_content['quest_create'].dt.month.apply(get_season)\n",
    "\n",
    "### 1b. Calculate \"Time of Day\"\n",
    "\n",
    "def get_time_of_day(hour):\n",
    "    if 5 <= hour < 8:\n",
    "        return 'Early Morning'\n",
    "    elif 8 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_ema_content['time_of_day'] = df_ema_content['quest_create'].dt.hour.apply(get_time_of_day)\n",
    "\n",
    "# **Additions End Here**\n",
    "\n",
    "### 2. Study Mapping and String Manipulation\n",
    "\n",
    "study_mapping = {\n",
    "    24: 0,\n",
    "    25: 0,\n",
    "    33: 1,\n",
    "    34: 1,\n",
    "    38: 2,\n",
    "    39: 2\n",
    "}\n",
    "df_ema_content['assess'] = df_ema_content['study'].map(study_mapping)\n",
    "df_ema_content['quest_title'] = df_ema_content['quest_title'].str.replace('_morning', '', regex=False)\n",
    "\n",
    "### 3. Weekend Indicator\n",
    "\n",
    "df_ema_content['weekend'] = df_ema_content['weekday'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "\n",
    "### 4. Extract Questionnaire Number\n",
    "\n",
    "df_ema_content['quest_nr'] = df_ema_content['questionnaire_name'].str.extract(r'(\\d+)')\n",
    "df_ema_content['quest_nr'] = df_ema_content['quest_nr'].astype(float)\n",
    "\n",
    "### 5. Count of Unique Questionnaires per Day\n",
    "\n",
    "df_ema_content['n_quest'] = df_ema_content.groupby(\n",
    "    ['study', 'customer', 'createdAt_day']\n",
    ")['questionnaire_name'].transform('nunique')\n",
    "\n",
    "### 6. Create Unique Day Identifier\n",
    "\n",
    "df_ema_content['quest_nr_str'] = df_ema_content['quest_nr'].fillna('unknown').astype(str)\n",
    "df_ema_content['unique_day_id'] = df_ema_content['createdAt_day'].dt.strftime('%Y%m%d') + '_' + df_ema_content['quest_nr_str']\n",
    "\n",
    "### 7. Compute Relative Start and End Dates per Phase and Customer\n",
    "\n",
    "# Calculate start and end dates for each phase\n",
    "phase_dates = df_ema_content.groupby(['customer', 'assess']).agg(\n",
    "    ema_relative_start=('createdAt_day', 'min'),\n",
    "    ema_relative_end=('createdAt_day', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Pivot the data to get phase-specific columns\n",
    "phase_dates_pivot = phase_dates.pivot(\n",
    "    index='customer', \n",
    "    columns='assess', \n",
    "    values=['ema_relative_start', 'ema_relative_end']\n",
    ")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "phase_dates_pivot.columns = [f\"{col[0]}_phase{int(col[1])}\" for col in phase_dates_pivot.columns]\n",
    "phase_dates_pivot = phase_dates_pivot.reset_index()\n",
    "\n",
    "# Merge the phase-specific dates back into the main DataFrame\n",
    "df_ema_content = df_ema_content.merge(\n",
    "    phase_dates_pivot, on='customer', how='left'\n",
    ")\n",
    "\n",
    "### 8. Calculate Absolute and Relative Day Indices\n",
    "\n",
    "# Create a mapping from 'assess' to the corresponding 'ema_relative_start_phaseX' column\n",
    "assess_to_start_col = {\n",
    "    0: 'ema_relative_start_phase0',\n",
    "    1: 'ema_relative_start_phase1',\n",
    "    2: 'ema_relative_start_phase2'\n",
    "}\n",
    "\n",
    "# Assign the appropriate 'ema_relative_start' based on 'assess'\n",
    "df_ema_content['ema_relative_start'] = df_ema_content.apply(\n",
    "    lambda row: row.get(assess_to_start_col.get(row['assess'], np.nan), np.nan), axis=1\n",
    ")\n",
    "\n",
    "# Calculate the Absolute Day Index\n",
    "df_ema_content['absolute_day_index'] = (\n",
    "    df_ema_content['createdAt_day'] - df_ema_content['ema_relative_start']\n",
    ").dt.days + 1\n",
    "\n",
    "# Calculate the Relative Day Index by ranking unique days per 'customer' and 'assess'\n",
    "df_ema_content['relative_day_index'] = df_ema_content.groupby(\n",
    "    ['customer', 'assess']\n",
    ")['createdAt_day'].rank(method='dense').astype(int)\n",
    "\n",
    "### 9. Remove Entries with Absolute Day Index > 16\n",
    "\n",
    "# Define the maximum allowed absolute day index\n",
    "max_allowed_days = 16\n",
    "\n",
    "# Filter the DataFrame to keep only entries with absolute_day_index <= 16\n",
    "df_ema_content = df_ema_content[df_ema_content['absolute_day_index'] <= max_allowed_days]\n",
    "\n",
    "# Optionally, reset the index after filtering\n",
    "df_ema_content.reset_index(drop=True, inplace=True)\n",
    "\n",
    "### 10. Check for High Absolute Day Indices (Post-Filtering)\n",
    "\n",
    "# Verify that no entries have absolute_day_index > 16\n",
    "high_indices = df_ema_content[df_ema_content['absolute_day_index'] > max_allowed_days]\n",
    "\n",
    "if not high_indices.empty:\n",
    "    print(\"Warning: Some entries still have unexpectedly high absolute day indices:\")\n",
    "    print(\"Customers with high absolute day indices:\")\n",
    "    print(high_indices['customer'].unique())\n",
    "else:\n",
    "    print(\"All entries have absolute_day_index <= 16.\")\n",
    "\n",
    "### 11. Calculate Questionnaire Counter\n",
    "\n",
    "df_unique = df_ema_content.drop_duplicates(subset=['customer', 'assess', 'unique_day_id']).copy()\n",
    "df_unique['questionnaire_counter'] = df_unique.groupby(['customer', 'assess']).cumcount() + 1\n",
    "\n",
    "df_ema_content = df_ema_content.merge(\n",
    "    df_unique[['customer', 'assess', 'unique_day_id', 'questionnaire_counter']],\n",
    "    on=['customer', 'assess', 'unique_day_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "### 12. Handle Missing Data\n",
    "\n",
    "df_ema_content['assess'] = df_ema_content['assess'].fillna('unknown')\n",
    "df_ema_content['absolute_day_index'] = df_ema_content['absolute_day_index'].where(\n",
    "    df_ema_content['ema_relative_start'].notna(), np.nan\n",
    ")\n",
    "\n",
    "# **Optional: View the Updated DataFrame**\n",
    "# print(df_ema_content.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_criteria = (df_ema_content['customer'] == 'UfMn') & \\\n",
    "                  (df_ema_content['study'] == 25) & \\\n",
    "                  (df_ema_content['quest_create'] > '2024-02-08')\n",
    "\n",
    "# Drop the entries that match the criteria of the wrong individual\n",
    "df_ema_content = df_ema_content[~filter_criteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_base = df_ema_content[[\"customer\", 'ema_relative_start_phase0', 'ema_relative_end_phase0', \n",
    "                             'ema_relative_start_phase1', 'ema_relative_end_phase1',\n",
    "                              'ema_relative_start_phase2', 'ema_relative_end_phase2']]\n",
    "df_ema_base = df_ema_base.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_ema = df_complete_filtered.merge(df_ema_base, on = \"customer\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_complete_ema_final = pd.concat([df_backup, df_complete_ema], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>type</th>\n",
       "      <th>startTimestamp</th>\n",
       "      <th>endTimestamp</th>\n",
       "      <th>doubleValue</th>\n",
       "      <th>longValue</th>\n",
       "      <th>booleanValue</th>\n",
       "      <th>dateValue</th>\n",
       "      <th>stringValue</th>\n",
       "      <th>userReliability</th>\n",
       "      <th>...</th>\n",
       "      <th>Heart_Rate_actual_days_with_data</th>\n",
       "      <th>Heart_Rate_data_coverage_per</th>\n",
       "      <th>ema_relative_start_phase0</th>\n",
       "      <th>ema_relative_end_phase0</th>\n",
       "      <th>ema_relative_start_phase1</th>\n",
       "      <th>ema_relative_end_phase1</th>\n",
       "      <th>ema_relative_start_phase2</th>\n",
       "      <th>ema_relative_end_phase2</th>\n",
       "      <th>for_id</th>\n",
       "      <th>ema_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>HeartRate</td>\n",
       "      <td>2023-05-17 18:58:01</td>\n",
       "      <td>2023-05-17 18:58:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>HeartRate</td>\n",
       "      <td>2023-05-17 19:18:03</td>\n",
       "      <td>2023-05-17 19:18:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>HeartRate</td>\n",
       "      <td>2023-05-17 19:28:00</td>\n",
       "      <td>2023-05-17 19:29:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>HeartRate</td>\n",
       "      <td>2023-05-17 19:38:27</td>\n",
       "      <td>2023-05-17 19:39:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>HeartRate</td>\n",
       "      <td>2023-05-17 19:48:20</td>\n",
       "      <td>2023-05-17 19:49:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer       type      startTimestamp        endTimestamp  doubleValue  \\\n",
       "3      4MLe  HeartRate 2023-05-17 18:58:01 2023-05-17 18:58:38          NaN   \n",
       "22     4MLe  HeartRate 2023-05-17 19:18:03 2023-05-17 19:18:42          NaN   \n",
       "23     4MLe  HeartRate 2023-05-17 19:28:00 2023-05-17 19:29:00          NaN   \n",
       "30     4MLe  HeartRate 2023-05-17 19:38:27 2023-05-17 19:39:30          NaN   \n",
       "40     4MLe  HeartRate 2023-05-17 19:48:20 2023-05-17 19:49:15          NaN   \n",
       "\n",
       "    longValue  booleanValue  dateValue stringValue  userReliability  ...  \\\n",
       "3        74.0         False        NaN         -99              NaN  ...   \n",
       "22       64.0         False        NaN         -99              NaN  ...   \n",
       "23       69.0         False        NaN         -99              NaN  ...   \n",
       "30       72.0         False        NaN         -99              NaN  ...   \n",
       "40       40.0         False        NaN         -99              NaN  ...   \n",
       "\n",
       "    Heart_Rate_actual_days_with_data  Heart_Rate_data_coverage_per  \\\n",
       "3                              343.0                     72.362869   \n",
       "22                             343.0                     72.362869   \n",
       "23                             343.0                     72.362869   \n",
       "30                             343.0                     72.362869   \n",
       "40                             343.0                     72.362869   \n",
       "\n",
       "   ema_relative_start_phase0 ema_relative_end_phase0  \\\n",
       "3                        NaT                     NaT   \n",
       "22                       NaT                     NaT   \n",
       "23                       NaT                     NaT   \n",
       "30                       NaT                     NaT   \n",
       "40                       NaT                     NaT   \n",
       "\n",
       "   ema_relative_start_phase1  ema_relative_end_phase1  \\\n",
       "3                        NaT                      NaT   \n",
       "22                       NaT                      NaT   \n",
       "23                       NaT                      NaT   \n",
       "30                       NaT                      NaT   \n",
       "40                       NaT                      NaT   \n",
       "\n",
       "    ema_relative_start_phase2 ema_relative_end_phase2 for_id ema_id  \n",
       "3                         NaT                     NaT   None   None  \n",
       "22                        NaT                     NaT   None   None  \n",
       "23                        NaT                     NaT   None   None  \n",
       "30                        NaT                     NaT   None   None  \n",
       "40                        NaT                     NaT   None   None  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete_ema_final.loc[df_complete_ema_final.type.isin([\"HeartRate\"])].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to keep only certain columns:\n",
    "keep_cols = [\n",
    "    'customer', 'type', 'startTimestamp_day', 'startTimestamp_hour',\n",
    "    'doubleValue', 'longValue', 'booleanValue', 'stringValue', 'ema_base_start'\n",
    "]\n",
    "df = df_complete_ema_final[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Define the type lists you had before ---\n",
    "double_value_types = [\n",
    "    'Steps',\n",
    "    'ActiveBurnedCalories',\n",
    "    'SPO2',\n",
    "    'ElevationGain',\n",
    "    'Latitude',\n",
    "    'Rmssd'\n",
    "]\n",
    "long_value_types = [\n",
    "    'HeartRate',\n",
    "    'ActivityTypeDetail2',\n",
    "    'ActivityTypeDetail1',\n",
    "    'ActivityType',\n",
    "    'FloorsClimbed',\n",
    "    'RespirationRateSleep'\n",
    "]\n",
    "string_value_types = [\n",
    "    'RawECGVoltage'\n",
    "]\n",
    "boolean_value_types = [\n",
    "    'WalkBinary',\n",
    "    'SleepAwakeBinary',\n",
    "    'SleepLightBinary',\n",
    "    'SleepBinary',\n",
    "    'SleepStateBinary',\n",
    "    'SleepDeepBinary',\n",
    "    'BikeBinary',\n",
    "    'ActiveBinary',\n",
    "    'RunBinary',\n",
    "    'SleepInBedBinary',\n",
    "    'AtrialFibrillationDetection',\n",
    "    'SleepREMBinary'\n",
    "]\n",
    "all_types = (\n",
    "    double_value_types +\n",
    "    long_value_types +\n",
    "    string_value_types +\n",
    "    boolean_value_types \n",
    ")\n",
    "\n",
    "\n",
    "# daily_agg_final now has columns:\n",
    "# [type, date, available_binary, available_hours, customer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter only needed types\n",
    "df = df[df['type'].isin(all_types)]\n",
    "df['type'] = df['type'].astype('category')\n",
    "df['customer'] = df['customer'].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m sub_df\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    106\u001b[0m daily_agg_final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def compute_availability_metrics(sub):\n",
    "    \"\"\"\n",
    "    Given a subset of the dataframe for a single participant,\n",
    "    return the daily availability metrics (including days with no data).\n",
    "    \"\"\"\n",
    "\n",
    "    def row_has_data(row):\n",
    "        t = row['type']\n",
    "        if t in double_value_types:\n",
    "            return not pd.isnull(row['doubleValue'])\n",
    "        elif t in long_value_types:\n",
    "            return not pd.isnull(row['longValue'])\n",
    "        elif t in string_value_types:\n",
    "            return not pd.isnull(row['stringValue'])\n",
    "        elif t in boolean_value_types:\n",
    "            return not pd.isnull(row['booleanValue'])\n",
    "        return False\n",
    "    \n",
    "    # 1) Mark rows that truly have data\n",
    "    sub['has_data'] = sub.apply(row_has_data, axis=1)\n",
    "\n",
    "    # 2) Aggregate at hourly level\n",
    "    hourly = (\n",
    "        sub.groupby(['type', 'startTimestamp_day', 'startTimestamp_hour'], observed=True)['has_data']\n",
    "           .any()  # yields boolean (True if at least one row has data in that hour)\n",
    "           .reset_index(name='has_data_in_hour')\n",
    "    )\n",
    "\n",
    "    # 3) Aggregate at daily level\n",
    "    daily_agg = (\n",
    "        hourly.groupby(['type', 'startTimestamp_day'], observed=True)\n",
    "              .agg(\n",
    "                  available_binary=('has_data_in_hour', 'any'),  # True if any hour had data\n",
    "                  available_hours=('has_data_in_hour', 'sum')    # count of hours with data\n",
    "              )\n",
    "              .reset_index()\n",
    "    )\n",
    "    # Convert boolean to int for binary, keep hours numeric\n",
    "    daily_agg['available_binary'] = daily_agg['available_binary'].astype(int)\n",
    "    daily_agg['available_hours'] = daily_agg['available_hours'].astype(int)\n",
    "\n",
    "    # ----------- GENERATE FULL DATE RANGE FOR THIS PARTICIPANT -----------\n",
    "    # We'll assume each participant subset has a column \"ema_base_start\"\n",
    "    # that indicates the earliest day we care about for that participant.\n",
    "\n",
    "    # Minimum \"ema_base_start\" across rows (in case there are multiple, or if it's the same).\n",
    "    min_date = sub['ema_base_start'].min()\n",
    "    # Maximum date actually appearing in sub's data\n",
    "    max_date = sub['startTimestamp_day'].max()\n",
    "\n",
    "    if pd.isnull(min_date) or pd.isnull(max_date):\n",
    "        # If there's no valid range, just return daily_agg as-is\n",
    "        # but make sure we add the 'customer' column\n",
    "        daily_agg['customer'] = sub['customer'].iloc[0]\n",
    "        return daily_agg\n",
    "\n",
    "    # Create a full daily date range from min_date to max_date\n",
    "    all_dates = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "\n",
    "    # Unique types present for this participant\n",
    "    unique_types = sub['type'].unique()\n",
    "\n",
    "    # Cartesian product of [all types] x [all dates]\n",
    "    all_combos = pd.MultiIndex.from_product(\n",
    "        [unique_types, all_dates], \n",
    "        names=['type', 'startTimestamp_day']\n",
    "    )\n",
    "    all_days_df = all_combos.to_frame(index=False)\n",
    "\n",
    "    # Merge daily_agg onto all possible (type, day) combos\n",
    "    daily_agg_full = pd.merge(\n",
    "        all_days_df,\n",
    "        daily_agg,\n",
    "        on=['type', 'startTimestamp_day'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill missing availability with 0\n",
    "    daily_agg_full['available_binary'] = daily_agg_full['available_binary'].fillna(0)\n",
    "    daily_agg_full['available_hours'] = daily_agg_full['available_hours'].fillna(0)\n",
    "\n",
    "    # Convert them to int\n",
    "    daily_agg_full['available_binary'] = daily_agg_full['available_binary'].astype(int)\n",
    "    daily_agg_full['available_hours'] = daily_agg_full['available_hours'].astype(int)\n",
    "\n",
    "    # Assign the participant ID\n",
    "    daily_agg_full['customer'] = sub['customer'].iloc[0]\n",
    "\n",
    "    return daily_agg_full\n",
    "\n",
    "# ------------------- Process in chunks by participant -------------------\n",
    "results = []\n",
    "all_customers = df['customer'].cat.categories  # or df['customer'].unique()\n",
    "\n",
    "for cust in all_customers:\n",
    "    sub_df = df[df['customer'] == cust].copy()\n",
    "    if sub_df.empty:\n",
    "        continue\n",
    "    sub_res = compute_availability_metrics(sub_df)\n",
    "    results.append(sub_res)\n",
    "\n",
    "    # Cleanup\n",
    "    del sub_df\n",
    "    gc.collect()\n",
    "\n",
    "daily_agg_final = pd.concat(results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_agg_final.available_binary.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_ema_final.startTimestamp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_ema_final.startTimestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_list_adherence = ['Steps',\n",
    " 'ActiveBurnedCalories',\n",
    " 'HeartRate',\n",
    " 'WalkBinary',\n",
    " 'ActivityTypeDetail2',\n",
    " 'ActivityTypeDetail1',\n",
    " 'ActivityType',\n",
    " 'RawECGVoltage',\n",
    " 'SPO2',\n",
    " 'SleepAwakeBinary',\n",
    " 'SleepLightBinary',\n",
    " 'SleepBinary',\n",
    " 'SleepStateBinary',\n",
    " 'SleepDeepBinary',\n",
    " 'BikeBinary',\n",
    " 'ActiveBinary',\n",
    " 'RunBinary',\n",
    " 'ElevationGain',\n",
    " 'FloorsClimbed',\n",
    " 'SleepInBedBinary',\n",
    " 'Latitude',\n",
    " 'AtrialFibrillationDetection',\n",
    " 'RespirationRateSleep',\n",
    " 'Rmssd',\n",
    " 'SleepREMBinary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate memory usage in bytes\n",
    "memory_usage_bytes = df_complete_ema_final.memory_usage(deep=True).sum()\n",
    "\n",
    "# Convert to megabytes\n",
    "memory_usage_mb = memory_usage_bytes / (1024 ** 2)\n",
    "\n",
    "# Convert to gigabytes\n",
    "memory_usage_gb = memory_usage_bytes / (1024 ** 3)\n",
    "\n",
    "# Convert to terabytes\n",
    "memory_usage_tb = memory_usage_bytes / (1024 ** 4)\n",
    "\n",
    "print(f\"Memory usage: {memory_usage_bytes} bytes\")\n",
    "print(f\"Memory usage: {memory_usage_mb:.2f} MB\")\n",
    "print(f\"Memory usage: {memory_usage_gb:.2f} GB\")\n",
    "print(f\"Memory usage: {memory_usage_tb:.2f} TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_path = raw_path + \"/backup_data_passive_actual.feather\"\n",
    "df_complete_ema_final.to_feather(backup_path)\n",
    "\n",
    "preprocessed_path_final = preprocessed_path + \"/backup_data_passive_actual.feather\"\n",
    "df_complete_ema_final.to_feather(preprocessed_path_final)\n",
    "\n",
    "\n",
    "with open(preprocessed_path + f'/ema_adherence_data.pkl', 'wb') as file:\n",
    "    pickle.dump(df_sess, file)\n",
    "    \n",
    "with open(preprocessed_path + f'/monitoring_data.pkl', 'wb') as file:\n",
    "    pickle.dump(df_monitoring, file)\n",
    "\n",
    "    \n",
    "with open(preprocessed_path + f'/ema_content.pkl', 'wb') as file:\n",
    "    pickle.dump(df_ema_content, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new CSV backup path\n",
    "#backup_path_csv = raw_path + \"/backup_data_passive_actual.csv\"\n",
    "#df_complete_ema.to_csv(backup_path_csv, index=False)\n",
    "\n",
    "# Export df_sess as CSV\n",
    "df_sess_csv_path = preprocessed_path + '/ema_adherence_data.csv'\n",
    "df_sess.to_csv(df_sess_csv_path, index=False)\n",
    "\n",
    "# Export df_monitoring as CSV\n",
    "df_monitoring_csv_path = preprocessed_path + '/monitoring_data.csv'\n",
    "df_monitoring.to_csv(df_monitoring_csv_path, index=False)\n",
    "\n",
    "# Export df_ema_content as CSV\n",
    "df_ema_content_csv_path = preprocessed_path + '/ema_content.csv'\n",
    "df_ema_content.to_csv(df_ema_content_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redcap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_redcap = pd.read_csv(redcap_path + \"FOR5187_DATA_2025-01-07_1511.csv\", low_memory=False)\n",
    "df_redcap_zert = pd.read_csv(redcap_path + \"ZERTIFIZIERUNGFOR518_DATA_2025-01-07_1518.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redcap_zert = df_redcap_zert[['for_id', 'redcap_event_name',\n",
    "       'basic_documentation_sheet_timestamp',  'age', 'gender','scid_cv_prim_cat',\n",
    "       'marital_status', 'partnership', 'graduation', 'profession', 'ema_start_date',\n",
    "       'years_of_education', 'employability', 'ses', 'ema_smartphone', 'ema_sleep', 'ema_watch', 'prior_treatment', 'ema_special_event', 'psychotropic', 'somatic_problems']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redcap = df_redcap[['for_id', 'redcap_event_name',\n",
    "       'basic_documentation_sheet_timestamp', 'age', 'gender', 'scid_cv_prim_cat',\n",
    "       'marital_status', 'partnership', 'graduation', 'profession', 'ema_start_date', \n",
    "       'years_of_education', 'employability', 'ses', 'ema_smartphone', 'ema_sleep', 'ema_watch','prior_treatment', 'ema_special_event', 'psychotropic', 'somatic_problems']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_redcap = pd.concat([df_redcap, df_redcap_zert],ignore_index=True)\n",
    "#df_redcap = pd.merge(df_redcap, df_t20,on='for_id', suffixes=('_base', '_t20'), how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by subject_id and merge rows\n",
    "df_redcap_merged = (\n",
    "    df_redcap\n",
    "    .groupby('for_id', as_index=False)\n",
    "    .agg({\n",
    "        'ema_watch': 'max',  # Takes the non-null value\n",
    "        **{col: 'first' for col in df_redcap.columns \n",
    "           if col not in ['for_id', 'ema_watch', 'redcap_event_name']}  # Keeps the first of other columns\n",
    "    })\n",
    ")\n",
    "\n",
    "# Optionally drop the 'redcap_event_name' column\n",
    "if 'redcap_event_name' in df_redcap_merged.columns:\n",
    "    df_redcap_merged = df_redcap_merged.drop(columns=['redcap_event_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gender_mapping = {\n",
    "    1: 'male',\n",
    "    2: 'female',\n",
    "    3: 'diverse',\n",
    "    4: 'no gender',\n",
    "    5: 'not specified'\n",
    "}\n",
    "\n",
    "scid_cv_cat_mapping = {\n",
    "    1: 'Depressive Disorder',\n",
    "    2: 'Specific Phobia',\n",
    "    3: 'Social Anxiety Disorder',\n",
    "    4: 'Agoraphobia and/or Panic Disorder',\n",
    "    5: 'Generalized Anxiety Disorder',\n",
    "    6: 'Obsessive-Compulsive Disorder',\n",
    "    7: 'Post-Traumatic Stress Disorder'\n",
    "}\n",
    "\n",
    "marital_status_mapping = {\n",
    "    1: 'single',\n",
    "    2: 'married/registered partnership',\n",
    "    3: 'divorced',\n",
    "    4: 'separated',\n",
    "    5: 'widowed',\n",
    "    6: 'other'\n",
    "}\n",
    "\n",
    "employability_mapping = {\n",
    "    0: 'employable',\n",
    "    1: 'unemployable (on sick leave)',\n",
    "    2: 'on disability pension',\n",
    "    3: 'on retirement pension',\n",
    "    4: 'other'\n",
    "}\n",
    "\n",
    "employability_mapping_simple = {\n",
    "    0: 'yes',\n",
    "    1: 'no',\n",
    "    2: 'no',\n",
    "    3: 'no',\n",
    "    4: 'no'\n",
    "}\n",
    "\n",
    "graduation_mapping = {\n",
    "    0: 'still in school',\n",
    "    1: 'no school degree',\n",
    "    2: 'elementary school degree or equivalent',\n",
    "    3: 'middle school degree or equivalent',\n",
    "    4: 'high school diploma/university entrance qualification',\n",
    "    5: 'other'\n",
    "}\n",
    "\n",
    "profession_mapping = {\n",
    "    0: 'still in training or studies',\n",
    "    1: 'no training degree',\n",
    "    2: 'vocational training, including technical school',\n",
    "    3: 'university or college degree',\n",
    "    4: 'other'\n",
    "}\n",
    "\n",
    "prior_treatment_mapping = {\n",
    "    0: 'no prior treatment',\n",
    "    1: 'outpatient psychotherapy',\n",
    "    2: 'inpatient or partial inpatient treatment/psychotherapy',\n",
    "    3: 'both',\n",
    "    4: 'yes'\n",
    "}\n",
    "\n",
    "prior_treatment_mapping_simple = {\n",
    "    0: 'no prior treatment',\n",
    "    1: 'prior psychotherapy',\n",
    "    2: 'prior inpatient',\n",
    "    3: 'prior inpatient',\n",
    "    4: 'prior psychotherapy'\n",
    "}\n",
    "\n",
    "psychotropic_medication_mapping = {\n",
    "    0: 'no',\n",
    "    1: 'yes'\n",
    "}\n",
    "somatic_mapping = {\n",
    "    0: 'no',\n",
    "    1: 'yes'\n",
    "}\n",
    "ema_smartphone_mapping = {\n",
    "    1: 'iPhone',\n",
    "    0: 'Android'\n",
    "}\n",
    "\n",
    "ema_special_event_mapping = {\n",
    "    0: 'usual',\n",
    "    1: 'special event'\n",
    "}\n",
    "def categorize_age(age):\n",
    "    if 18 <= age <= 24:\n",
    "        return 0\n",
    "    elif 25 <= age <= 34:\n",
    "        return 1\n",
    "    elif 35 <= age <= 44:\n",
    "        return 2\n",
    "    elif 45 <= age <= 54:\n",
    "        return 3\n",
    "    elif 55 <= age <= 64:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply mappings\n",
    "df_redcap_merged['gender_description'] = df_redcap_merged['gender'].map(gender_mapping)\n",
    "df_redcap_merged['scid_cv_description'] = df_redcap_merged['scid_cv_prim_cat'].map(scid_cv_cat_mapping)\n",
    "df_redcap_merged['marital_status_description'] = df_redcap_merged['marital_status'].map(marital_status_mapping)\n",
    "df_redcap_merged['employability_description'] = df_redcap_merged['employability'].map(employability_mapping)\n",
    "df_redcap_merged['employability_description_simple'] = df_redcap_merged['employability'].map(employability_mapping_simple)\n",
    "df_redcap_merged['prior_treatment_description_simple'] = df_redcap_merged['prior_treatment'].map(prior_treatment_mapping_simple)\n",
    "df_redcap_merged['graduation_description'] = df_redcap_merged['graduation'].map(graduation_mapping)\n",
    "df_redcap_merged['profession_description'] = df_redcap_merged['profession'].map(profession_mapping)\n",
    "df_redcap_merged['prior_treatment_description'] = df_redcap_merged['prior_treatment'].map(prior_treatment_mapping)\n",
    "df_redcap_merged['ema_smartphone_description'] = df_redcap_merged['ema_smartphone'].map(ema_smartphone_mapping)\n",
    "df_redcap_merged['ema_special_event_description'] = df_redcap_merged['ema_special_event'].map(ema_special_event_mapping)\n",
    "df_redcap_merged['age_description'] = df_redcap_merged['age'].apply(categorize_age)\n",
    "df_redcap_merged['somatic_description'] = df_redcap_merged['somatic_problems'].map(somatic_mapping)\n",
    "df_redcap_merged['psychotropic_description'] = df_redcap_merged['psychotropic'].map(psychotropic_medication_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_monitoring[\"for_id\"] = df_monitoring.for_id.str.strip()\n",
    "df_forid = df_monitoring[[\"for_id\",\"customer\"]]\n",
    "df_redcap = pd.merge(df_forid, df_redcap_merged, on=\"for_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_df = df_redcap.dropna(subset=['ema_start_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(preprocessed_path_freezed + f'/redcap_data.pkl', 'wb') as file:\n",
    "    pickle.dump(valid_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tiki]",
   "language": "python",
   "name": "conda-env-.conda-tiki-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
