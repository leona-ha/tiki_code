{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"TIKI: Data Compliance and Quality\"\n",
    "author: \"Leona Hammelrath\"\n",
    "date: \"10 April 2024\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "\n",
    "format: html\n",
    "execute:\n",
    "    eval: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "First Data for Passive Data Collection using Smartwatches and GPS from the PREACT Study. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Treatment personalization is highly discussed to counteract insufficient response rates in psychotherapy. In the quest for criteria allowing informed selection or adaptation, ambulatory assessment data (i.e. EMA, passive sensing)are a key component, as processes happening outside of therapy sessions can be depicted in high temporal and/or spatial resolution.\n",
    "\n",
    "PREACT is a multicenter prospective-longitudinal study investigating different predictors of non-response (i.e. EEG, fMRI) in around 500 patients undergoing cognitive behavioral therapy for internalizing disorders (https://forschungsgruppe5187.de/de). \n",
    "\n",
    "## Methods\n",
    "Patients can enroll for therapy-accompanying ambulatory assessment. They are provided with a customized study app and a state-of-the-art smartwatch collecting passive data like GPS and heart rate for up to 365 days. In parallel, three 14-day EMA phases (pre-, mid- and post-therapy) cover transdiagnostic (i.e. emotion regulation), contextual and therapy-related aspects.  \n",
    "\n",
    "Here, we present first results on data compliance and quality for the passive sensing data as well as EMA assessments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import regex as re\n",
    "\n",
    "\n",
    "from server_config import datapath, proj_sheet,preprocessed_path, raw_path, redcap_path, preprocessed_path_freezed\n",
    "\n",
    "today = date.today().strftime(\"%d%m%Y\")\n",
    "today_day = pd.to_datetime('today').normalize()\n",
    "\n",
    "df_monitoring = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{proj_sheet}/export?format=csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual passive + ema_data\n",
    "datapath1 = raw_path + f\"/export_tiki_{today}/\"\n",
    "file_pattern = os.path.join(datapath1, \"epoch_part*.csv\")\n",
    "file_list = glob.glob(file_pattern)\n",
    "file_list.sort()\n",
    "df_complete = pd.concat((pd.read_csv(f, encoding=\"latin-1\", low_memory=False) for f in file_list), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1 Import epoch level passive + GPS data\n",
    "\n",
    "# Extract customer identifier\n",
    "df_complete[\"customer\"] = df_complete.customer.str.split(\"@\").str.get(0)\n",
    "df_complete[\"customer\"] = df_complete[\"customer\"].str[:4]\n",
    "\n",
    "# Convert timestamps from milliseconds since epoch to datetime\n",
    "df_complete[\"startTimestamp\"] = pd.to_datetime(df_complete[\"startTimestamp\"], unit='ms')\n",
    "df_complete[\"endTimestamp\"] = pd.to_datetime(df_complete[\"endTimestamp\"], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Timezone offset adjustment\n",
    "\n",
    "# Fill NaN timezoneOffsets with 0\n",
    "df_complete['timezoneOffset_filled'] = df_complete['timezoneOffset'].fillna(0)\n",
    "\n",
    "# Convert timezoneOffset to timedelta\n",
    "df_complete['timezoneOffset_timedelta'] = pd.to_timedelta(df_complete['timezoneOffset_filled'], unit='ms')\n",
    "\n",
    "# Adjust the timestamps\n",
    "df_complete['startTimestamp'] = df_complete['startTimestamp'] + df_complete['timezoneOffset_timedelta']\n",
    "df_complete['endTimestamp'] = df_complete['endTimestamp'] + df_complete['timezoneOffset_timedelta']\n",
    "\n",
    "# Calculate the duration after adjusting timestamps\n",
    "df_complete[\"start_end\"] = df_complete[\"endTimestamp\"] - df_complete[\"startTimestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Timedelta to total seconds (float)\n",
    "df_complete['start_end'] = df_complete['start_end'].dt.total_seconds()\n",
    "\n",
    "# Extract date and hour\n",
    "df_complete[\"startTimestamp_day\"] = df_complete.startTimestamp.dt.normalize()\n",
    "df_complete[\"startTimestamp_hour\"] = df_complete.startTimestamp.dt.hour\n",
    "\n",
    "# Drop temporary columns if not needed\n",
    "df_complete.drop(columns=['timezoneOffset_filled', 'timezoneOffset_timedelta'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-02-17 01:22:52.060000')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete['startTimestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with backup data\n",
    "backup_path = preprocessed_path + \"/backup_data_passive_actual.feather\"\n",
    "\n",
    "df_backup = pd.read_feather(backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-02-10 01:14:05.060000')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_backup['startTimestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_timestamp = df_backup['startTimestamp'].max()\n",
    "\n",
    "# Filter the second dataframe to include only entries after the latest timestamp\n",
    "df_complete_filtered = df_complete[df_complete['startTimestamp'] > latest_timestamp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_filtered = df_complete_filtered.drop(columns=['valueType', 'createdAt', 'source', \n",
    "                                                              'trustworthiness', 'medicalGrade', 'generation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = df_monitoring.copy()\n",
    "df_monitoring.rename(columns = {\"Pseudonym\": \"customer\", \"EMA_ID\": \"ema_id\", \"Status\": \"status\",\n",
    "                                \"Studienversion\":\"study_version\", \"FOR_ID\":\"for_id\", \n",
    "                           \"Start EMA Baseline\": \"ema_base_start\", \"Ende EMA Baseline\": \"ema_base_end\", \n",
    "                           \"Freischaltung/ Start EMA T20\": \"ema_t20_start\",\"Ende EMA T20\":\"ema_t20_end\", \n",
    "                                \"Freischaltung/ Start EMA Post\":\"ema_post_start\",\n",
    "                               \"Ende EMA Post\":\"ema_post_end\", \"T20=Post\":\"t20_post\" }, inplace=True)\n",
    "\n",
    "df_monitoring = df_monitoring[['for_id', 'ema_id', 'customer', 'study_version', 'status',\n",
    "       't20_post', 'ema_base_start', 'ema_base_end', 'ema_t20_start', 'ema_t20_end',\n",
    "       'ema_post_start', 'ema_post_end']]\n",
    "\n",
    "df_monitoring[\"customer\"] = df_monitoring[\"customer\"].str[:4]\n",
    "df_monitoring[\"for_id\"] = df_monitoring.for_id.str.strip()\n",
    "\n",
    "df_monitoring[\"ema_base_start\"] = pd.to_datetime(df_monitoring[\"ema_base_start\"], dayfirst=True)\n",
    "df_monitoring[\"ema_base_end\"] = pd.to_datetime(df_monitoring[\"ema_base_end\"], dayfirst=True)\n",
    "\n",
    "df_monitoring_short = df_monitoring[[\"customer\", \"for_id\",\"ema_id\",\"status\", \"study_version\", \"ema_base_start\",\"ema_base_end\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_filtered= df_complete_filtered.merge(df_monitoring_short, on=\"customer\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = [\"booleanValue\", \"stringValue\",\"customer\", \"type\", \"status\", \"study_version\"] \n",
    "\n",
    "# Fill NaN values with -99 for the specified columns\n",
    "for col in object_cols:\n",
    "    df_complete_filtered[col] = df_complete_filtered[col].fillna(-99)\n",
    "\n",
    "# Convert \"booleanValue\" to boolean\n",
    "df_complete_filtered['booleanValue'] = df_complete_filtered['booleanValue'].apply(lambda x: bool(x) if x != -99 else False)\n",
    "\n",
    "# Convert \"stringValue\", \"status\", \"study_version\" to string using StringDtype\n",
    "df_complete_filtered['stringValue'] = df_complete_filtered['stringValue'].astype('string')\n",
    "df_complete_filtered['status'] = df_complete_filtered['status'].astype('string')\n",
    "df_complete_filtered['study_version'] = df_complete_filtered['study_version'].astype('string')\n",
    "df_complete_filtered['customer'] = df_complete_filtered['customer'].astype('string')\n",
    "df_complete_filtered['type'] = df_complete_filtered['type'].astype('string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_groups = {\n",
    "    'GPS': [\"Latitude\"],\n",
    "    # Add more groups as needed\n",
    "    'Activity': [\"Steps\"],\n",
    "    'Sleep': [\"SleepBinary\"],\n",
    "    'Heart_Rate': [\"HeartRate\"]\n",
    "    # Add more groups as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load and match relevant data from separate .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from separate csv files\n",
    "session = pd.read_csv(datapath1 + \"questionnaireSession.csv\",low_memory=False)\n",
    "answers = pd.read_csv(datapath1 + \"answers.csv\", low_memory=False)\n",
    "choice = pd.read_csv(datapath1 + \"choice.csv\",low_memory=False)\n",
    "questions = pd.read_csv(datapath1 + \"questions.csv\",low_memory=False)\n",
    "questionnaire = pd.read_csv(datapath1 + \"questionnaires.csv\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session data\n",
    "session[\"user\"] = session[\"user\"].str[:4]\n",
    "session.rename(columns = {\"user\":\"customer\",\"completedAt\": \"quest_complete\", \"createdAt\": \"quest_create\", \"expirationTimestamp\": \"quest_expir\"}, inplace=True)\n",
    "session[\"quest_create\"] = (pd.to_datetime(session[\"quest_create\"],unit='ms'))\n",
    "session[\"quest_complete\"] = (pd.to_datetime(session[\"quest_complete\"],unit='ms'))\n",
    "\n",
    "df_sess = session[[\"customer\", \"sessionRun\", \"quest_create\", \"quest_complete\", \"study\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer data \n",
    "answers[\"user\"] = answers[\"user\"].str[:4]\n",
    "answers = answers[[\"user\", \"questionnaire\", \"study\", \"question\",\"element\", \"createdAt\"]]\n",
    "answers[\"createdAt\"] = (pd.to_datetime(answers[\"createdAt\"],unit='ms'))\n",
    "answers.rename(columns={\"user\":\"customer\", \"createdAt\": \"quest_create\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item description data\n",
    "choice = choice[[\"element\", \"choice_id\", \"text\", \"question\"]]\n",
    "choice.rename(columns={\"text\":\"choice_text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question description data\n",
    "questions = questions[[\"id\", \"title\"]]\n",
    "questions.rename(columns={\"id\":\"question\",\"title\":\"quest_title\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire = questionnaire[[\"id\", \"name\"]]\n",
    "questionnaire.rename(columns={\"id\":\"questionnaire\",\"name\":\"questionnaire_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_merged = pd.merge(answers, choice, on= [\"question\",\"element\"])\n",
    "answer_merged = pd.merge(answer_merged, questions, on= \"question\")\n",
    "answer_merged = pd.merge(answer_merged, questionnaire, on= \"questionnaire\")\n",
    "answer_merged[\"quest_create_day\"] = answer_merged.quest_create.dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_merged = pd.merge(answer_merged, df_monitoring, on = \"customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Calculate EMA coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = pd.merge(df_sess, df_monitoring, on = \"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess[['customer', 'sessionRun', 'quest_create', 'quest_complete', 'study',\n",
    "       'for_id', 'ema_id', 'study_version', 'status', 't20_post',\n",
    "       'ema_base_start', 'ema_base_end','ema_t20_start', 'ema_t20_end',\n",
    "       'ema_post_start', 'ema_post_end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.copy()\n",
    "df_sess[\"quest_create_day\"] = df_sess.quest_create.dt.normalize()\n",
    "df_sess[\"quest_complete_day\"] = df_sess.quest_complete.dt.normalize()\n",
    "\n",
    "df_sess[\"quest_create_hour\"] = df_sess.quest_create.dt.hour\n",
    "df_sess[\"quest_complete_hour\"] = df_sess.quest_complete.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of completed EMA beeps in first phase\n",
    "df_sess1 = df_sess.loc[df_sess.study.isin([24,25])]\n",
    "df_sess1 = df_sess1.copy()\n",
    "\n",
    "df_sess2 = df_sess.loc[df_sess.study.isin([33,34])]\n",
    "df_sess2 = df_sess2.copy()\n",
    "\n",
    "df_sess3 = df_sess.loc[df_sess.study.isin([38,39])]\n",
    "df_sess3 = df_sess3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess1['quest_complete_relative1'] = (df_sess1['quest_complete_day'] - df_sess1['ema_base_start']).dt.days\n",
    "\n",
    "\n",
    "sess_count1 = df_sess1.dropna(subset=[\"quest_create\"]).groupby(\"customer\")[\"quest_create\"].size()\\\n",
    ".reset_index()\n",
    "sess_count1 = sess_count1.rename(columns = {\"quest_create\":\"nquest_EMA1\"})\n",
    "\n",
    "# count number of completed EMA beeps in second phase\n",
    "sess_count2 = df_sess2.dropna(subset=[\"quest_create\"]).groupby(\"customer\")[\"quest_create\"].size()\\\n",
    ".reset_index()\n",
    "sess_count2 = sess_count2.rename(columns = {\"quest_create\":\"nquest_EMA2\"})\n",
    "\n",
    "# count number of completed EMA beeps in second phase\n",
    "sess_count3 = df_sess3.dropna(subset=[\"quest_create\"]).groupby(\"customer\")[\"quest_create\"].size()\\\n",
    ".reset_index()\n",
    "sess_count3 = sess_count3.rename(columns = {\"quest_create\":\"nquest_EMA3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.merge(sess_count1, on=['customer'], how='left')\n",
    "df_sess = df_sess.merge(sess_count2, on=['customer'], how='left')\n",
    "df_sess = df_sess.merge(sess_count3, on=['customer'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate auxiliary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_content = answer_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All entries have absolute_day_index <= 16.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_ema_content is your DataFrame and is already loaded\n",
    "\n",
    "### 1. Date and Time Manipulations\n",
    "\n",
    "df_ema_content['weekday'] = df_ema_content['quest_create'].dt.day_name()\n",
    "df_ema_content['createdAt_day'] = df_ema_content['quest_create'].dt.floor('D')\n",
    "\n",
    "date_cols = ['ema_base_start', 'ema_t20_start', 'ema_post_start']\n",
    "for col in date_cols:\n",
    "    df_ema_content[col] = pd.to_datetime(df_ema_content[col], dayfirst=True, errors='coerce')\n",
    "\n",
    "# **Additions Start Here**\n",
    "\n",
    "### 1a. Calculate \"Season\"\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "df_ema_content['season'] = df_ema_content['quest_create'].dt.month.apply(get_season)\n",
    "\n",
    "### 1b. Calculate \"Time of Day\"\n",
    "\n",
    "def get_time_of_day(hour):\n",
    "    if 5 <= hour < 8:\n",
    "        return 'Early Morning'\n",
    "    elif 8 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_ema_content['time_of_day'] = df_ema_content['quest_create'].dt.hour.apply(get_time_of_day)\n",
    "\n",
    "# **Additions End Here**\n",
    "\n",
    "### 2. Study Mapping and String Manipulation\n",
    "\n",
    "study_mapping = {\n",
    "    24: 0,\n",
    "    25: 0,\n",
    "    33: 1,\n",
    "    34: 1,\n",
    "    38: 2,\n",
    "    39: 2\n",
    "}\n",
    "df_ema_content['assess'] = df_ema_content['study'].map(study_mapping)\n",
    "df_ema_content['quest_title'] = df_ema_content['quest_title'].str.replace('_morning', '', regex=False)\n",
    "\n",
    "### 3. Weekend Indicator\n",
    "\n",
    "df_ema_content['weekend'] = df_ema_content['weekday'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "\n",
    "### 4. Extract Questionnaire Number\n",
    "\n",
    "df_ema_content['quest_nr'] = df_ema_content['questionnaire_name'].str.extract(r'(\\d+)')\n",
    "df_ema_content['quest_nr'] = df_ema_content['quest_nr'].astype(float)\n",
    "\n",
    "### 5. Count of Unique Questionnaires per Day\n",
    "\n",
    "df_ema_content['n_quest'] = df_ema_content.groupby(\n",
    "    ['study', 'customer', 'createdAt_day']\n",
    ")['questionnaire_name'].transform('nunique')\n",
    "\n",
    "### 6. Create Unique Day Identifier\n",
    "\n",
    "df_ema_content['quest_nr_str'] = df_ema_content['quest_nr'].fillna('unknown').astype(str)\n",
    "df_ema_content['unique_day_id'] = df_ema_content['createdAt_day'].dt.strftime('%Y%m%d') + '_' + df_ema_content['quest_nr_str']\n",
    "\n",
    "### 7. Compute Relative Start and End Dates per Phase and Customer\n",
    "\n",
    "# Calculate start and end dates for each phase\n",
    "phase_dates = df_ema_content.groupby(['customer', 'assess']).agg(\n",
    "    ema_relative_start=('createdAt_day', 'min'),\n",
    "    ema_relative_end=('createdAt_day', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Pivot the data to get phase-specific columns\n",
    "phase_dates_pivot = phase_dates.pivot(\n",
    "    index='customer', \n",
    "    columns='assess', \n",
    "    values=['ema_relative_start', 'ema_relative_end']\n",
    ")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "phase_dates_pivot.columns = [f\"{col[0]}_phase{int(col[1])}\" for col in phase_dates_pivot.columns]\n",
    "phase_dates_pivot = phase_dates_pivot.reset_index()\n",
    "\n",
    "# Merge the phase-specific dates back into the main DataFrame\n",
    "df_ema_content = df_ema_content.merge(\n",
    "    phase_dates_pivot, on='customer', how='left'\n",
    ")\n",
    "\n",
    "### 8. Calculate Absolute and Relative Day Indices\n",
    "\n",
    "# Create a mapping from 'assess' to the corresponding 'ema_relative_start_phaseX' column\n",
    "assess_to_start_col = {\n",
    "    0: 'ema_relative_start_phase0',\n",
    "    1: 'ema_relative_start_phase1',\n",
    "    2: 'ema_relative_start_phase2'\n",
    "}\n",
    "\n",
    "# Assign the appropriate 'ema_relative_start' based on 'assess'\n",
    "df_ema_content['ema_relative_start'] = df_ema_content.apply(\n",
    "    lambda row: row.get(assess_to_start_col.get(row['assess'], np.nan), np.nan), axis=1\n",
    ")\n",
    "\n",
    "# Calculate the Absolute Day Index\n",
    "df_ema_content['absolute_day_index'] = (\n",
    "    df_ema_content['createdAt_day'] - df_ema_content['ema_relative_start']\n",
    ").dt.days + 1\n",
    "\n",
    "# Calculate the Relative Day Index by ranking unique days per 'customer' and 'assess'\n",
    "df_ema_content['relative_day_index'] = df_ema_content.groupby(\n",
    "    ['customer', 'assess']\n",
    ")['createdAt_day'].rank(method='dense').astype(int)\n",
    "\n",
    "### 9. Remove Entries with Absolute Day Index > 16\n",
    "\n",
    "# Define the maximum allowed absolute day index\n",
    "max_allowed_days = 16\n",
    "\n",
    "# Filter the DataFrame to keep only entries with absolute_day_index <= 16\n",
    "df_ema_content = df_ema_content[df_ema_content['absolute_day_index'] <= max_allowed_days]\n",
    "\n",
    "# Optionally, reset the index after filtering\n",
    "df_ema_content.reset_index(drop=True, inplace=True)\n",
    "\n",
    "### 10. Check for High Absolute Day Indices (Post-Filtering)\n",
    "\n",
    "# Verify that no entries have absolute_day_index > 16\n",
    "high_indices = df_ema_content[df_ema_content['absolute_day_index'] > max_allowed_days]\n",
    "\n",
    "if not high_indices.empty:\n",
    "    print(\"Warning: Some entries still have unexpectedly high absolute day indices:\")\n",
    "    print(\"Customers with high absolute day indices:\")\n",
    "    print(high_indices['customer'].unique())\n",
    "else:\n",
    "    print(\"All entries have absolute_day_index <= 16.\")\n",
    "\n",
    "### 11. Calculate Questionnaire Counter\n",
    "\n",
    "df_unique = df_ema_content.drop_duplicates(subset=['customer', 'assess', 'unique_day_id']).copy()\n",
    "df_unique['questionnaire_counter'] = df_unique.groupby(['customer', 'assess']).cumcount() + 1\n",
    "\n",
    "df_ema_content = df_ema_content.merge(\n",
    "    df_unique[['customer', 'assess', 'unique_day_id', 'questionnaire_counter']],\n",
    "    on=['customer', 'assess', 'unique_day_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "### 12. Handle Missing Data\n",
    "\n",
    "df_ema_content['assess'] = df_ema_content['assess'].fillna('unknown')\n",
    "df_ema_content['absolute_day_index'] = df_ema_content['absolute_day_index'].where(\n",
    "    df_ema_content['ema_relative_start'].notna(), np.nan\n",
    ")\n",
    "\n",
    "# **Optional: View the Updated DataFrame**\n",
    "# print(df_ema_content.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_criteria = (df_ema_content['customer'] == 'UfMn') & \\\n",
    "                  (df_ema_content['study'] == 25) & \\\n",
    "                  (df_ema_content['quest_create'] > '2024-02-08')\n",
    "\n",
    "# Drop the entries that match the criteria of the wrong individual\n",
    "df_ema_content = df_ema_content[~filter_criteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_base = df_ema_content[[\"customer\", 'ema_relative_start_phase0', 'ema_relative_end_phase0', \n",
    "                             'ema_relative_start_phase1', 'ema_relative_end_phase1',\n",
    "                              'ema_relative_start_phase2', 'ema_relative_end_phase2']]\n",
    "df_ema_base = df_ema_base.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_ema = df_complete_filtered.merge(df_ema_base, on = \"customer\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_complete_ema_final = pd.concat([df_backup, df_complete_ema], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-05-17 18:44:00')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete_ema_final.startTimestamp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-02-17 01:22:52.060000')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete_ema_final.startTimestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44236127061 bytes\n",
      "Memory usage: 42186.86 MB\n",
      "Memory usage: 41.20 GB\n",
      "Memory usage: 0.04 TB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate memory usage in bytes\n",
    "memory_usage_bytes = df_complete_ema_final.memory_usage(deep=True).sum()\n",
    "\n",
    "# Convert to megabytes\n",
    "memory_usage_mb = memory_usage_bytes / (1024 ** 2)\n",
    "\n",
    "# Convert to gigabytes\n",
    "memory_usage_gb = memory_usage_bytes / (1024 ** 3)\n",
    "\n",
    "# Convert to terabytes\n",
    "memory_usage_tb = memory_usage_bytes / (1024 ** 4)\n",
    "\n",
    "print(f\"Memory usage: {memory_usage_bytes} bytes\")\n",
    "print(f\"Memory usage: {memory_usage_mb:.2f} MB\")\n",
    "print(f\"Memory usage: {memory_usage_gb:.2f} GB\")\n",
    "print(f\"Memory usage: {memory_usage_tb:.2f} TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_path = raw_path + \"/backup_data_passive_actual.feather\"\n",
    "df_complete_ema_final.to_feather(backup_path)\n",
    "\n",
    "preprocessed_path_final = preprocessed_path + \"/backup_data_passive_actual.feather\"\n",
    "df_complete_ema_final.to_feather(preprocessed_path_final)\n",
    "\n",
    "\n",
    "with open(preprocessed_path + f'/ema_adherence_data.pkl', 'wb') as file:\n",
    "    pickle.dump(df_sess, file)\n",
    "    \n",
    "with open(preprocessed_path + f'/monitoring_data.pkl', 'wb') as file:\n",
    "    pickle.dump(df_monitoring, file)\n",
    "\n",
    "    \n",
    "with open(preprocessed_path + f'/ema_content.pkl', 'wb') as file:\n",
    "    pickle.dump(df_ema_content, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new CSV backup path\n",
    "#backup_path_csv = raw_path + \"/backup_data_passive_actual.csv\"\n",
    "#df_complete_ema.to_csv(backup_path_csv, index=False)\n",
    "\n",
    "# Export df_sess as CSV\n",
    "df_sess_csv_path = preprocessed_path + '/ema_adherence_data.csv'\n",
    "df_sess.to_csv(df_sess_csv_path, index=False)\n",
    "\n",
    "# Export df_monitoring as CSV\n",
    "df_monitoring_csv_path = preprocessed_path + '/monitoring_data.csv'\n",
    "df_monitoring.to_csv(df_monitoring_csv_path, index=False)\n",
    "\n",
    "# Export df_ema_content as CSV\n",
    "df_ema_content_csv_path = preprocessed_path + '/ema_content.csv'\n",
    "df_ema_content.to_csv(df_ema_content_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>type</th>\n",
       "      <th>startTimestamp</th>\n",
       "      <th>endTimestamp</th>\n",
       "      <th>doubleValue</th>\n",
       "      <th>longValue</th>\n",
       "      <th>booleanValue</th>\n",
       "      <th>dateValue</th>\n",
       "      <th>stringValue</th>\n",
       "      <th>userReliability</th>\n",
       "      <th>...</th>\n",
       "      <th>Heart_Rate_actual_days_with_data</th>\n",
       "      <th>Heart_Rate_data_coverage_per</th>\n",
       "      <th>ema_relative_start_phase0</th>\n",
       "      <th>ema_relative_end_phase0</th>\n",
       "      <th>ema_relative_start_phase1</th>\n",
       "      <th>ema_relative_end_phase1</th>\n",
       "      <th>ema_relative_start_phase2</th>\n",
       "      <th>ema_relative_end_phase2</th>\n",
       "      <th>for_id</th>\n",
       "      <th>ema_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>Steps</td>\n",
       "      <td>2023-05-17 18:44:00</td>\n",
       "      <td>2023-05-17 18:45:00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>ActiveBurnedCalories</td>\n",
       "      <td>2023-05-17 18:44:00</td>\n",
       "      <td>2023-05-17 18:45:00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>CoveredDistance</td>\n",
       "      <td>2023-05-17 18:44:00</td>\n",
       "      <td>2023-05-17 18:45:00</td>\n",
       "      <td>4.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>HeartRate</td>\n",
       "      <td>2023-05-17 18:58:01</td>\n",
       "      <td>2023-05-17 18:58:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4MLe</td>\n",
       "      <td>ActiveBurnedCalories</td>\n",
       "      <td>2023-05-17 19:04:00</td>\n",
       "      <td>2023-05-17 19:05:00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>72.362869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer                  type      startTimestamp        endTimestamp  \\\n",
       "0     4MLe                 Steps 2023-05-17 18:44:00 2023-05-17 18:45:00   \n",
       "1     4MLe  ActiveBurnedCalories 2023-05-17 18:44:00 2023-05-17 18:45:00   \n",
       "2     4MLe       CoveredDistance 2023-05-17 18:44:00 2023-05-17 18:45:00   \n",
       "3     4MLe             HeartRate 2023-05-17 18:58:01 2023-05-17 18:58:38   \n",
       "4     4MLe  ActiveBurnedCalories 2023-05-17 19:04:00 2023-05-17 19:05:00   \n",
       "\n",
       "   doubleValue  longValue  booleanValue  dateValue stringValue  \\\n",
       "0         6.00        NaN         False        NaN         -99   \n",
       "1         0.14        NaN         False        NaN         -99   \n",
       "2         4.62        NaN         False        NaN         -99   \n",
       "3          NaN       74.0         False        NaN         -99   \n",
       "4         0.36        NaN         False        NaN         -99   \n",
       "\n",
       "   userReliability  ...  Heart_Rate_actual_days_with_data  \\\n",
       "0              NaN  ...                             343.0   \n",
       "1              NaN  ...                             343.0   \n",
       "2              NaN  ...                             343.0   \n",
       "3              NaN  ...                             343.0   \n",
       "4              NaN  ...                             343.0   \n",
       "\n",
       "   Heart_Rate_data_coverage_per ema_relative_start_phase0  \\\n",
       "0                     72.362869                       NaT   \n",
       "1                     72.362869                       NaT   \n",
       "2                     72.362869                       NaT   \n",
       "3                     72.362869                       NaT   \n",
       "4                     72.362869                       NaT   \n",
       "\n",
       "  ema_relative_end_phase0 ema_relative_start_phase1  ema_relative_end_phase1  \\\n",
       "0                     NaT                       NaT                      NaT   \n",
       "1                     NaT                       NaT                      NaT   \n",
       "2                     NaT                       NaT                      NaT   \n",
       "3                     NaT                       NaT                      NaT   \n",
       "4                     NaT                       NaT                      NaT   \n",
       "\n",
       "   ema_relative_start_phase2 ema_relative_end_phase2 for_id ema_id  \n",
       "0                        NaT                     NaT   None   None  \n",
       "1                        NaT                     NaT   None   None  \n",
       "2                        NaT                     NaT   None   None  \n",
       "3                        NaT                     NaT   None   None  \n",
       "4                        NaT                     NaT   None   None  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete_ema_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redcap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_redcap = pd.read_csv(redcap_path + \"FOR5187_DATA_2025-01-07_1511.csv\", low_memory=False)\n",
    "df_redcap_zert = pd.read_csv(redcap_path + \"ZERTIFIZIERUNGFOR518_DATA_2025-01-07_1518.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redcap_zert = df_redcap_zert[['for_id', 'redcap_event_name',\n",
    "       'basic_documentation_sheet_timestamp',  'age', 'gender','scid_cv_prim_cat',\n",
    "       'marital_status', 'partnership', 'graduation', 'profession', 'ema_start_date',\n",
    "       'years_of_education', 'employability', 'ses', 'ema_smartphone', 'ema_sleep', 'ema_watch', 'prior_treatment', 'ema_special_event', 'psychotropic', 'somatic_problems']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redcap = df_redcap[['for_id', 'redcap_event_name',\n",
    "       'basic_documentation_sheet_timestamp', 'age', 'gender', 'scid_cv_prim_cat',\n",
    "       'marital_status', 'partnership', 'graduation', 'profession', 'ema_start_date', \n",
    "       'years_of_education', 'employability', 'ses', 'ema_smartphone', 'ema_sleep', 'ema_watch','prior_treatment', 'ema_special_event', 'psychotropic', 'somatic_problems']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_redcap = pd.concat([df_redcap, df_redcap_zert],ignore_index=True)\n",
    "#df_redcap = pd.merge(df_redcap, df_t20,on='for_id', suffixes=('_base', '_t20'), how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2511, 21)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_redcap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_redcap.for_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by subject_id and merge rows\n",
    "df_redcap_merged = (\n",
    "    df_redcap\n",
    "    .groupby('for_id', as_index=False)\n",
    "    .agg({\n",
    "        'ema_watch': 'max',  # Takes the non-null value\n",
    "        **{col: 'first' for col in df_redcap.columns \n",
    "           if col not in ['for_id', 'ema_watch', 'redcap_event_name']}  # Keeps the first of other columns\n",
    "    })\n",
    ")\n",
    "\n",
    "# Optionally drop the 'redcap_event_name' column\n",
    "if 'redcap_event_name' in df_redcap_merged.columns:\n",
    "    df_redcap_merged = df_redcap_merged.drop(columns=['redcap_event_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gender_mapping = {\n",
    "    1: 'male',\n",
    "    2: 'female',\n",
    "    3: 'diverse',\n",
    "    4: 'no gender',\n",
    "    5: 'not specified'\n",
    "}\n",
    "\n",
    "scid_cv_cat_mapping = {\n",
    "    1: 'Depressive Disorder',\n",
    "    2: 'Specific Phobia',\n",
    "    3: 'Social Anxiety Disorder',\n",
    "    4: 'Agoraphobia and/or Panic Disorder',\n",
    "    5: 'Generalized Anxiety Disorder',\n",
    "    6: 'Obsessive-Compulsive Disorder',\n",
    "    7: 'Post-Traumatic Stress Disorder'\n",
    "}\n",
    "\n",
    "marital_status_mapping = {\n",
    "    1: 'single',\n",
    "    2: 'married/registered partnership',\n",
    "    3: 'divorced',\n",
    "    4: 'separated',\n",
    "    5: 'widowed',\n",
    "    6: 'other'\n",
    "}\n",
    "\n",
    "employability_mapping = {\n",
    "    0: 'employable',\n",
    "    1: 'unemployable (on sick leave)',\n",
    "    2: 'on disability pension',\n",
    "    3: 'on retirement pension',\n",
    "    4: 'other'\n",
    "}\n",
    "\n",
    "employability_mapping_simple = {\n",
    "    0: 'yes',\n",
    "    1: 'no',\n",
    "    2: 'no',\n",
    "    3: 'no',\n",
    "    4: 'no'\n",
    "}\n",
    "\n",
    "graduation_mapping = {\n",
    "    0: 'still in school',\n",
    "    1: 'no school degree',\n",
    "    2: 'elementary school degree or equivalent',\n",
    "    3: 'middle school degree or equivalent',\n",
    "    4: 'high school diploma/university entrance qualification',\n",
    "    5: 'other'\n",
    "}\n",
    "\n",
    "profession_mapping = {\n",
    "    0: 'still in training or studies',\n",
    "    1: 'no training degree',\n",
    "    2: 'vocational training, including technical school',\n",
    "    3: 'university or college degree',\n",
    "    4: 'other'\n",
    "}\n",
    "\n",
    "prior_treatment_mapping = {\n",
    "    0: 'no prior treatment',\n",
    "    1: 'outpatient psychotherapy',\n",
    "    2: 'inpatient or partial inpatient treatment/psychotherapy',\n",
    "    3: 'both',\n",
    "    4: 'yes'\n",
    "}\n",
    "\n",
    "prior_treatment_mapping_simple = {\n",
    "    0: 'no prior treatment',\n",
    "    1: 'prior psychotherapy',\n",
    "    2: 'prior inpatient',\n",
    "    3: 'prior inpatient',\n",
    "    4: 'prior psychotherapy'\n",
    "}\n",
    "\n",
    "psychotropic_medication_mapping = {\n",
    "    0: 'no',\n",
    "    1: 'yes'\n",
    "}\n",
    "somatic_mapping = {\n",
    "    0: 'no',\n",
    "    1: 'yes'\n",
    "}\n",
    "ema_smartphone_mapping = {\n",
    "    1: 'iPhone',\n",
    "    0: 'Android'\n",
    "}\n",
    "\n",
    "ema_special_event_mapping = {\n",
    "    0: 'usual',\n",
    "    1: 'special event'\n",
    "}\n",
    "def categorize_age(age):\n",
    "    if 18 <= age <= 24:\n",
    "        return 0\n",
    "    elif 25 <= age <= 34:\n",
    "        return 1\n",
    "    elif 35 <= age <= 44:\n",
    "        return 2\n",
    "    elif 45 <= age <= 54:\n",
    "        return 3\n",
    "    elif 55 <= age <= 64:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply mappings\n",
    "df_redcap_merged['gender_description'] = df_redcap_merged['gender'].map(gender_mapping)\n",
    "df_redcap_merged['scid_cv_description'] = df_redcap_merged['scid_cv_prim_cat'].map(scid_cv_cat_mapping)\n",
    "df_redcap_merged['marital_status_description'] = df_redcap_merged['marital_status'].map(marital_status_mapping)\n",
    "df_redcap_merged['employability_description'] = df_redcap_merged['employability'].map(employability_mapping)\n",
    "df_redcap_merged['employability_description_simple'] = df_redcap_merged['employability'].map(employability_mapping_simple)\n",
    "df_redcap_merged['prior_treatment_description_simple'] = df_redcap_merged['prior_treatment'].map(prior_treatment_mapping_simple)\n",
    "df_redcap_merged['graduation_description'] = df_redcap_merged['graduation'].map(graduation_mapping)\n",
    "df_redcap_merged['profession_description'] = df_redcap_merged['profession'].map(profession_mapping)\n",
    "df_redcap_merged['prior_treatment_description'] = df_redcap_merged['prior_treatment'].map(prior_treatment_mapping)\n",
    "df_redcap_merged['ema_smartphone_description'] = df_redcap_merged['ema_smartphone'].map(ema_smartphone_mapping)\n",
    "df_redcap_merged['ema_special_event_description'] = df_redcap_merged['ema_special_event'].map(ema_special_event_mapping)\n",
    "df_redcap_merged['age_description'] = df_redcap_merged['age'].apply(categorize_age)\n",
    "df_redcap_merged['somatic_description'] = df_redcap_merged['somatic_problems'].map(somatic_mapping)\n",
    "df_redcap_merged['psychotropic_description'] = df_redcap_merged['psychotropic'].map(psychotropic_medication_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_monitoring[\"for_id\"] = df_monitoring.for_id.str.strip()\n",
    "df_forid = df_monitoring[[\"for_id\",\"customer\"]]\n",
    "df_redcap = pd.merge(df_forid, df_redcap_merged, on=\"for_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_df = df_redcap.dropna(subset=['ema_start_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(preprocessed_path_freezed + f'/redcap_data.pkl', 'wb') as file:\n",
    "    pickle.dump(valid_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tiki]",
   "language": "python",
   "name": "conda-env-.conda-tiki-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
