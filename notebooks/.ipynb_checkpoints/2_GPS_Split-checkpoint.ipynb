{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8daf40e",
   "metadata": {},
   "source": [
    "# GPS Data Preprocessing and Analysis\n",
    "\n",
    "This notebook preprocesses GPS data in accordance with Mueller et al. (2021) and performs analyses by splitting the data into two weekly segments.\n",
    "\n",
    "1. **Load Data**: Load necessary data from pickle files.\n",
    "2. **Preprocess Data**: Filter and transform the data for analysis.\n",
    "3. **Split Data**: Divide the data into two parts based on the `ema_base_start` variable.\n",
    "4. **Analyze Data**: Perform analyses separately for the two parts.\n",
    "5. **Calculate Internal Consistency**: Evaluate the internal consistency of features between the first and second weeks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafae9c",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "This section loads the necessary data from the pickle files and initializes relevant parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca7ad8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'db2' from 'gps_features' (/Users/leonahammelrath/FU_Psychoinformatik/Github/tiki_code/library/gps_features.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m library_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibrary\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     11\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(library_path)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgps_features\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m haversine, db2, identify_home,calculate_metrics, calculate_transition_time, calculate_internal_consistency\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'db2' from 'gps_features' (/Users/leonahammelrath/FU_Psychoinformatik/Github/tiki_code/library/gps_features.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from config import datapath\n",
    "\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "library_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'library'))\n",
    "sys.path.append(library_path)\n",
    "\n",
    "from gps_features import haversine, identify_home,calculate_metrics, calculate_transition_time, calculate_internal_consistency\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import statistics \n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr\n",
    "from math import radians, cos, sin, asin, sqrt, log\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns \n",
    "sns.set_context(\"notebook\", rc={\"axes.labelsize\": 14, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': True})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = dt.date.today().strftime(\"%d%m%Y\")\n",
    "today_day = pd.to_datetime('today').normalize()\n",
    "\n",
    "with open(datapath + f'ema_data.pkl', 'rb') as file:\n",
    "    df_active = pickle.load(file)\n",
    "\n",
    "with open(datapath + f'gps_data.pkl', 'rb') as file:\n",
    "    df_gps = pickle.load(file)\n",
    "    \n",
    "with open(datapath + f'passive_data.pkl', 'rb') as file:\n",
    "    df_passive = pickle.load(file)\n",
    "\n",
    "with open(datapath + f'monitoring_data.pkl', 'rb') as file:\n",
    "    df_monitoring = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2957c",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33abe348",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hour_daily = 8\n",
    "min_days_data = 12\n",
    "\n",
    "#stationary filtering\n",
    "max_distance = 150 \n",
    "speed_limit = 1.4  # Max allowed speed in m/s\n",
    "\n",
    "# DBSCAN\n",
    "kms_per_radian = 6371.0088 # equitorial radius of the earth = 6,371.1 \n",
    "epsilon = 0.03/kms_per_radian\n",
    "min_samples = 5\n",
    "\n",
    "# Kmeans\n",
    "DKmeans = 500\n",
    "\n",
    "#home featurenight\n",
    "min_nights_obs = 4\n",
    "min_f_home = 0.5  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e0a04",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Filter and transform the data as per the requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393397ff",
   "metadata": {},
   "source": [
    "## Prepare the PD dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb56f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_passive = df_passive.loc[df_passive.status.isin([\"Abgeschlossen\", \"Post_Erhebung_1\",\n",
    "                                                             \"Erhebung_2_aktiv\",\"Post_Erhebung_2\"])]\n",
    "df_passive = df_passive[df_passive['startTimestamp'] <= df_passive['ema_end_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steps = df_passive.loc[df_passive.type.isin(['Steps'])][[\"customer\",\"type\",\"startTimestamp\",  \"endTimestamp\", \"doubleValue\",\n",
    "                                                      'startTimestamp_day','startTimestamp_hour', 'ema_start_date','ema_end_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns for clarity\n",
    "df_steps = df_steps.rename(columns={\n",
    "    'startTimestamp': 'startTimestamp_steps',\n",
    "    'endTimestamp': 'endTimestamp_steps',\n",
    "    'startTimestamp_day': 'day_steps',  # Keeping one 'Day' column\n",
    "    'startTimestamp_hour_Latitude': 'hour_steps'  # Keeping one 'Hour' column\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steps[\"daily_steps\"] = df_steps.groupby([\"customer\", \"day_steps\"])[\"doubleValue\"].transform(\"sum\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e185a",
   "metadata": {},
   "source": [
    "## Prepare the GPS dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92362062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for participants that have finished 1. EMA Phase\n",
    "\n",
    "df_gps = df_gps.loc[df_gps.status.isin([\"Abgeschlossen\", \"Post_Erhebung_1\",\n",
    "                                                             \"Erhebung_2_aktiv\",\"Post_Erhebung_2\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8271ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps = df_gps[df_gps['startTimestamp'] <= df_gps['ema_end_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee024f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps.customer.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d925d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_gps.pivot_table(\n",
    "    index=[\"customer\", \"startTimestamp\", \"ema_start_date\"],\n",
    "    columns=\"type\",\n",
    "    values=[\"doubleValue\", \"startTimestamp_hour\", \"startTimestamp_day\"],\n",
    "    aggfunc='first'  # Using 'first' since each type should theoretically have only one entry per customer and timestamp\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex in columns\n",
    "df_int.columns = ['_'.join(col).strip() for col in df_int.columns.values]\n",
    "\n",
    "df_int = df_int.rename_axis(None, axis=1).reset_index()\n",
    "\n",
    "# Drop redundant day and hour columns for longitude (assuming latitude day and hour are kept)\n",
    "df_int = df_int.drop(columns=[\n",
    "    'startTimestamp_day_Longitude',\n",
    "    'startTimestamp_hour_Longitude'\n",
    "])\n",
    "\n",
    "# Rename the columns for clarity\n",
    "df_int = df_int.rename(columns={\n",
    "    'doubleValue_Latitude': 'Latitude',\n",
    "    'doubleValue_Longitude': 'Longitude',\n",
    "    'startTimestamp_day_Latitude': 'day_gps',  # Keeping one 'Day' column\n",
    "    'startTimestamp_hour_Latitude': 'hour_gps'  # Keeping one 'Hour' column\n",
    "})\n",
    "\n",
    "df_int['weekday'] = df_int['day_gps'].dt.day_name()\n",
    "df_int[\"n_hours\"] = df_int.groupby([\"customer\", \"day_gps\"])[\"hour_gps\"].transform(\"nunique\")\n",
    "df_int[\"n_data\"] = df_int.groupby(\"customer\")[\"Longitude\"].transform(\"size\")\n",
    "df_int[\"n_data_day\"] = df_int.groupby([\"customer\", \"day_gps\"])[\"Longitude\"].transform(\"size\")\n",
    "df_int[\"n_data_hour\"] = df_int.groupby([\"customer\", \"hour_gps\"])[\"Longitude\"].transform(\"size\")\n",
    "\n",
    "df_int = df_int.loc[df_int[\"n_hours\"] >= min_hour_daily]\n",
    "df_int[\"n_days_8\"] = df_int.groupby(\"customer\")[\"day_gps\"].transform(\"nunique\")\n",
    "df_int = df_int.loc[df_int[\"n_days_8\"] >= min_days_data]\n",
    "\n",
    "# Ensure your DataFrame is sorted by customer and day\n",
    "df_int = df_int.sort_values(by=['customer', 'day_gps'])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x='customer', y='n_data', data=df_int)\n",
    "plt.title('Number of GPS points per ID',fontsize=14)\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "\n",
    "#plt.savefig(\"barplot_high_quality.png\", dpi=300, format='png', bbox_inches='tight')\n",
    "# Showing the plot\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x='customer', y='n_data_day', data=df_int)\n",
    "plt.title('Number of data per day per customer')\n",
    "plt.ylabel('Number of data per day')\n",
    "plt.xlabel('Customer ID')\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x='hour_gps', y='n_data_hour', data=df_int)\n",
    "plt.title('Number of data per day per hour')\n",
    "plt.ylabel('Number of data per hour')\n",
    "plt.xlabel('Hour')\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debec53b",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "Divide the `df_int` dataframe into two parts based on the `ema_base_start` variable. The first part covers the first week of data, and the second part covers the following week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int.customer.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'day_gps' and 'ema_base_start' to datetime if not already\n",
    "df_int['day_gps'] = pd.to_datetime(df_int['day_gps'])\n",
    "df_int['ema_start_date'] = pd.to_datetime(df_int['ema_start_date'])\n",
    "\n",
    "# Define the time boundaries for the first and second week\n",
    "df_int['first_week_end'] = df_int['ema_start_date'] + pd.Timedelta(days=7)\n",
    "df_int['second_week_end'] = df_int['ema_start_date'] + pd.Timedelta(days=14)\n",
    "\n",
    "# Filter data for the first and second week\n",
    "first_week_df = df_int[(df_int['day_gps'] >= df_int['ema_start_date']) & \n",
    "                       (df_int['day_gps'] < df_int['first_week_end'])]\n",
    "\n",
    "second_week_df = df_int[(df_int['day_gps'] >= df_int['first_week_end']) & \n",
    "                        (df_int['day_gps'] < df_int['second_week_end'])]\n",
    "\n",
    "# Example analysis (e.g., plotting or statistical calculations) for the first week\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x='customer', y='n_data', data=first_week_df)\n",
    "plt.title('Number of GPS points per ID - First Week', fontsize=14)\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98718f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x='customer', y='n_data', data=second_week_df)\n",
    "plt.title('Number of GPS points per ID - Second Week', fontsize=14)\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c86784",
   "metadata": {},
   "source": [
    "## Analyze Data\n",
    "\n",
    "Perform separate analyses on the first and second week dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weekly_data(df):\n",
    "    df_speed = df.copy()\n",
    "\n",
    "    daily_transition_times = calculate_transition_time(df_speed, group_by=['customer', 'day_gps'])\n",
    "    general_transition_times = calculate_transition_time(df_speed, group_by=['customer'])\n",
    "\n",
    "    # Merge the daily and total metrics on 'customer'\n",
    "    merged_transition = pd.merge(daily_transition_times, general_transition_times, on='customer', suffixes=('_daily', '_total'))\n",
    "    df_speed = pd.merge(df_speed, merged_transition, on=[\"customer\", \"day_gps\"])\n",
    "\n",
    "    return df_speed\n",
    "\n",
    "# Apply the function to both first_week_df and second_week_df\n",
    "df_speed_first = process_weekly_data(first_week_df)\n",
    "df_speed_second = process_weekly_data(second_week_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_distance_time_speed(df, speed_limit, max_distance):\n",
    "    # Initialize columns to store calculated values\n",
    "    df['distance'] = np.nan\n",
    "    df['time_diff'] = np.nan\n",
    "    df['speed'] = np.nan\n",
    "\n",
    "    # Calculating distance, time difference, and speed for each customer independently\n",
    "    for customer in df['customer'].unique():\n",
    "        mask = df['customer'] == customer\n",
    "\n",
    "        df.loc[mask, 'distance'] = np.concatenate([\n",
    "            haversine(\n",
    "                df.loc[mask, 'Longitude'].values[:-1], df.loc[mask, 'Latitude'].values[:-1],\n",
    "                df.loc[mask, 'Longitude'].values[1:], df.loc[mask, 'Latitude'].values[1:]\n",
    "            ),\n",
    "            [0]\n",
    "        ])\n",
    "\n",
    "        df.loc[mask, 'time_diff'] = df.loc[mask, 'startTimestamp'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "        # Avoid division by zero and replace NaN if time_diff is 0\n",
    "        df.loc[mask, 'speed'] = df.loc[mask, 'distance'] / df.loc[mask, 'time_diff'].replace(0, np.nan)\n",
    "\n",
    "    # Creating the stationary DataFrame\n",
    "    stationary_df = df[(df['speed'] < speed_limit) & (df['distance'] < max_distance)]\n",
    "    \n",
    "    return stationary_df\n",
    "\n",
    "\n",
    "# Apply the calculate_distance_time_speed function to the resulting dataframes\n",
    "stationary_df_first = calculate_distance_time_speed(df_speed_first, speed_limit, max_distance)\n",
    "stationary_df_second = calculate_distance_time_speed(df_speed_second, speed_limit, max_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the clustering function\n",
    "def apply_clustering(df, epsilon, min_samples):\n",
    "    def db2(x):\n",
    "        clustering_model = DBSCAN(eps=epsilon, min_samples=min_samples, metric=\"haversine\")\n",
    "        cluster_labels = clustering_model.fit_predict(x[['Longitude', 'Latitude']].apply(np.radians))\n",
    "        return pd.DataFrame({'cluster_100m': cluster_labels})\n",
    "    \n",
    "    # Group by 'customer' and apply clustering function\n",
    "    geodata_cluster_df = df.groupby('customer').apply(lambda x: db2(x)).reset_index()\n",
    "    return geodata_cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18af9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the apply_clustering function to the stationary dataframes\n",
    "geodata_cluster_df_first = apply_clustering(stationary_df_first, epsilon, min_samples)\n",
    "geodata_cluster_df_second = apply_clustering(stationary_df_second, epsilon, min_samples)\n",
    "\n",
    "# Merge the clusters with the main dataframes\n",
    "geodata_clusters_first = pd.concat([stationary_df_first.reset_index(drop=True), geodata_cluster_df_first['cluster_100m']], axis=1)\n",
    "geodata_clusters_second = pd.concat([stationary_df_second.reset_index(drop=True), geodata_cluster_df_second['cluster_100m']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70419a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function\n",
    "def plot_cluster_counts(geodata_clusters, title):\n",
    "    # Initializing a new DataFrame to store processed data\n",
    "    plot_data = pd.DataFrame()\n",
    "\n",
    "    # Calculating the count of \"-\" values per customer\n",
    "    plot_data['negative_count'] = geodata_clusters[geodata_clusters['cluster_100m'] == -1].groupby('customer').size()\n",
    "\n",
    "    # Calculating the count of non \"-\" values per customer\n",
    "    plot_data['positive_count'] = geodata_clusters[geodata_clusters['cluster_100m'] != -1].groupby('customer').size()\n",
    "\n",
    "    # Filling NaN with 0s (for customers with no \"-\" values)\n",
    "    plot_data = plot_data.fillna(0)\n",
    "\n",
    "    # Plotting\n",
    "    ax = plot_data.plot(kind='bar', stacked=True, figsize=(10, 6), color=['salmon', 'cornflowerblue'], width=0.8)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.ylabel('')\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Adjusting the legend\n",
    "    plt.legend([\"Noise\", \"Assigned to Cluster\"], loc='upper right')\n",
    "\n",
    "    plt.savefig(f'{title}.png', dpi=300)\n",
    "    # Showing the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the results for first week\n",
    "plot_cluster_counts(geodata_clusters_first, f'Results of DBScan Clustering for First Week with eps={epsilon} and min_samples={min_samples}')\n",
    "\n",
    "# Plotting the results for second week\n",
    "plot_cluster_counts(geodata_clusters_second, f'Results of DBScan Clustering for Second Week with eps={epsilon} and min_samples={min_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out noise points from geodata_clusters_first and geodata_clusters_second\n",
    "geodata_clusters_first = geodata_clusters_first[geodata_clusters_first['cluster_100m'] != -1]\n",
    "geodata_clusters_second = geodata_clusters_second[geodata_clusters_second['cluster_100m'] != -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd065e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_clusters_first.customer.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_clusters_second.customer.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function for unique clusters\n",
    "def plot_unique_clusters(geodata_clusters, title):\n",
    "    # Count unique clusters per customer\n",
    "    unique_clusters = geodata_clusters.groupby('customer')['cluster_100m'].nunique()\n",
    "    \n",
    "    # Plotting\n",
    "    unique_clusters.plot(kind='bar', figsize=(10, 6), color='skyblue')\n",
    "    plt.xlabel('Customer')\n",
    "    plt.ylabel('Number of Unique Clusters')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)  # Rotate labels to avoid overlap, adjust as necessary\n",
    "    plt.tight_layout()  # Adjusts plot to ensure everything fits without overlap\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_unique_clusters(geodata_clusters_first, f'Number of Unique Clusters per Customer for First Week')\n",
    "plot_unique_clusters(geodata_clusters_second, f'Number of Unique Clusters per Customer for Second Week')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique IDs for clusters for first week using .loc\n",
    "geodata_clusters_first.loc[:, 'clusterID'] = geodata_clusters_first['customer'].astype(str) + '00' + geodata_clusters_first['cluster_100m'].astype(str)\n",
    "\n",
    "# Generate unique IDs for clusters for second week using .loc\n",
    "geodata_clusters_second.loc[:, 'clusterID'] = geodata_clusters_second['customer'].astype(str) + '00' + geodata_clusters_second['cluster_100m'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a4719",
   "metadata": {},
   "source": [
    "## Generate Home Location from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca88966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_night_clusters(geodata_clusters, min_nights_obs, min_f_home):\n",
    "    # Filter data for night hours (midnight to 6:00 am)\n",
    "    geodata_night = geodata_clusters.loc[(geodata_clusters['hour_gps'] >= 0) & (geodata_clusters['hour_gps'] < 6)].copy()\n",
    "\n",
    "    # Find the mode of clusterID per user during night hours\n",
    "    geodata_night['home'] = geodata_night.groupby('customer')['clusterID'].transform(lambda x: statistics.mode(x))\n",
    "\n",
    "    # Calculating various metrics to validate the home cluster\n",
    "    geodata_night['nights_with_obs'] = geodata_night.groupby('customer')['day_gps'].transform('nunique')\n",
    "    geodata_night['night_obs'] = geodata_night.groupby('customer')['day_gps'].transform('size')\n",
    "\n",
    "    # Finding the frequency of the mode\n",
    "    geodata_night['n_home'] = geodata_night.groupby('customer')['home'].transform(lambda x: x.value_counts().iloc[0])\n",
    "    geodata_night['f_home'] = geodata_night['n_home'] / geodata_night['night_obs']\n",
    "\n",
    "    # Updating the 'home' label based on conditions\n",
    "    geodata_night['home'] = geodata_night.apply(\n",
    "        lambda x: x['home'] if x['nights_with_obs'] >= min_nights_obs and x['f_home'] > min_f_home else None, axis=1\n",
    "    )\n",
    "\n",
    "    # Extracting a mapping of userID to home cluster\n",
    "    user_home_mapping = geodata_night[['customer', 'home']].drop_duplicates()\n",
    "\n",
    "    # Merging back to the full dataset\n",
    "    geodata_clusters = pd.merge(geodata_clusters, user_home_mapping, on='customer', how='left')\n",
    "    geodata_clusters['home'] = geodata_clusters['home'].replace([None], np.nan)\n",
    "\n",
    "    return geodata_clusters, geodata_night\n",
    "\n",
    "# Apply the function to both first week and second week data\n",
    "geodata_clusters_first, geodata_night_first = analyze_night_clusters(geodata_clusters_first, min_nights_obs, min_f_home)\n",
    "geodata_clusters_second, geodata_night_second = analyze_night_clusters(geodata_clusters_second, min_nights_obs, min_f_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c129f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b45139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_and_merge_metrics(geodata_clusters):\n",
    "    # Calculate general and daily entropy metrics\n",
    "    general_entropy = calculate_metrics(geodata_clusters, group_by=['customer'])\n",
    "    daily_entropy = calculate_metrics(geodata_clusters, group_by=['customer', 'day_gps'])\n",
    "\n",
    "    # Merge the daily and general metrics on 'customer'\n",
    "    merged_metrics = pd.merge(daily_entropy, general_entropy, on='customer', suffixes=('_daily', '_total'))\n",
    "    geodata_clusters = pd.merge(geodata_clusters, merged_metrics, on=[\"customer\", \"day_gps\"])\n",
    "\n",
    "    return geodata_clusters\n",
    "\n",
    "# Apply the metric calculation and merging function to both first week and second week data\n",
    "geodata_clusters_first = calculate_and_merge_metrics(geodata_clusters_first)\n",
    "geodata_clusters_second = calculate_and_merge_metrics(geodata_clusters_second)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66594b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c646dc60",
   "metadata": {},
   "source": [
    "## Merge with Activity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_and_deduplicate_geodata(geodata_clusters):\n",
    "    columns_to_keep = ['customer', 'home','day_gps', 'n_hours', 'n_data', 'n_data_day', 'n_data_hour',\n",
    "                       'n_days_8', 'transition_time_daily', 'transition_time_total',\n",
    "                       'distance', 'time_diff', 'speed', 'cluster_100m', 'clusterID', 'home',\n",
    "                       'raw_entropy_daily', 'normalized_entropy_daily', 'total_distance_daily',\n",
    "                       'percentage_time_at_home_daily', 'num_unique_clusters_daily',\n",
    "                       'num_total_clusters_daily', 'raw_entropy_total',\n",
    "                       'normalized_entropy_total', 'total_distance_total',\n",
    "                       'percentage_time_at_home_total', 'num_unique_clusters_total',\n",
    "                       'num_total_clusters_total']\n",
    "\n",
    "    # Reduce the dataframe to the specified columns\n",
    "    geodata_cluster_red = geodata_clusters[columns_to_keep]\n",
    "\n",
    "    # Drop duplicates based on 'customer' and 'day_gps'\n",
    "    geodata_cluster_red = geodata_cluster_red.drop_duplicates(subset=['customer', 'day_gps'])\n",
    "\n",
    "    return geodata_cluster_red\n",
    "\n",
    "# Apply the function to both first week and second week data\n",
    "geodata_cluster_red_first = reduce_and_deduplicate_geodata(geodata_clusters_first)\n",
    "geodata_cluster_red_second = reduce_and_deduplicate_geodata(geodata_clusters_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_cluster_red = pd.merge(geodata_cluster_red_first, geodata_cluster_red_second, on=\"customer\", how=\"inner\",suffixes=('_first', '_second'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7077484",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_cluster_red = geodata_cluster_red.drop_duplicates(subset=['customer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2234b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_cluster_red[[\"home_first\", \"home_second\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df951b4",
   "metadata": {},
   "source": [
    "Es bleiben nur 7 Personen Ã¼brig, bei denen in Woche 1+2 das gleiche Home Cluster identifiert wurde!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to calculate internal consistency for\n",
    "features = ['n_data_day', 'n_data_hour','total_distance_daily', 'total_distance_total',\n",
    "            'num_unique_clusters_daily','num_unique_clusters_total',\n",
    "            'num_total_clusters_daily', 'num_total_clusters_total','percentage_time_at_home_daily', \n",
    "            'percentage_time_at_home_total', 'raw_entropy_daily', 'normalized_entropy_daily',\n",
    "           'raw_entropy_total', 'normalized_entropy_total']\n",
    "\n",
    "# Assume geodata_cluster_merged is your merged dataframe\n",
    "# Calculate the internal consistency between the first and second week\n",
    "correlations = calculate_internal_consistency(geodata_cluster_red, features)\n",
    "\n",
    "# Print the results\n",
    "for feature, correlation in correlations.items():\n",
    "    print(f\"Correlation for {feature} between first and second week: {correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['customer',\n",
    "    'daily_steps', \n",
    "    'n_data_day', \n",
    "    'total_distance_daily', \n",
    "    'num_unique_clusters_total', \n",
    "    'num_total_clusters_total',\n",
    "    'num_total_clusters_daily',\n",
    "    'num_unique_clusters_daily',\n",
    "    'percentage_time_at_home_daily',\n",
    "    'percentage_time_at_home_total'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_merged = activity_merged[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_merged_numeric = activity_merged.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b216e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log transform the right-skewed variables\n",
    "skewed_variables = ['daily_steps','n_data_day', 'total_distance_daily', \n",
    "                    'num_unique_clusters_total', 'num_total_clusters_total',\n",
    "                    'num_total_clusters_daily', 'num_unique_clusters_daily',\n",
    "                    'percentage_time_at_home_daily', 'percentage_time_at_home_total']\n",
    "\n",
    "for variable in skewed_variables:\n",
    "    activity_merged_numeric[f'{variable}_log'] = np.log1p(activity_merged_numeric[variable])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation(df, columns):\n",
    "    \"\"\"\n",
    "    Calculate correlation coefficients and p-values for pairs of columns in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the data.\n",
    "    columns (list): List of column names for which to calculate correlations.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing correlation coefficients and p-values.\n",
    "    \"\"\"\n",
    "    correlations = []\n",
    "    for col1 in columns:\n",
    "        for col2 in columns:\n",
    "            if col1 != col2:\n",
    "                corr_coeff, p_value = pearsonr(df[col1], df[col2])\n",
    "                correlations.append({\n",
    "                    'Column1': col1,\n",
    "                    'Column2': col2,\n",
    "                    'Correlation Coefficient': corr_coeff,\n",
    "                    'P-Value': p_value\n",
    "                })\n",
    "    return pd.DataFrame(correlations)\n",
    "\n",
    "# Get log-transformed columns\n",
    "log_transformed_columns = [f'{variable}_log' for variable in skewed_variables]\n",
    "\n",
    "# Calculate correlations for log-transformed columns\n",
    "correlation_results_log = calculate_correlation(activity_merged_numeric, log_transformed_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b46819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge correlation coefficients and p-values into one DataFrame\n",
    "correlation_results_log['Correlation Coefficient'] = correlation_results_log['Correlation Coefficient'].astype(float)\n",
    "correlation_results_log['P-Value'] = correlation_results_log['P-Value'].astype(float)\n",
    "\n",
    "# Format correlation coefficients and p-values\n",
    "correlation_results_log['Correlation'] = correlation_results_log.apply(lambda x: f\"{x['Correlation Coefficient']:.2f} (p={x['P-Value']:.2f})\", axis=1)\n",
    "\n",
    "# Create a pivot table for the merged DataFrame\n",
    "merged_matrix = correlation_results_log.pivot(index='Column1', columns='Column2', values='Correlation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec18045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of variables\n",
    "variables = ['daily_steps', 'n_data_day', 'total_distance_daily', \n",
    "             'num_unique_clusters_total', 'num_total_clusters_total',\n",
    "             'num_total_clusters_daily', 'num_unique_clusters_daily',\n",
    "             'percentage_time_at_home_daily', 'percentage_time_at_home_total']\n",
    "\n",
    "# Plot histograms for each variable\n",
    "for variable in variables:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(activity_merged_numeric[variable], kde=True)\n",
    "    plt.title(f'Distribution of {variable}')\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ff631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot histograms for each variable\n",
    "for variable in log_transformed_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(activity_merged_numeric[variable], kde=True)\n",
    "    plt.title(f'Distribution of {variable}')\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855643c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc84551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate internal consistency between the first and second week features\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assuming 'feature_column' is the column we are interested in\n",
    "# Replace 'feature_column' with actual feature names\n",
    "\n",
    "features = ['n_data', 'n_data_day', 'n_data_hour', 'n_days_8']  # Example features, replace with actual feature names\n",
    "consistency_results = {}\n",
    "\n",
    "for feature in features:\n",
    "    first_week_features = first_week_df[feature]\n",
    "    second_week_features = second_week_df[feature]\n",
    "    \n",
    "    # Calculate Pearson correlation\n",
    "    corr, _ = pearsonr(first_week_features, second_week_features)\n",
    "    consistency_results[feature] = corr\n",
    "\n",
    "# Display the results\n",
    "consistency_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c84b6",
   "metadata": {},
   "source": [
    "## Calculate Internal Consistency\n",
    "\n",
    "Evaluate the internal consistency of features between the first and second weeks using Pearson correlation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
