{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20ac61a",
   "metadata": {},
   "source": [
    "# 4 Detailed Preprocessing of Passive Data\n",
    "\n",
    "This notebook shows the analysis of situational context using EMA and passive sensing data\n",
    "\n",
    "1. **Load Data**: Load necessary data from pickle files.\n",
    "2. **Preprocess EMA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cd56bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import regex as re\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from config import datapath, preprocessed_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import statistics  # Make sure this is imported\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_context(\"notebook\", rc={\"axes.labelsize\": 14, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': True})\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696265f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backup_path = preprocessed_path + \"backup_data_passive_actual.feather\"\n",
    "df_backup = pd.read_feather(backup_path)\n",
    "\n",
    "with open(preprocessed_path + '/ema_data.pkl', 'rb') as file:\n",
    "    df_ema_framework = pickle.load(file)\n",
    "\n",
    "with open(preprocessed_path + '/ema_content.pkl', 'rb') as file:\n",
    "    df_ema_content = pickle.load(file)  \n",
    "\n",
    "with open(preprocessed_path + '/monitoring_data.pkl', 'rb') as file:\n",
    "    df_monitoring = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba14b26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# Check min. amount of EMA data available to map to passive data\n",
    "\n",
    "timedelta_hours = 2\n",
    "assess = 0\n",
    "\n",
    "#GPS data\n",
    "speed_limit = 1.4\n",
    "max_distance = 150 \n",
    "kms_per_radian = 6371000\n",
    "epsilon = 100/kms_per_radian\n",
    "min_samples = 10\n",
    "min_nights_obs = 7\n",
    "min_f_home = 0.5\n",
    "\n",
    "# EMA\n",
    "assessment_phase = [0] #1,2\n",
    "min_num_daily = 4\n",
    "min_days_data = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d27fda9-0ca4-4ea8-9504-ef00001d8fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema1 = df_ema_content.loc[df_ema_content.study.isin([24,25])] # first assessment phase\n",
    "df_ema1 = df_ema1.loc[df_ema1[\"n_quest\"] >= min_num_daily]\n",
    "df_ema1[\"n_days_min\"] = df_ema1.groupby(\"customer\")['quest_complete_day'].transform(\"nunique\")\n",
    "df_ema1 = df_ema1.loc[df_ema1.n_days_min >= min_days_data]\n",
    "df_ema1_customers = df_ema1.customer.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20265259",
   "metadata": {},
   "source": [
    "## 1. Prepare passive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "973f3003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act = df_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc02478e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only keep data that were collected during the first assessment phase\n",
    "df_pass_act_base = df_pass_act[df_pass_act.startTimestamp <= df_pass_act.ema_base_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cab1c-c515-4365-a3c1-6efbfd465284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pass_act_base = df_pass_act_base.loc[df_pass_act_base.customer.isin(df_ema1_customers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40577ed8-9cdf-4909-a565-847daca726dd",
   "metadata": {},
   "source": [
    "### 1.1 Calculate GPS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ead0d9e-b89c-44d1-ba1d-6e1f3b2e17ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act_loc =df_pass_act_base[df_pass_act_base.type.isin([\"Latitude\", \"Longitude\"])][[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4e8c7eb-9534-456c-900b-2b6aced14f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_loc = df_pass_act_loc.pivot_table(\n",
    "    index=[\"customer\", \"startTimestamp\"],\n",
    "    columns=\"type\",\n",
    "    values=[\"doubleValue\"],\n",
    "    aggfunc='first'  # Using 'first' since each type should theoretically have only one entry per customer and timestamp\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex in columns\n",
    "df_loc.columns = ['_'.join(col).strip() for col in df_loc.columns.values]\n",
    "\n",
    "df_loc = df_loc.rename_axis(None, axis=1).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "df_loc = df_loc.rename(columns={\n",
    "    'doubleValue_Latitude': 'Latitude',\n",
    "    'doubleValue_Longitude': 'Longitude',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9530c26c-b77e-4268-9455-1c2f58cf8e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import statistics\n",
    "\n",
    "class HomeClusterExtractor:\n",
    "    def __init__(self, df, speed_limit, max_distance, epsilon, min_samples, min_nights_obs, min_f_home):\n",
    "        self.df = df.copy()\n",
    "        self.speed_limit = speed_limit  # Speed limit in meters per second (m/s)\n",
    "        self.max_distance = max_distance  # Maximum distance to consider for stationary points (in meters)\n",
    "        self.epsilon = epsilon  # Epsilon value for DBSCAN clustering\n",
    "        self.min_samples = min_samples  # Minimum samples for DBSCAN clustering\n",
    "        self.min_nights_obs = min_nights_obs  # Minimum nights of observation to identify home cluster\n",
    "        self.min_f_home = min_f_home  # Minimum fraction of time spent at home for it to be considered home\n",
    "        \n",
    "        self.df['hour_gps'] = self.df['startTimestamp'].dt.hour\n",
    "        self.df['day_gps'] = self.df['startTimestamp'].dt.date\n",
    "\n",
    "    def calculate_distances_and_speeds(self):\n",
    "        \"\"\"Calculate distances and speeds for each customer.\"\"\"\n",
    "        self.df['distance'], self.df['time_diff'], self.df['speed'] = np.nan, np.nan, np.nan\n",
    "\n",
    "        for customer in self.df['customer'].unique():\n",
    "            mask = self.df['customer'] == customer\n",
    "            customer_data = self.df.loc[mask]\n",
    "\n",
    "            # Calculate distances between consecutive points\n",
    "            distances = self._calculate_distances(customer_data)\n",
    "            time_diffs = customer_data['startTimestamp'].diff().dt.total_seconds().fillna(0)\n",
    "            speeds = distances / time_diffs.replace(0, np.nan)\n",
    "\n",
    "            self.df.loc[mask, 'distance'] = distances\n",
    "            self.df.loc[mask, 'time_diff'] = time_diffs\n",
    "            self.df.loc[mask, 'speed'] = speeds\n",
    "\n",
    "    def calculate_stationary_and_transition(self):\n",
    "        \"\"\"\n",
    "        Determine stationary points and transition status based on speed and distance.\n",
    "        All points not classified as stationary will be marked as in transition.\n",
    "\n",
    "        Returns:\n",
    "        - Updates self.df with 'stationary' and 'transition' columns.\n",
    "        \"\"\"\n",
    "        # Filter out points with speed > 220 km/h\n",
    "        self.df = self.df[self.df['speed'] <= 220 * 1000 / 3600]  # 220 km/h converted to m/s\n",
    "\n",
    "        # Calculate stationary status: True for stationary points, False for transition points\n",
    "        self.df['stationary'] = (self.df['speed'] < self.speed_limit) & (self.df['distance'] < self.max_distance)\n",
    "\n",
    "        # Calculate transition status: Not stationary means in transition\n",
    "        self.df['transition'] = np.where(self.df['stationary'], 0, 1)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def _calculate_distances(self, df):\n",
    "        \"\"\"Helper method to calculate distances using haversine formula.\"\"\"\n",
    "        coords = df[['Latitude', 'Longitude']].values\n",
    "        distances = np.array([\n",
    "            self._haversine(coords[i-1][1], coords[i-1][0], coords[i][1], coords[i][0])\n",
    "            for i in range(1, len(coords))\n",
    "        ])\n",
    "        return np.append(distances, 0)  # Append 0 for the last point\n",
    "\n",
    "    def _haversine(self, lon1, lat1, lon2, lat2):\n",
    "        # Haversine formula to calculate distance between two lat/lon points in meters\n",
    "        R = 6371000  # Radius of Earth in meters\n",
    "        lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "\n",
    "        a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        distance = R * c  # Output distance in meters\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def apply_clustering(self, df):\n",
    "        \"\"\"Apply DBSCAN clustering on stationary points.\"\"\"\n",
    "        return df.groupby('customer').apply(self._apply_dbscan).reset_index(drop=True)\n",
    "\n",
    "    def _apply_dbscan(self, df):\n",
    "        \"\"\"Helper method to apply DBSCAN clustering.\"\"\"\n",
    "        clustering_model = DBSCAN(eps=self.epsilon, min_samples=self.min_samples, metric=\"haversine\")\n",
    "        cluster_labels = clustering_model.fit_predict(df[['Longitude', 'Latitude']].apply(np.radians))\n",
    "\n",
    "        # Convert cluster labels to integers\n",
    "        cluster_labels = cluster_labels.astype(int)\n",
    "\n",
    "        return pd.DataFrame({'cluster': cluster_labels}, index=df.index)\n",
    "\n",
    "    def find_home_cluster(self, geodata_clusters):\n",
    "        \"\"\"Identify the home cluster based on nighttime data.\"\"\"\n",
    "        # Filter for night hours\n",
    "        geodata_night = geodata_clusters.loc[\n",
    "            (geodata_clusters['hour_gps'] >= 20) | (geodata_clusters['hour_gps'] <= 7)\n",
    "        ].copy()\n",
    "\n",
    "        # Filter out any rows with a clusterID of -1\n",
    "        geodata_night = geodata_night[geodata_night['cluster'] != -1]\n",
    "\n",
    "        # Calculate the mode of clusterID per user during night hours\n",
    "        geodata_night['home'] = geodata_night.groupby('customer')['clusterID'].transform(\n",
    "            lambda x: statistics.mode(x)\n",
    "        )\n",
    "\n",
    "        # Ensure 'home' is treated as a string without floating-point suffix\n",
    "        geodata_night['home'] = geodata_night['home'].astype(str).str.replace('.0', '')\n",
    "\n",
    "        # Calculate various metrics to validate the home cluster\n",
    "        geodata_night['nights_with_obs'] = geodata_night.groupby('customer')['day_gps'].transform('nunique')\n",
    "        geodata_night['night_obs'] = geodata_night.groupby('customer')['day_gps'].transform('size')\n",
    "        geodata_night['n_home'] = geodata_night.groupby('customer')['home'].transform(lambda x: x.value_counts().iloc[0])\n",
    "        geodata_night['f_home'] = geodata_night['n_home'] / geodata_night['night_obs']\n",
    "\n",
    "        # Update the 'home' label based on conditions\n",
    "        geodata_night['home'] = geodata_night.apply(\n",
    "            lambda x: x['home'] if x['nights_with_obs'] >= self.min_nights_obs and x['f_home'] > self.min_f_home else None, axis=1)\n",
    "\n",
    "        # Extract a mapping of userID to home cluster\n",
    "        user_home_mapping = geodata_night[['customer', 'home']].drop_duplicates()\n",
    "\n",
    "        # Merging back to the full dataset\n",
    "        return pd.merge(geodata_clusters, user_home_mapping, on='customer', how='left')\n",
    "\n",
    "    def determine_if_at_home(self, df):\n",
    "        \"\"\"Determine if a person is at home.\"\"\"\n",
    "        # Ensure that both 'clusterID' and 'home' are treated as strings for comparison\n",
    "        df['clusterID'] = df['clusterID'].astype(str).str.replace('.0', '')\n",
    "        df['home'] = df['home'].astype(str).str.replace('.0', '')\n",
    "\n",
    "        # Create a column 'at_home' that compares 'clusterID' with 'home'\n",
    "        df['at_home'] = df.apply(\n",
    "            lambda x: 1 if x['clusterID'] == x['home'] else (0 if pd.notna(x['home']) else -1), axis=1\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the full extraction process.\"\"\"\n",
    "        self.calculate_distances_and_speeds()\n",
    "\n",
    "        # Calculate stationary and transition points\n",
    "        self.df = self.calculate_stationary_and_transition()\n",
    "\n",
    "        # Apply clustering only to stationary points\n",
    "        stationary_df = self.df[self.df['stationary']]  # Filter only stationary points\n",
    "        geodata_cluster_df = self.apply_clustering(stationary_df)\n",
    "\n",
    "        # Merge clustering results back to the original dataframe, including transition points\n",
    "        geodata_clusters = pd.concat([self.df.reset_index(drop=True), geodata_cluster_df[['cluster']]], axis=1)\n",
    "\n",
    "        # Fill NaN cluster labels for transition points with a placeholder (e.g., -1)\n",
    "        geodata_clusters['cluster'].fillna(-1, inplace=True)\n",
    "\n",
    "        # Create clusterID only for non-transition points\n",
    "        geodata_clusters['clusterID'] = geodata_clusters.apply(\n",
    "            lambda x: f\"{x['customer']}00{int(x['cluster'])}\" if x['cluster'] != -1 else None, axis=1\n",
    "        )\n",
    "\n",
    "        # Identify home clusters\n",
    "        geodata_clusters = self.find_home_cluster(geodata_clusters)\n",
    "\n",
    "        # Determine if the person is at home\n",
    "        geodata_clusters = self.determine_if_at_home(geodata_clusters)\n",
    "\n",
    "        return geodata_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3caf9eb-ade7-4215-b09b-031095780183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the class\n",
    "extractor = HomeClusterExtractor(\n",
    "    df=df_loc,\n",
    "    speed_limit=speed_limit,\n",
    "    max_distance=max_distance,\n",
    "    epsilon=epsilon,\n",
    "    min_samples=min_samples,\n",
    "    min_nights_obs=min_nights_obs,\n",
    "    min_f_home=min_f_home\n",
    ")\n",
    "\n",
    "# Run the extraction process\n",
    "home_clusters = extractor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9895a55-a53d-4ab1-9b07-1159723e50e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home_clusters_red = home_clusters[[\"customer\", \"startTimestamp\", \"at_home\",\"transition\", \"distance\", \"time_diff\", \"speed\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9d2bb91-fbea-4ae0-990a-d50a4bdbc15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_clusters.home.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596a358",
   "metadata": {},
   "source": [
    "## 2. Prepare EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "133d9e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_udi = df_ema_content[[\"customer\", \"createdAt_day\", \"quest_create\", \"unique_day_id\", \"assess\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43dbd254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by customer and unique_day_id and calculate the minimum quest_create\n",
    "df_min_quest = df_ema_udi.groupby(['customer', 'unique_day_id'])['quest_create'].min().reset_index()\n",
    "\n",
    "# Rename the column to sensor_block_end\n",
    "df_min_quest.rename(columns={'quest_create': 'sensor_block_end'}, inplace=True)\n",
    "\n",
    "# Merge the minimum quest_create back to the original DataFrame\n",
    "df_ema_udi = pd.merge(df_ema_udi, df_min_quest, on=['customer', 'unique_day_id'], how='left')\n",
    "\n",
    "# Create the sensor_block_start column, which is 2 hours before quest_create\n",
    "df_ema_udi.drop(columns=['quest_create'], inplace=True)\n",
    "df_ema_udi = df_ema_udi.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79939ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_udi['sensor_block_start'] = df_ema_udi['sensor_block_end'] - pd.Timedelta(hours=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "041705f9-ae62-4345-a71d-afc2f06224d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only include first assessment phase\n",
    "df_ema_udi_base = df_ema_udi.loc[df_ema_udi.assess == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c98e545-b195-4c99-a137-6c508372c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema_udi_test = df_ema_udi_base.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c3f7312-1c8c-41b7-abb9-38e362cce259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act_test = df_pass_act_base.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5c641-d497-4956-96a2-35a09594aa57",
   "metadata": {},
   "source": [
    "## 3. Merge EMA to passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd79898b-4f3c-42c9-83aa-5c155f842b67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class EMAMapper:\n",
    "    def __init__(self, df_ema, df_data, df_home_clusters=None):\n",
    "        self.df_ema = df_ema.copy()\n",
    "        self.df_data = df_data.copy()\n",
    "\n",
    "        if df_home_clusters is not None:\n",
    "            self.df_home_clusters = df_home_clusters.copy()\n",
    "            self.df_home_clusters['startTimestamp'] = pd.to_datetime(self.df_home_clusters['startTimestamp'])\n",
    "        else:\n",
    "            self.df_home_clusters = None\n",
    "\n",
    "        self.df_ema['sensor_block_start'] = pd.to_datetime(self.df_ema['sensor_block_start'])\n",
    "        self.df_ema['sensor_block_end'] = pd.to_datetime(self.df_ema['sensor_block_end'])\n",
    "        self.df_data['startTimestamp'] = pd.to_datetime(self.df_data['startTimestamp'])\n",
    "\n",
    "    def map_steps_to_ema(self):\n",
    "        \"\"\"Map steps to EMA blocks.\"\"\"\n",
    "        n_steps_values = []\n",
    "\n",
    "        df_steps = self.df_data[self.df_data['type'] == 'Steps']\n",
    "\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            sensor_block_start = ema_row['sensor_block_start']\n",
    "            sensor_block_end = ema_row['sensor_block_end']\n",
    "\n",
    "            # Use df_steps, not df_filtered, for filtering\n",
    "            df_filtered = df_steps[(df_steps['startTimestamp'] < sensor_block_end) & \n",
    "                                   (df_steps['endTimestamp'] > sensor_block_start)]\n",
    "\n",
    "            if df_filtered.empty:\n",
    "                n_steps_values.append(0)\n",
    "            else:\n",
    "                overlap_start = df_filtered['startTimestamp'].combine(sensor_block_start, max)\n",
    "                overlap_end = df_filtered['endTimestamp'].combine(sensor_block_end, min)\n",
    "\n",
    "                overlap_duration = (overlap_end - overlap_start).dt.total_seconds()\n",
    "                step_duration = (df_filtered['endTimestamp'] - df_filtered['startTimestamp']).dt.total_seconds()\n",
    "\n",
    "                proportion = overlap_duration / step_duration\n",
    "                weighted_value = proportion * df_filtered['doubleValue']\n",
    "\n",
    "                n_steps = weighted_value.sum()\n",
    "                n_steps_values.append(round(n_steps))\n",
    "\n",
    "        self.df_ema['n_steps'] = n_steps_values\n",
    "        return self.df_ema\n",
    "\n",
    "\n",
    "    def map_gps_and_transition_to_ema(self):\n",
    "        \"\"\"\n",
    "        Map GPS, stationary, and transition data to EMA blocks in one process.\n",
    "\n",
    "        Returns:\n",
    "        - df_ema with additional columns: 'n_GPS', 'total_distance_km', 'transition', 'transition_minutes', 'at_home_minute', and 'at_home_binary'.\n",
    "        \"\"\"\n",
    "        if self.df_home_clusters is None:\n",
    "            raise ValueError(\"df_home_clusters is not provided during initialization.\")\n",
    "\n",
    "        gps_counts = []\n",
    "        total_distances = []\n",
    "        transition_values = []\n",
    "        transition_minute_values = []\n",
    "        at_home_minute_values = []\n",
    "        at_home_binary_values = []\n",
    "\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            sensor_block_start = ema_row['sensor_block_start']\n",
    "            sensor_block_end = ema_row['sensor_block_end']\n",
    "            customer = ema_row['customer']\n",
    "\n",
    "            # Filter the home clusters for the current EMA block and customer\n",
    "            df_filtered = self.df_home_clusters[\n",
    "                (self.df_home_clusters['startTimestamp'] >= sensor_block_start) & \n",
    "                (self.df_home_clusters['startTimestamp'] <= sensor_block_end) & \n",
    "                (self.df_home_clusters['customer'] == customer)\n",
    "            ]\n",
    "\n",
    "            if df_filtered.empty:\n",
    "                # If no GPS data is found in the block, set to default values\n",
    "                gps_counts.append(0)\n",
    "                total_distances.append(0)\n",
    "                transition_values.append(-1)  # Unknown\n",
    "                transition_minute_values.append(0)\n",
    "                at_home_minute_values.append(0)\n",
    "                at_home_binary_values.append(-1)  # Unknown (no data)\n",
    "            else:\n",
    "                # Count GPS points\n",
    "                gps_count = df_filtered.shape[0]\n",
    "                gps_counts.append(gps_count)\n",
    "\n",
    "                # Calculate total distance (sum of distances)\n",
    "                total_distance = df_filtered['distance'].sum() / 1000  # Convert to kilometers\n",
    "                total_distances.append(total_distance)\n",
    "\n",
    "                # Calculate transition data\n",
    "                transition_minutes = df_filtered[df_filtered['transition'] == 1]['time_diff'].sum() / 60  # Seconds to minutes\n",
    "                transition_minute_values.append(transition_minutes)\n",
    "                transition_status = 1 if transition_minutes > 0 else 0\n",
    "                transition_values.append(transition_status)\n",
    "\n",
    "                # Calculate at_home_minute\n",
    "                at_home_minutes = df_filtered[df_filtered['at_home'] == 1]['time_diff'].sum() / 60  # Seconds to minutes\n",
    "                at_home_minute_values.append(at_home_minutes)\n",
    "\n",
    "                # Calculate at_home_binary: 1 if at least one row has at_home = 1, otherwise 0 or -1 if unknown\n",
    "                if df_filtered['at_home'].eq(1).any():\n",
    "                    at_home_binary_values.append(1)  # At least one point at home\n",
    "                elif df_filtered['at_home'].eq(0).all():\n",
    "                    at_home_binary_values.append(0)  # All points not at home\n",
    "                else:\n",
    "                    at_home_binary_values.append(-1)  # Unknown (if no valid data)\n",
    "\n",
    "        self.df_ema['n_GPS'] = gps_counts\n",
    "        self.df_ema['total_distance_km'] = total_distances\n",
    "        self.df_ema['transition'] = transition_values\n",
    "        self.df_ema['transition_minutes'] = transition_minute_values\n",
    "        self.df_ema['at_home_minute'] = at_home_minute_values\n",
    "        self.df_ema['at_home_binary'] = at_home_binary_values\n",
    "\n",
    "        return self.df_ema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbecc8ab-0a6a-471a-abaf-111b726456be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>createdAt_day</th>\n",
       "      <th>unique_day_id</th>\n",
       "      <th>assess</th>\n",
       "      <th>sensor_block_end</th>\n",
       "      <th>sensor_block_start</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>n_GPS</th>\n",
       "      <th>total_distance_km</th>\n",
       "      <th>transition</th>\n",
       "      <th>transition_minutes</th>\n",
       "      <th>at_home_minute</th>\n",
       "      <th>at_home_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>20230919_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-19 07:31:20.352</td>\n",
       "      <td>2023-09-19 05:31:20.352</td>\n",
       "      <td>295</td>\n",
       "      <td>115</td>\n",
       "      <td>0.445619</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>20230919_6</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-19 17:14:33.463</td>\n",
       "      <td>2023-09-19 15:14:33.463</td>\n",
       "      <td>11126</td>\n",
       "      <td>286</td>\n",
       "      <td>16.757672</td>\n",
       "      <td>1</td>\n",
       "      <td>18.233333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-28</td>\n",
       "      <td>20230928_6</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-28 18:29:55.737</td>\n",
       "      <td>2023-09-28 16:29:55.737</td>\n",
       "      <td>15424</td>\n",
       "      <td>2</td>\n",
       "      <td>19.321239</td>\n",
       "      <td>1</td>\n",
       "      <td>60.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-25</td>\n",
       "      <td>20230925_6</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-25 17:08:34.592</td>\n",
       "      <td>2023-09-25 15:08:34.592</td>\n",
       "      <td>15565</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-22</td>\n",
       "      <td>20230922_6</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-22 17:06:14.166</td>\n",
       "      <td>2023-09-22 15:06:14.166</td>\n",
       "      <td>6507</td>\n",
       "      <td>85</td>\n",
       "      <td>6.815990</td>\n",
       "      <td>1</td>\n",
       "      <td>11.550000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer createdAt_day unique_day_id  assess        sensor_block_end  \\\n",
       "0      MYAi    2023-09-19    20230919_1       0 2023-09-19 07:31:20.352   \n",
       "32     MYAi    2023-09-19    20230919_6       0 2023-09-19 17:14:33.463   \n",
       "33     MYAi    2023-09-28    20230928_6       0 2023-09-28 18:29:55.737   \n",
       "35     MYAi    2023-09-25    20230925_6       0 2023-09-25 17:08:34.592   \n",
       "39     MYAi    2023-09-22    20230922_6       0 2023-09-22 17:06:14.166   \n",
       "\n",
       "        sensor_block_start  n_steps  n_GPS  total_distance_km  transition  \\\n",
       "0  2023-09-19 05:31:20.352      295    115           0.445619           0   \n",
       "32 2023-09-19 15:14:33.463    11126    286          16.757672           1   \n",
       "33 2023-09-28 16:29:55.737    15424      2          19.321239           1   \n",
       "35 2023-09-25 15:08:34.592    15565      0           0.000000          -1   \n",
       "39 2023-09-22 15:06:14.166     6507     85           6.815990           1   \n",
       "\n",
       "    transition_minutes  at_home_minute  at_home_binary  \n",
       "0             0.000000             0.0               0  \n",
       "32           18.233333             0.0               0  \n",
       "33           60.050000             0.0               0  \n",
       "35            0.000000             0.0              -1  \n",
       "39           11.550000             0.0               0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 2: Map EMA data\n",
    "ema_mapper = EMAMapper(df_ema_udi_base, df_pass_act_base, df_home_clusters=home_clusters_red)\n",
    "\n",
    "# Step 3: Map steps, GPS data, and transitions\n",
    "df_ema_with_steps = ema_mapper.map_steps_to_ema()\n",
    "df_ema_with_gps_and_transition = ema_mapper.map_gps_and_transition_to_ema()\n",
    "df_ema_with_at_home = ema_mapper.map_gps_and_transition_to_ema()\n",
    "\n",
    "# Final DataFrame with all the mapped data\n",
    "df_ema_with_at_home.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a15b834-d8ce-49f7-bf59-ccb0e05ff54c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17087.000000\n",
       "mean        18.095328\n",
       "std         90.882889\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max       3301.950000\n",
       "Name: at_home_minute, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ema_with_at_home.at_home_minute.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dad818d2-b04d-4cf6-b10e-f7ff7a33e800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(preprocessed_path + '/map_ema_passive.pkl', 'wb') as file:\n",
    "    pickle.dump(df_ema_with_at_home, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a26233-94bb-4e2b-9dd4-f2a6beb422bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
