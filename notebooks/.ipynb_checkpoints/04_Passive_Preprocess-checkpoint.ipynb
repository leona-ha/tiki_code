{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20ac61a",
   "metadata": {},
   "source": [
    "# 04 Detailed Preprocessing of Passive Data\n",
    "\n",
    "This notebook shows the analysis of situational context using EMA and passive sensing data\n",
    "\n",
    "1. **Load Data**: Load necessary data from pickle files.\n",
    "2. **Preprocess EMA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cd56bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import regex as re\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from config import datapath, preprocessed_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_context(\"notebook\", rc={\"axes.labelsize\": 14, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': True})\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696265f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backup_path = preprocessed_path + \"backup_data_passive.feather\"\n",
    "df_backup = pd.read_feather(backup_path)\n",
    "\n",
    "with open(preprocessed_path + '/ema_data.pkl', 'rb') as file:\n",
    "    df_ema_framework = pickle.load(file)\n",
    "\n",
    "with open(preprocessed_path + '/ema_content.pkl', 'rb') as file:\n",
    "    df_ema_content = pickle.load(file)  \n",
    "\n",
    "with open(preprocessed_path + '/monitoring_data.pkl', 'rb') as file:\n",
    "    df_monitoring = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba14b26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# Check min. amount of EMA data available to map to passive data\n",
    "\n",
    "timedelta_hours = 2\n",
    "assess = 0\n",
    "\n",
    "#GPS data\n",
    "speed_limit = 1.4\n",
    "max_distance = 150 \n",
    "kms_per_radian = 6371.0088\n",
    "epsilon = 0.03/kms_per_radian\n",
    "min_samples = 10\n",
    "min_nights_obs = 5\n",
    "min_f_home = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20265259",
   "metadata": {},
   "source": [
    "## 1. Prepare passive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973f3003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act = df_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc02478e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only keep data that were collected during the first assessment phase\n",
    "df_pass_act_base = df_pass_act[df_pass_act.startTimestamp_day <= df_pass_act.ema_relative_end_phase0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40577ed8-9cdf-4909-a565-847daca726dd",
   "metadata": {},
   "source": [
    "### 1.1 Calculate GPS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ead0d9e-b89c-44d1-ba1d-6e1f3b2e17ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pass_act_loc =df_pass_act_base[df_pass_act_base.type.isin([\"Latitude\", \"Longitude\"])][[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4e8c7eb-9534-456c-900b-2b6aced14f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_loc = df_pass_act_loc.pivot_table(\n",
    "    index=[\"customer\", \"startTimestamp\"],\n",
    "    columns=\"type\",\n",
    "    values=[\"doubleValue\"],\n",
    "    aggfunc='first'  # Using 'first' since each type should theoretically have only one entry per customer and timestamp\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex in columns\n",
    "df_loc.columns = ['_'.join(col).strip() for col in df_loc.columns.values]\n",
    "\n",
    "df_loc = df_loc.rename_axis(None, axis=1).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "df_loc = df_loc.rename(columns={\n",
    "    'doubleValue_Latitude': 'Latitude',\n",
    "    'doubleValue_Longitude': 'Longitude',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc6b05-a40a-4e1a-b200-5f4ebc569259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HomeClusterExtractor:\n",
    "    def __init__(self, df, speed_limit, max_distance, epsilon, min_samples, min_nights_obs, min_f_home):\n",
    "        self.df = df.copy()\n",
    "        self.speed_limit = speed_limit\n",
    "        self.max_distance = max_distance\n",
    "        self.epsilon = epsilon\n",
    "        self.min_samples = min_samples\n",
    "        self.min_nights_obs = min_nights_obs\n",
    "        self.min_f_home = min_f_home\n",
    "\n",
    "    def calculate_distances_and_speeds(self):\n",
    "        \"\"\"Calculate distances and speeds for each customer.\"\"\"\n",
    "        self.df['distance'], self.df['time_diff'], self.df['speed'] = np.nan, np.nan, np.nan\n",
    "\n",
    "        for customer in self.df['customer'].unique():\n",
    "            mask = self.df['customer'] == customer\n",
    "            customer_data = self.df.loc[mask]\n",
    "\n",
    "            # Calculate distances between consecutive points\n",
    "            distances = self._calculate_distances(customer_data)\n",
    "            time_diffs = customer_data['startTimestamp'].diff().dt.total_seconds().fillna(0)\n",
    "            speeds = distances / time_diffs.replace(0, np.nan)\n",
    "\n",
    "            self.df.loc[mask, 'distance'] = distances\n",
    "            self.df.loc[mask, 'time_diff'] = time_diffs\n",
    "            self.df.loc[mask, 'speed'] = speeds\n",
    "\n",
    "    def _calculate_distances(self, df):\n",
    "        \"\"\"Helper method to calculate distances using haversine formula.\"\"\"\n",
    "        coords = df[['Latitude', 'Longitude']].values\n",
    "        distances = np.array([\n",
    "            haversine(coords[i-1][1], coords[i-1][0], coords[i][1], coords[i][0])\n",
    "            for i in range(1, len(coords))\n",
    "        ])\n",
    "        return np.append(distances, 0)  # Append 0 for the last point\n",
    "\n",
    "    def extract_stationary_points(self):\n",
    "        \"\"\"Filter stationary points based on speed and distance.\"\"\"\n",
    "        return self.df[(self.df['speed'] < self.speed_limit) & (self.df['distance'] < self.max_distance)]\n",
    "\n",
    "    def apply_clustering(self, df):\n",
    "        \"\"\"Apply DBSCAN clustering on stationary points.\"\"\"\n",
    "        return df.groupby('customer').apply(self._apply_dbscan).reset_index(drop=True)\n",
    "\n",
    "    def _apply_dbscan(self, df):\n",
    "        \"\"\"Helper method to apply DBSCAN clustering.\"\"\"\n",
    "        clustering_model = DBSCAN(eps=self.epsilon, min_samples=self.min_samples, metric=\"haversine\")\n",
    "        cluster_labels = clustering_model.fit_predict(df[['Longitude', 'Latitude']].apply(np.radians))\n",
    "        return pd.DataFrame({'cluster_100m': cluster_labels}, index=df.index)\n",
    "\n",
    "    def find_home_cluster(self, geodata_clusters):\n",
    "        \"\"\"Identify the home cluster based on nighttime data.\"\"\"\n",
    "        geodata_night = geodata_clusters[(geodata_clusters['hour_gps'] >= 20) | (geodata_clusters['hour_gps'] <= 6)]\n",
    "\n",
    "        geodata_night['home'] = geodata_night.groupby('customer')['clusterID'].transform(\n",
    "            lambda x: statistics.mode(x)\n",
    "        )\n",
    "        geodata_night['nights_with_obs'] = geodata_night.groupby('customer')['day_gps'].transform('nunique')\n",
    "        geodata_night['night_obs'] = geodata_night.groupby('customer')['day_gps'].transform('size')\n",
    "        geodata_night['n_home'] = geodata_night.groupby('customer')['home'].transform(lambda x: x.value_counts().iloc[0])\n",
    "        geodata_night['f_home'] = geodata_night['n_home'] / geodata_night['night_obs']\n",
    "\n",
    "        geodata_night['home'] = geodata_night.apply(\n",
    "            lambda x: x['home'] if x['nights_with_obs'] >= self.min_nights_obs and x['f_home'] > self.min_f_home else None, axis=1\n",
    "        )\n",
    "\n",
    "        user_home_mapping = geodata_night[['customer', 'home']].drop_duplicates()\n",
    "        return pd.merge(geodata_clusters, user_home_mapping, on='customer', how='left')\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the full extraction process.\"\"\"\n",
    "        self.calculate_distances_and_speeds()\n",
    "        stationary_df = self.extract_stationary_points()\n",
    "        geodata_cluster_df = self.apply_clustering(stationary_df)\n",
    "\n",
    "        geodata_clusters = pd.concat([stationary_df.reset_index(drop=True), geodata_cluster_df['cluster_100m']], axis=1)\n",
    "        geodata_clusters['clusterID'] = geodata_clusters['customer'].astype(str) + '00' + geodata_clusters['cluster_100m'].astype(str)\n",
    "\n",
    "        return self.find_home_cluster(geodata_clusters)\n",
    "\n",
    "# Example usage:\n",
    "extractor = HomeClusterExtractor(df_speed, speed_limit=30, max_distance=50, epsilon=0.001, min_samples=5, min_nights_obs=3, min_f_home=0.7)\n",
    "home_clusters = extractor.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596a358",
   "metadata": {},
   "source": [
    "## 2. Prepare EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "133d9e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_udi = df_ema_content[[\"customer\", \"createdAt_day\", \"quest_create\", \"unique_day_id\", \"assess\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43dbd254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by customer and unique_day_id and calculate the minimum quest_create\n",
    "df_min_quest = df_ema_udi.groupby(['customer', 'unique_day_id'])['quest_create'].min().reset_index()\n",
    "\n",
    "# Rename the column to sensor_block_end\n",
    "df_min_quest.rename(columns={'quest_create': 'sensor_block_end'}, inplace=True)\n",
    "\n",
    "# Merge the minimum quest_create back to the original DataFrame\n",
    "df_ema_udi = pd.merge(df_ema_udi, df_min_quest, on=['customer', 'unique_day_id'], how='left')\n",
    "\n",
    "# Create the sensor_block_start column, which is 2 hours before quest_create\n",
    "df_ema_udi.drop(columns=['quest_create'], inplace=True)\n",
    "df_ema_udi = df_ema_udi.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79939ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_udi['sensor_block_start'] = df_ema_udi['sensor_block_end'] - pd.Timedelta(hours=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "041705f9-ae62-4345-a71d-afc2f06224d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only include first assessment phase\n",
    "df_ema_udi_base = df_ema_udi.loc[df_ema_udi.assess == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5c641-d497-4956-96a2-35a09594aa57",
   "metadata": {},
   "source": [
    "## 3. Merge EMA to passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0998ab9-4c62-4904-b7ef-3b38f8f7908e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class EMAMapper:\n",
    "    def __init__(self, df_ema, df_data):\n",
    "        \"\"\"\n",
    "        Initialize the EMAMapper class with EMA blocks and combined data.\n",
    "\n",
    "        Parameters:\n",
    "        - df_ema: DataFrame containing the EMA blocks with 'sensor_block_start' and 'sensor_block_end'.\n",
    "        - df_data: DataFrame containing various data types with 'type', 'startTimestamp', and 'endTimestamp'.\n",
    "        \"\"\"\n",
    "        self.df_ema = df_ema.copy()\n",
    "        self.df_data = df_data.copy()\n",
    "\n",
    "        # Ensure datetime format\n",
    "        self.df_ema['sensor_block_start'] = pd.to_datetime(self.df_ema['sensor_block_start'])\n",
    "        self.df_ema['sensor_block_end'] = pd.to_datetime(self.df_ema['sensor_block_end'])\n",
    "        self.df_data['startTimestamp'] = pd.to_datetime(self.df_data['startTimestamp'])\n",
    "        if 'endTimestamp' in self.df_data.columns:\n",
    "            self.df_data['endTimestamp'] = pd.to_datetime(self.df_data['endTimestamp'])\n",
    "\n",
    "    def map_steps_to_ema(self):\n",
    "        \"\"\"\n",
    "        Maps Steps data to EMA blocks based on time overlap and calculates n_steps.\n",
    "        Rounds n_steps to the nearest whole number and fills with 0 if no Steps data is found within the block.\n",
    "\n",
    "        Returns:\n",
    "        - df_ema with an additional column 'n_steps' containing the summed and rounded Steps values.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a list to store the mapped step values\n",
    "        n_steps_values = []\n",
    "\n",
    "        # Filter df_data for Steps data only\n",
    "        df_steps = self.df_data[self.df_data['type'] == 'Steps']\n",
    "\n",
    "        # Iterate over each row in df_ema\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            # Extract the current block's start and end times\n",
    "            sensor_block_start = ema_row['sensor_block_start']\n",
    "            sensor_block_end = ema_row['sensor_block_end']\n",
    "\n",
    "            # Filter the Steps data within the current EMA block\n",
    "            df_filtered = df_steps[(df_steps['startTimestamp'] < sensor_block_end) & \n",
    "                                   (df_steps['endTimestamp'] > sensor_block_start)]\n",
    "\n",
    "            if df_filtered.empty:\n",
    "                n_steps_values.append(0)\n",
    "            else:\n",
    "                # Calculate overlap start and end using Pandas functions\n",
    "                overlap_start = df_filtered['startTimestamp'].combine(sensor_block_start, max)\n",
    "                overlap_end = df_filtered['endTimestamp'].combine(sensor_block_end, min)\n",
    "\n",
    "                # Calculate overlap duration\n",
    "                overlap_duration = (overlap_end - overlap_start).dt.total_seconds()\n",
    "\n",
    "                # Calculate step duration\n",
    "                step_duration = (df_filtered['endTimestamp'] - df_filtered['startTimestamp']).dt.total_seconds()\n",
    "\n",
    "                # Calculate proportion and weighted value\n",
    "                proportion = overlap_duration / step_duration\n",
    "                weighted_value = proportion * df_filtered['doubleValue']\n",
    "\n",
    "                # Sum the weighted values to get the total number of steps for this EMA block\n",
    "                n_steps = weighted_value.sum()\n",
    "\n",
    "                # Round n_steps to the nearest whole number\n",
    "                n_steps_values.append(round(n_steps))\n",
    "\n",
    "        # Assign the rounded n_steps values back to the df_ema\n",
    "        self.df_ema['n_steps'] = n_steps_values\n",
    "\n",
    "        return self.df_ema\n",
    "\n",
    "\n",
    "    def count_gps_rows_within_blocks(self):\n",
    "        \"\"\"\n",
    "        Counts the number of GPS rows (n_GPS) where startTimestamp lies between sensor_block_start and sensor_block_end.\n",
    "        Fills with 0 if no GPS data is found within the block.\n",
    "\n",
    "        Returns:\n",
    "        - df_ema with an additional column 'n_GPS' containing the count of GPS rows within each block.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a list to store the counts of GPS rows\n",
    "        gps_counts = []\n",
    "\n",
    "        # Filter df_data for GPS data (Latitude and Longitude)\n",
    "        df_gps = self.df_data[self.df_data['type'].isin(['Latitude', 'Longitude'])]\n",
    "\n",
    "        # Iterate over each row in df_ema\n",
    "        for idx, ema_row in self.df_ema.iterrows():\n",
    "            # Extract the current block's start and end times\n",
    "            sensor_block_start = ema_row['sensor_block_start']\n",
    "            sensor_block_end = ema_row['sensor_block_end']\n",
    "\n",
    "            # Filter the data within the current EMA block for GPS data\n",
    "            df_filtered = df_gps[(df_gps['startTimestamp'] >= sensor_block_start) & \n",
    "                                 (df_gps['startTimestamp'] <= sensor_block_end)]\n",
    "\n",
    "            # Count GPS entries (each GPS entry has two rows: Latitude and Longitude)\n",
    "            gps_count = df_filtered.shape[0] // 2 if not df_filtered.empty else 0\n",
    "\n",
    "            # Append the count to the list\n",
    "            gps_counts.append(gps_count)\n",
    "\n",
    "        # Assign the counts back to the df_ema\n",
    "        self.df_ema['n_GPS'] = gps_counts\n",
    "\n",
    "        return self.df_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c38f06e-8784-4a4f-8df8-f60edb043040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the EMAMapper class\n",
    "ema_mapper = EMAMapper(df_ema_udi_base, df_pass_act_base)\n",
    "\n",
    "# Map passive steps to EMA blocks\n",
    "df_ema_with_mapped_values = ema_mapper.map_steps_to_ema()\n",
    "\n",
    "# Count GPS rows within EMA blocks\n",
    "df_ema_with_gps_counts = ema_mapper.count_gps_rows_within_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec280165-9e2a-4140-a946-54a34ff11f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test= df_ema_with_gps_counts.loc[df_ema_with_gps_counts.n_GPS == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e755e6a3-2cc6-4371-82fa-b87d8fc0f275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test1 = df_ema_with_gps_counts.loc[df_ema_with_gps_counts.n_steps == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1daabc7-3eef-4e3f-8169-e877882fd753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>createdAt_day</th>\n",
       "      <th>unique_day_id</th>\n",
       "      <th>assess</th>\n",
       "      <th>sensor_block_end</th>\n",
       "      <th>sensor_block_start</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>n_GPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>MYAi</td>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>20230930_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-30 09:00:56.682</td>\n",
       "      <td>2023-09-30 07:00:56.682</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10313</th>\n",
       "      <td>f1J2</td>\n",
       "      <td>2023-08-28</td>\n",
       "      <td>20230828_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-28 08:05:01.498</td>\n",
       "      <td>2023-08-28 06:05:01.498</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10315</th>\n",
       "      <td>f1J2</td>\n",
       "      <td>2023-09-03</td>\n",
       "      <td>20230903_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-03 08:14:41.018</td>\n",
       "      <td>2023-09-03 06:14:41.018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10476</th>\n",
       "      <td>f1J2</td>\n",
       "      <td>2023-09-02</td>\n",
       "      <td>20230902_2</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-02 09:39:34.210</td>\n",
       "      <td>2023-09-02 07:39:34.210</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11065</th>\n",
       "      <td>f1J2</td>\n",
       "      <td>2023-09-05</td>\n",
       "      <td>20230905_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-05 07:36:58.530</td>\n",
       "      <td>2023-09-05 05:36:58.530</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677894</th>\n",
       "      <td>asYV</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>20230630_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-06-30 08:08:19.130</td>\n",
       "      <td>2023-06-30 06:08:19.130</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677926</th>\n",
       "      <td>WAu8</td>\n",
       "      <td>2023-11-11</td>\n",
       "      <td>20231111_5</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-11 16:46:18.889</td>\n",
       "      <td>2023-11-11 14:46:18.889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677959</th>\n",
       "      <td>WAu8</td>\n",
       "      <td>2023-11-11</td>\n",
       "      <td>20231111_3</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-11 13:08:02.992</td>\n",
       "      <td>2023-11-11 11:08:02.992</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677995</th>\n",
       "      <td>WAu8</td>\n",
       "      <td>2023-11-10</td>\n",
       "      <td>20231110_7</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-10 20:22:50.383</td>\n",
       "      <td>2023-11-10 18:22:50.383</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678278</th>\n",
       "      <td>TuLo</td>\n",
       "      <td>2023-08-19</td>\n",
       "      <td>20230819_3</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-19 09:16:43.619</td>\n",
       "      <td>2023-08-19 07:16:43.619</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1636 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer createdAt_day unique_day_id  assess        sensor_block_end  \\\n",
       "1765       MYAi    2023-09-30    20230930_1       0 2023-09-30 09:00:56.682   \n",
       "10313      f1J2    2023-08-28    20230828_1       0 2023-08-28 08:05:01.498   \n",
       "10315      f1J2    2023-09-03    20230903_1       0 2023-09-03 08:14:41.018   \n",
       "10476      f1J2    2023-09-02    20230902_2       0 2023-09-02 09:39:34.210   \n",
       "11065      f1J2    2023-09-05    20230905_1       0 2023-09-05 07:36:58.530   \n",
       "...         ...           ...           ...     ...                     ...   \n",
       "677894     asYV    2023-06-30    20230630_1       0 2023-06-30 08:08:19.130   \n",
       "677926     WAu8    2023-11-11    20231111_5       0 2023-11-11 16:46:18.889   \n",
       "677959     WAu8    2023-11-11    20231111_3       0 2023-11-11 13:08:02.992   \n",
       "677995     WAu8    2023-11-10    20231110_7       0 2023-11-10 20:22:50.383   \n",
       "678278     TuLo    2023-08-19    20230819_3       0 2023-08-19 09:16:43.619   \n",
       "\n",
       "            sensor_block_start  n_steps  n_GPS  \n",
       "1765   2023-09-30 07:00:56.682        0      0  \n",
       "10313  2023-08-28 06:05:01.498        0     62  \n",
       "10315  2023-09-03 06:14:41.018        0      0  \n",
       "10476  2023-09-02 07:39:34.210        0     44  \n",
       "11065  2023-09-05 05:36:58.530        0      0  \n",
       "...                        ...      ...    ...  \n",
       "677894 2023-06-30 06:08:19.130        0     17  \n",
       "677926 2023-11-11 14:46:18.889        0      0  \n",
       "677959 2023-11-11 11:08:02.992        0      0  \n",
       "677995 2023-11-10 18:22:50.383        0      0  \n",
       "678278 2023-08-19 07:16:43.619        0      2  \n",
       "\n",
       "[1636 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd6a49-9e0a-4b38-9a22-9b50987da219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
