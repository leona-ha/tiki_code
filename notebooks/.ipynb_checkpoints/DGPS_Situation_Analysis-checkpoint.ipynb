{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20ac61a",
   "metadata": {},
   "source": [
    "# DGPS 2024: Situation Assessment in PREACT\n",
    "\n",
    "This notebook shows the analysis of situational context using EMA and passive sensing data\n",
    "\n",
    "1. **Load Data**: Load necessary data from pickle files.\n",
    "2. **Preprocess EMA**:\n",
    "- Keep only first assessment phase \n",
    "- Remove all entries that have no complete EMA assessment \n",
    "- Remove all participants with too few data \n",
    "- Create blocks of assessment \n",
    "3. **Perform Item Analysis according to Siepe et al. (2022)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cd56bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Markdown\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datapath, preprocessed_path\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import regex as re\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from server_config import datapath, preprocessed_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_context(\"notebook\", rc={\"axes.labelsize\": 14, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': True})\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ada525-80a7-4b82-bc10-1b06b1381a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "# Original colors for situation categories mapped to the simplified names\n",
    "situation_colors = {\n",
    "    'other': '#1f77b4',\n",
    "    'care work': '#ff7f0e',\n",
    "    'travelling': '#2ca02c',\n",
    "    'chores': '#d62728',\n",
    "    'eating - drinking - selfcare': '#9467bd',  # Updated name to match the new mapping\n",
    "    'active leisure': '#8c564b',\n",
    "    'smartphone - social media': '#e377c2',  # Updated name to match the new mapping\n",
    "    'passive leisure': '#7f7f7f',\n",
    "    'work or study': '#bcbd22'\n",
    "}\n",
    "\n",
    "\n",
    "# Paired colors for diagnosis categories\n",
    "diagnosis_colors = {\n",
    "    'Depressive Disorder': '#a6cee3',   # Light Blue\n",
    "    'Social Anxiety Disorder': '#1f78b4',  # Dark Blue\n",
    "    'Obsessive-Compulsive Disorder': '#b2df8a',  # Light Green\n",
    "    'Generalized Anxiety Disorder': '#33a02c',  # Dark Green\n",
    "    'Agoraphobia and/or Panic Disorder': '#fb9a99',  # Light Pink\n",
    "    'Post-Traumatic Stress Disorder': '#e31a1c',  # Red\n",
    "    'Specific Phobia': '#fdbf6f'}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a36ca6-093e-4a65-979d-312aa51673b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "# Function to get color for situation category\n",
    "def get_situation_color(category):\n",
    "    return situation_colors.get(category, '#000000')  # Default to black if category not found\n",
    "\n",
    "# Function to get color for diagnosis category\n",
    "def get_diagnosis_color(category):\n",
    "    return diagnosis_colors.get(category, '#000000')  # Default to black if category not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696265f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup_path = preprocessed_path + \"backup_data_passive.feather\"\n",
    "#df_backup = pd.read_feather(backup_path)\n",
    "\n",
    "with open(preprocessed_path + 'ema_data.pkl', 'rb') as file:\n",
    "    df_ema_framework = pickle.load(file)\n",
    "\n",
    "with open(preprocessed_path + '/ema_content.pkl', 'rb') as file:\n",
    "    df_ema_content = pickle.load(file)  \n",
    "\n",
    "with open(preprocessed_path + '/monitoring_data.pkl', 'rb') as file:\n",
    "    df_monitoring = pickle.load(file)\n",
    "\n",
    "with open(preprocessed_path + '/redcap_data.pkl', 'rb') as file:\n",
    "    df_redcap = pickle.load(file)\n",
    "\n",
    "with open(preprocessed_path + '/map_ema_passive.pkl', 'rb') as file:\n",
    "    df_ema_passive = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031fc950-73df-4bab-a520-681db981096a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ema_passive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77185ada-492d-4424-a866-f78987b66f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20265259",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMA\n",
    "assessment_phase = [0] #1,2\n",
    "min_num_daily = 4\n",
    "min_days_data = 7\n",
    "\n",
    "#stationary filtering\n",
    "max_distance = 150 \n",
    "speed_limit = 1.4  # Max allowed speed in m/s\n",
    "\n",
    "# DBSCAN\n",
    "kms_per_radian = 6371.0088 # equitorial radius of the earth = 6,371.1 \n",
    "epsilon = 0.03/kms_per_radian\n",
    "min_samples = 10\n",
    "\n",
    "# Kmeans\n",
    "DKmeans = 500\n",
    "\n",
    "#home featurenight\n",
    "min_nights_obs = 4\n",
    "min_f_home = 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94cfda",
   "metadata": {},
   "source": [
    "### 1. Include only patients with finished assessments and enough quests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b95336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first assessment phase finished\n",
    "df_ema = df_ema_content.loc[df_ema_content.status.isin([\"Abgeschlossen\", \"Post_Erhebung_1\",\n",
    "                                                             \"Erhebung_2_aktiv\",\"Post_Erhebung_2\", \"Erhebung_3_aktiv\", \"Dropout\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema1 = df_ema.loc[df_ema.study.isin([24,25])] # first assessment phase\n",
    "df_ema2 = df_ema.loc[df_ema.study.isin([33,34])] # second assessment phase\n",
    "df_ema3 = df_ema.loc[df_ema.study.isin([33,34])] # third assessment phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7472ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema1 = df_ema1.loc[df_ema1[\"n_quest\"] >= min_num_daily]\n",
    "df_ema1[\"n_days_min\"] = df_ema1.groupby(\"customer\")['quest_complete_day'].transform(\"nunique\")\n",
    "df_ema1 = df_ema1.loc[df_ema1.n_days_min >= min_days_data]\n",
    "df_ema1_customers = df_ema1.customer.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema2 = df_ema2.loc[df_ema2[\"n_quest\"] >= min_num_daily]\n",
    "df_ema2[\"n_days_min\"] = df_ema2.groupby(\"customer\")['quest_complete_day'].transform(\"nunique\")\n",
    "df_ema2 = df_ema2.loc[df_ema2.n_days_min >= min_days_data]\n",
    "df_ema2_customers = df_ema2.customer.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bad65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ema3 = df_ema3.loc[df_ema3[\"n_quest\"] >= min_num_daily]\n",
    "df_ema3[\"n_days_min\"] = df_ema3.groupby(\"customer\")['quest_complete_day'].transform(\"nunique\")\n",
    "df_ema3 = df_ema3.loc[df_ema3.n_days_min >= min_days_data]\n",
    "df_ema3_customers = df_ema3.customer.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a98a7",
   "metadata": {},
   "source": [
    "### 2. Pivot table to get assessments merged together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ce875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_social = df_ema[df_ema.quest_title == \"event_social2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b73ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sit = df_ema[df_ema.quest_title ==\"situation1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0034413",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nosit = df_ema[(df_ema['quest_title'] != 'event_social2') & (df_ema['quest_title'] != 'situation1')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table as specified\n",
    "df_sit = df_sit.pivot_table(\n",
    "    index=[\"customer\", \"unique_day_id\", \"quest_create\", \"choice_id\"],\n",
    "    columns=\"quest_title\",\n",
    "    values=\"choice_text\",\n",
    "    aggfunc='first'  # Using 'first' since each entry should theoretically be unique per group\n",
    ")\n",
    "\n",
    "# The columns are now a single level Index with just the quest_title values since 'values' is not a list anymore\n",
    "df_sit.columns = [col for col in df_sit.columns.values]\n",
    "\n",
    "# Reset the index to turn the MultiIndex into columns\n",
    "df_sit = df_sit.reset_index()\n",
    "\n",
    "df_sit = df_sit.drop_duplicates(subset=['customer', 'unique_day_id', 'choice_id'])\n",
    "\n",
    "\n",
    "df_sit['situation_count'] = df_sit.groupby([\"customer\",\"unique_day_id\"])[\"choice_id\"].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table as specified\n",
    "df_social = df_social.pivot_table(\n",
    "    index=[\"customer\", \"unique_day_id\", \"choice_id\"],\n",
    "    columns=\"quest_title\",\n",
    "    values=\"choice_text\",\n",
    "    aggfunc='first'  # Using 'first' since each entry should theoretically be unique per group\n",
    ")\n",
    "\n",
    "# The columns are now a single level Index with just the quest_title values since 'values' is not a list anymore\n",
    "df_social.columns = [col for col in df_social.columns.values]\n",
    "\n",
    "# Reset the index to turn the MultiIndex into columns\n",
    "df_social = df_social.reset_index()\n",
    "df_sit = df_sit.drop_duplicates(subset=['customer', 'unique_day_id', 'choice_id'])\n",
    "\n",
    "df_social['social_contact_count'] = df_social.groupby([\"customer\",\"unique_day_id\"])[\"choice_id\"].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e405a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_social = df_social.drop(columns=['choice_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table as specified\n",
    "df_piv = df_nosit.pivot_table(\n",
    "    index=[\"customer\", \"unique_day_id\", \"assess\", \"quest_complete_day\", \"absolute_day_index\", \"relative_day_index\", \"weekend\", \"quest_nr\", \"weekday\"],\n",
    "    columns=\"quest_title\",\n",
    "    values=\"choice_text\",\n",
    "    aggfunc='first'  # Using 'first' since each entry should theoretically be unique per group\n",
    ")\n",
    "\n",
    "# The columns are now a single level Index with just the quest_title values since 'values' is not a list anymore\n",
    "df_piv.columns = [col for col in df_piv.columns.values]\n",
    "\n",
    "# Reset the index to turn the MultiIndex into columns\n",
    "df_piv = df_piv.reset_index()\n",
    "df_piv = df_piv.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd515d9-432c-4d1c-ad01-6cbb206ec8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benni_list = ['customer', 'unique_day_id', 'assess', 'quest_complete_day',\n",
    "       'absolute_day_index', 'relative_day_index',\n",
    "       'er_acceptance', 'er_control', 'er_distraction', 'er_intensity',\n",
    "       'er_reappraisal', 'er_relaxation', 'er_rumination', 'er_suppression',\n",
    "       'panas_attentiveness', 'panas_fatigue', 'panas_fear1', 'panas_fear2',\n",
    "       'panas_guilt1', 'panas_guilt2', 'panas_hostility1', 'panas_hostility2',\n",
    "       'panas_joviality1', 'panas_joviality2', 'panas_loneliness',\n",
    "       'panas_sadness1', 'panas_sadness2', 'panas_selfassurance',\n",
    "       'panas_serenity1', 'panas_serenity2', 'panas_shyness',\n",
    "       'ta_behavioral', 'ta_behavioral_2','ta_kognitiv', 'ta_kognitiv_2', 'mean_pa', 'mean_na']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d473eaa-075b-4f33-8432-24ed902a2703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35289c92-0670-4fba-bd04-cf7b4c12da64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pa_scale = ['panas_attentiveness', 'panas_joviality1', 'panas_joviality2', \n",
    "            'panas_selfassurance', 'panas_serenity1', 'panas_serenity2']\n",
    "na_scale = ['panas_fatigue', 'panas_fear1', 'panas_fear2', 'panas_guilt1', \n",
    "            'panas_guilt2', 'panas_hostility1', 'panas_hostility2', \n",
    "            'panas_loneliness', 'panas_sadness1', 'panas_sadness2', 'panas_shyness']\n",
    "\n",
    "# Step 1: Ensure the columns in pa_scale and na_scale are numeric\n",
    "df_piv[pa_scale + na_scale] = df_piv[pa_scale + na_scale].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Step 2: Calculate the mean for PA and NA scales per unique_day_id\n",
    "df_piv['mean_pa'] = df_piv.groupby(['customer', 'unique_day_id'])[pa_scale].transform('mean').mean(axis=1)\n",
    "df_piv['mean_na'] = df_piv.groupby(['customer', 'unique_day_id'])[na_scale].transform('mean').mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5674362-b073-4d1a-9750-4d26bb5cc914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_piv_benni = df_piv.loc[df_piv.assess == 1][benni_list]\n",
    "df_piv_benni.to_csv(f\"data_benni_19092024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddca47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only customers with complete assessment\n",
    "df_piv_merged = df_piv.merge(df_sit, on=[\"customer\", \"unique_day_id\"], how=\"right\")\n",
    "\n",
    "# Keep only customers with complete assessment\n",
    "df_piv_merged = df_piv_merged.merge(df_social, on=[\"customer\", \"unique_day_id\"], how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a6c67-d81d-4a44-a1f4-8ae0beb699e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the situation mapping\n",
    "situation_mapping = {\n",
    "    'Etwas Anderes': 'other',  \n",
    "    'Kümmern um Kinder / Angehörige': 'care work',\n",
    "    'Unterwegs (z.B. in der U-Bahn)': 'travelling',\n",
    "    'Hausarbeit oder Erledigungen': 'chores',\n",
    "    'Essen/ Trinken/ Körperpflege': 'eating - drinking - selfcare',  # Changed commas to hyphens\n",
    "    'Freizeitaktivität, eher aktiv (z.B. Sport, Unternehmungen)': 'active leisure',\n",
    "    'Smartphone/ Soziale Medien': 'smartphone - social media',  # Changed commas to hyphens\n",
    "    'Freizeitaktivität, eher passiv (z.B. Film schauen, Lesen)': 'passive leisure',\n",
    "    'Arbeit oder Studium': 'work or study'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'situation1' column\n",
    "df_piv_merged['situation1_simplified'] = df_piv_merged['situation1'].map(situation_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b754d5-9c5d-4dee-b540-7d78b90bac78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keep only first assessment phase\n",
    "df_piv_merged = df_piv_merged.loc[df_piv_merged.assess.isin(assessment_phase)]\n",
    "\n",
    "# keep inly customers with engough data in first assessment phase\n",
    "df_piv_merged = df_piv_merged.loc[df_piv_merged.customer.isin(df_ema1_customers)]\n",
    "df_piv_merged = df_piv_merged.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f69dd4",
   "metadata": {},
   "source": [
    "### 3. Create binary negative affect score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71817bda-85dc-4820-abe0-c44f70d96bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Calculate the mode of 'mean_pa' and 'mean_na' for each customer\n",
    "def calculate_customer_modes(df):\n",
    "    # Group by customer and apply mode calculation\n",
    "    modes = df.groupby('customer').agg({\n",
    "        'mean_pa': lambda x: x.mode()[0] if not x.mode().empty else None,  # Mode for mean_pa\n",
    "        'mean_na': lambda x: x.mode()[0] if not x.mode().empty else None   # Mode for mean_na\n",
    "    })\n",
    "    return modes\n",
    "\n",
    "# Step 2: Add the mode values as new columns to the original dataframe\n",
    "modes = calculate_customer_modes(df_piv_merged)\n",
    "\n",
    "# Merge the mode values back to the original dataframe\n",
    "df_piv_merged = df_piv_merged.merge(modes, on='customer', suffixes=('', '_mode'))\n",
    "\n",
    "# Step 3: Create new columns for the binary classification based on mode comparison\n",
    "def label_rows(df):\n",
    "    # Label 'mean_pa' as 1 if above mode, otherwise 0\n",
    "    df['mean_pa_label'] = (df['mean_pa'] < df['mean_pa_mode']).astype(int)\n",
    "    \n",
    "    # Label 'mean_na' as 1 if below mode (improvement), otherwise 0\n",
    "    df['mean_na_label'] = (df['mean_na'] > df['mean_na_mode']).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the labeling function\n",
    "df_piv_merged = label_rows(df_piv_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c91189-5a50-4d6f-9ffe-8555ceee0d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_piv_merged.groupby(\"mean_na_label\")[\"customer\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856211f9",
   "metadata": {},
   "source": [
    "### 4. Analyse situation (evaluation) variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51269f5d-55ef-4a92-895d-5401e3515066",
   "metadata": {},
   "source": [
    "#### 4.1 Blau Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541acad-a1ec-4021-8118-e8f4d4104a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_piv_merged_sit = df_piv_merged[[\"customer\", \"situation1\", 'situation1_simplified',\"absolute_day_index\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c43635-3754-41ed-a56c-495612a6066f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming df_piv_merged is your DataFrame\n",
    "# Step 1: Calculate the proportion of each situation in the entire dataset\n",
    "situation_counts = df_piv_merged['situation1'].value_counts()\n",
    "total_counts = situation_counts.sum()\n",
    "proportions = situation_counts / total_counts\n",
    "\n",
    "# Step 2: Calculate the Blau Index across all individuals\n",
    "squared_proportions = proportions ** 2\n",
    "blau_index_across_individuals = 1 - np.sum(squared_proportions)\n",
    "\n",
    "print(f\"Blau Index Across Individuals: {blau_index_across_individuals:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76496ec-8685-404d-8d45-d07a7bb53607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Define a function to calculate the Blau Index for a single individual\n",
    "def calculate_blau_index(group):\n",
    "    situation_counts = group['situation1'].value_counts()\n",
    "    total_counts = situation_counts.sum()\n",
    "    proportions = situation_counts / total_counts\n",
    "    squared_proportions = proportions ** 2\n",
    "    blau_index = 1 - np.sum(squared_proportions)\n",
    "    return blau_index\n",
    "\n",
    "# Step 2: Apply this function to each individual\n",
    "blau_index_within_individuals = df_piv_merged.groupby('customer').apply(calculate_blau_index).reset_index(name='blau_index')\n",
    "# Step 3: Merge the BLAU Index back into the original DataFrame\n",
    "df_piv_merged = pd.merge(df_piv_merged, blau_index_within_individuals, on='customer', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365b78c-17f4-45a7-a81f-11a3ae7cef28",
   "metadata": {},
   "source": [
    "## 5. Match with Redcap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec7e21-0e70-4755-9267-2ad83fe68b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redcap_full = pd.merge(df_redcap, df_piv_merged, on= \"customer\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33bb7a2-472e-45d2-91dd-3dca12a21ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_piv_merged.groupby(\"customer\")[\"unique_day_id\"].nunique().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0205dc-0ca9-46ef-bbac-74e156696a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_piv_merged.groupby(\"customer\")[\"unique_day_id\"].nunique().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43a834-27bd-45a7-8095-5d25623e36c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_piv_merged.groupby(\"customer\")[\"unique_day_id\"].nunique().std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e090d6-0f8e-48bd-9197-b9de59a2c95a",
   "metadata": {},
   "source": [
    "#### 5.1 Situation distribution across categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd389c-b760-44e8-8034-3349df8dab00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the situations of interest using the simplified labels\n",
    "situation_diagnosis_dist = df_redcap_full.loc[df_redcap_full['situation1_simplified'].isin(situation_colors.keys())]\n",
    "\n",
    "# Group by 'scid_cv_description', 'customer', and 'unique_day_id' to count unique occurrences of each situation\n",
    "situation_diagnosis_dist = situation_diagnosis_dist.groupby(\n",
    "    ['scid_cv_description', 'customer', 'unique_day_id', 'situation1_simplified']\n",
    ").size().reset_index(name='count')\n",
    "\n",
    "# Aggregate counts by 'scid_cv_description' and 'situation1_simplified'\n",
    "situation_diagnosis_dist = situation_diagnosis_dist.groupby(\n",
    "    ['scid_cv_description', 'situation1_simplified']\n",
    ")['count'].sum().reset_index()\n",
    "\n",
    "\n",
    "# Calculate the total number of unique assessments per customer, then sum per diagnosis\n",
    "assessment_counts = df_redcap_full.groupby(['scid_cv_description', 'customer'])['unique_day_id'].nunique().reset_index(name='unique_days_per_customer')\n",
    "assessment_counts = assessment_counts.groupby('scid_cv_description')['unique_days_per_customer'].sum().reset_index(name='total_assessments')\n",
    "\n",
    "# Merge the counts with the assessment counts\n",
    "situation_diagnosis_dist = pd.merge(situation_diagnosis_dist, assessment_counts, on='scid_cv_description')\n",
    "\n",
    "# Normalize the counts by the total number of assessments with each diagnosis\n",
    "situation_diagnosis_dist['normalized_count'] = situation_diagnosis_dist['count'] / situation_diagnosis_dist['total_assessments'] * 100\n",
    "\n",
    "# Define the three categories to be included in the plot\n",
    "selected_situations = [\"smartphone - social media\", \"travelling\", \"eating - drinking - selfcare\"]\n",
    "\n",
    "# Filter the situation_diagnosis_dist DataFrame to only include these three categories\n",
    "filtered_situation_diagnosis_dist = situation_diagnosis_dist[situation_diagnosis_dist['situation1_simplified'].isin(selected_situations)]\n",
    "\n",
    "# Create an empty figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Loop through each situation in the selected categories and add a scatter trace\n",
    "for situation in selected_situations:\n",
    "    filtered_data = filtered_situation_diagnosis_dist[filtered_situation_diagnosis_dist['situation1_simplified'] == situation]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=filtered_data['scid_cv_description'],\n",
    "            y=filtered_data['normalized_count'],\n",
    "            mode='lines+markers',  # Add both lines and dots\n",
    "            name=situation,\n",
    "            marker=dict(\n",
    "                color=situation_colors.get(situation, '#d3d3d3'),  # Default color if not in situation_colors\n",
    "                size=8  # Make the dots a bit smaller for better readability with lines\n",
    "            ),\n",
    "            line=dict(\n",
    "                color=situation_colors.get(situation, '#d3d3d3'),  # Same color for the lines\n",
    "                width=2\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Update layout to adjust figure size and titles\n",
    "fig.update_layout(\n",
    "    width=1200,  # Width for the entire figure\n",
    "    height=600,  # Adjusted height for easier stacking\n",
    "    font=dict(size=16),  # Increase font size\n",
    "    title_text=\"Normalized Distribution of Selected Situations per Diagnosis (by Unique Assessments)\",\n",
    "    xaxis_title=\"Diagnosis\",\n",
    "    yaxis_title=\"Normalized Count\",\n",
    "    legend_title_text=\"Situation Type\",\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caae778-4458-4fd6-bac7-dcaa72488e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Filter for the three diagnoses of interest\n",
    "diagnoses_of_interest = ['Depressive Disorder', 'Social Anxiety Disorder', 'Obsessive-Compulsive Disorder']\n",
    "situation_diagnosis_dist = situation_diagnosis_dist[situation_diagnosis_dist['scid_cv_description'].isin(diagnoses_of_interest)]\n",
    "\n",
    "# Get the top 3 situations for each diagnosis\n",
    "top_situations = situation_diagnosis_dist.groupby('scid_cv_description').apply(lambda x: x.nlargest(3, 'normalized_count')).reset_index(drop=True)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,  # 1 row, 3 columns\n",
    "    subplot_titles=(\"Depressive\", \"Social Anxiety\", \"OCD\"),  # Set your custom titles here\n",
    "    column_widths=[0.3, 0.3, 0.3]  # Adjust column width to make plots smaller\n",
    ")\n",
    "\n",
    "# Add bar plots for each diagnosis\n",
    "for i, diagnosis in enumerate(diagnoses_of_interest):\n",
    "    diagnosis_data = top_situations[top_situations['scid_cv_description'] == diagnosis]\n",
    "    if not diagnosis_data.empty:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=diagnosis_data['situation1_simplified'],\n",
    "                y=diagnosis_data['normalized_count'],\n",
    "                marker_color=[situation_colors[sit] for sit in diagnosis_data['situation1_simplified']],\n",
    "                textposition='none'  # Remove percentages from bars\n",
    "            ),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "\n",
    "# Update layout with increased font size\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=1000,\n",
    "    showlegend=False,\n",
    "    font=dict(size=14),  # Increase font size\n",
    "    margin=dict(l=50, r=50, t=50, b=50)\n",
    ")\n",
    "\n",
    "# Update y-axis to range from 0 to 100%\n",
    "fig.update_yaxes(title_text='Normalized Count (%)', range=[0, 100], row=1, col=1)\n",
    "fig.update_yaxes(range=[0, 100], row=1, col=2)\n",
    "fig.update_yaxes(range=[0, 100], row=1, col=3)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ca409-b8b6-49cf-a344-b3aedbdf0589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter data for single situations using the `situation_count` variable\n",
    "single_situations = df_redcap_full[df_redcap_full['situation_count'] == 1]\n",
    "\n",
    "# Count occurrences of each single situation\n",
    "top_single_situations = single_situations['situation1_simplified'].value_counts().nlargest(3).reset_index()\n",
    "top_single_situations.columns = ['situation1_simplified', 'count']\n",
    "\n",
    "# Identify situations that were frequently selected with others (situation_count > 1)\n",
    "frequent_with_others = df_redcap_full[df_redcap_full['situation_count'] > 1]\n",
    "top_frequent_with_others = frequent_with_others['situation1_simplified'].value_counts().nlargest(3).reset_index()\n",
    "top_frequent_with_others.columns = ['situation1_simplified', 'count']\n",
    "\n",
    "# Group by customer and unique_day_id and apply the renamed situations\n",
    "combinations = frequent_with_others.groupby(['customer', 'unique_day_id'])['situation1_simplified'].apply(lambda x: ', '.join(sorted(x))).reset_index()\n",
    "\n",
    "# Get the top 3 combinations\n",
    "combinations_count = combinations['situation1_simplified'].value_counts().nlargest(3).reset_index()\n",
    "combinations_count.columns = ['combination', 'count']\n",
    "\n",
    "# Prepare data for the stacked bar plot\n",
    "situation_combinations = combinations_count['combination'].apply(lambda x: x.split(', '))\n",
    "top_combination_data = pd.DataFrame({\n",
    "    'Combination': [f'Combination {i+1}' for i in range(len(combinations_count))],\n",
    "    'Situations': situation_combinations,\n",
    "    'Count': combinations_count['count']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364540c-d2d7-4399-9ef8-6278dd71ea9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Remove rows with NaN or Inf in the relevant columns\n",
    "df_clean = df_redcap_full[['customer','blau_index', 'bsi_gsi_base', 'age']].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Drop NaN rows to clean the data\n",
    "df_clean = df_clean.dropna()\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "# Create subplots: 1 row, 2 columns with reduced spacing\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"BLAU Index vs Symptom Severity\", \"BLAU Index vs Age\"), shared_yaxes=True, column_widths=[0.45, 0.45], horizontal_spacing=0.05)\n",
    "\n",
    "# Scatterplot for BSI GSI Base vs BLAU Index\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_clean['bsi_gsi_base'], \n",
    "        y=df_clean['blau_index'], \n",
    "        mode='markers',\n",
    "        marker=dict(color='blue'),\n",
    "        name='Symptom Severity'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Fit a linear regression model for BSI GSI Base vs BLAU Index\n",
    "model_bsi = LinearRegression().fit(df_clean[['bsi_gsi_base']], df_clean['blau_index'])\n",
    "trendline_bsi = model_bsi.predict(df_clean[['bsi_gsi_base']])\n",
    "\n",
    "# Add trendline for BSI GSI Base plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_clean['bsi_gsi_base'],\n",
    "        y=trendline_bsi,\n",
    "        mode='lines',\n",
    "        line=dict(color='red'),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Scatterplot for Age vs BLAU Index\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_clean['age'], \n",
    "        y=df_clean['blau_index'], \n",
    "        mode='markers',\n",
    "        marker=dict(color='green'),\n",
    "        name='Age'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Fit a linear regression model for Age vs BLAU Index\n",
    "model_age = LinearRegression().fit(df_clean[['age']], df_clean['blau_index'])\n",
    "trendline_age = model_age.predict(df_clean[['age']])\n",
    "\n",
    "# Add trendline for Age plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_clean['age'],\n",
    "        y=trendline_age,\n",
    "        mode='lines',\n",
    "        line=dict(color='red'),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update the layout to adjust figure size and titles\n",
    "fig.update_layout(\n",
    "    width=1200,  # Increased width\n",
    "    height=400,  # Adjust height\n",
    "    title_text=\"Scatterplots of BLAU Index vs Symptom Severity and Age\",\n",
    "    title_font=dict(size=20)\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Symptom Severity (BSI GSI)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Age\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"BLAU Index\", row=1, col=1)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e55d529-3165-4909-9c93-8d6a62d988dc",
   "metadata": {},
   "source": [
    "#### 5.3 Situation complexity/ clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992554b-2f36-4a5b-8015-51ad2b68f736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Filter to get unique entries per customer and unique_day_id combination for situation count per assessment\n",
    "df_unique_per_assessment = df_piv_merged.drop_duplicates(subset=['customer', 'unique_day_id'])\n",
    "\n",
    "# Total number of unique (customer, unique_day_id) combinations for both plots\n",
    "total_unique_customer_day_id_combinations = df_unique_per_assessment.shape[0]\n",
    "\n",
    "# Group by the simplified situation1_simplified column for the first plot (Situation category count)\n",
    "situation_counts = df_piv_merged.groupby(\"situation1_simplified\")[\"customer\"].count().reset_index()\n",
    "situation_counts = situation_counts.sort_values(by=\"customer\", ascending=False)\n",
    "\n",
    "# Normalize situation_counts by the total number of unique (customer, unique_day_id) combinations\n",
    "situation_counts['percentage'] = (situation_counts['customer'] / total_unique_customer_day_id_combinations) * 100\n",
    "\n",
    "# For the second plot, group by \"situation_count\" using the filtered unique data\n",
    "situation_count_counts = df_unique_per_assessment.groupby(\"situation_count\")[\"customer\"].count().reset_index()\n",
    "\n",
    "# Normalize situation_count_counts by the total number of unique (customer, unique_day_id) combinations\n",
    "situation_count_counts['percentage'] = (situation_count_counts['customer'] / total_unique_customer_day_id_combinations) * 100\n",
    "\n",
    "# Make sure all situation_count categories are represented\n",
    "all_situation_counts = list(range(int(df_unique_per_assessment[\"situation_count\"].min()), int(df_unique_per_assessment[\"situation_count\"].max()) + 1))\n",
    "situation_count_counts = situation_count_counts.set_index(\"situation_count\").reindex(all_situation_counts, fill_value=0).reset_index()\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,  # 1 row, 2 columns\n",
    "    subplot_titles=(\"\", \"\")\n",
    ")\n",
    "\n",
    "# Bar plot for simplified situation1_simplified with color mapping\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=situation_counts[\"situation1_simplified\"],\n",
    "        y=situation_counts[\"percentage\"],  # Use percentage for normalized values\n",
    "        marker=dict(color=[situation_colors[sit] for sit in situation_counts[\"situation1_simplified\"]])\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Bar plot for situation_count with no text inside bars (filtered for unique entries)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=situation_count_counts[\"situation_count\"],\n",
    "        y=situation_count_counts[\"percentage\"],  # Use percentage for normalized values\n",
    "        marker=dict(color=situation_colors['other']),  # Using the color for 'other' as an example\n",
    "        width=0.7  # Adjust width as needed\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    showlegend=False,  # Hide legends as the colors are self-explanatory\n",
    "    title_text=\"\",\n",
    "    margin=dict(l=100, r=100, t=100, b=150),  # Adjust margins\n",
    "    plot_bgcolor='rgba(0,0,0,0)',  # Transparent background\n",
    "    barmode='group'  # Set barmode to group to control bar width manually\n",
    ")\n",
    "\n",
    "# Set both y-axes to have the same range from 0 to 100%\n",
    "fig.update_yaxes(range=[0, 70], title_text=\"Percentage (%)\", row=1, col=1)\n",
    "fig.update_yaxes(range=[0, 70], title_text=\"Percentage (%)\", row=1, col=2)\n",
    "\n",
    "# Customize the x-axis labels\n",
    "fig.update_xaxes(title_text=\"\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"\", row=1, col=2)\n",
    "\n",
    "# Ensure all x-axis ticks are shown for situation_count\n",
    "fig.update_xaxes(tickmode='linear', dtick=1, row=1, col=2)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52fa87b-7a67-4d2d-b31c-7847dab15e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Group by the simplified situation1_simplified column\n",
    "situation_counts = df_piv_merged[[\"customer\", \"unique_day_id\",\"situation1_simplified\", \"situation_count\"]]\n",
    "situation_counts = situation_counts.groupby(\"situation1_simplified\")[\"customer\"].count().reset_index()\n",
    "situation_counts = situation_counts.sort_values(by=\"customer\", ascending=False)\n",
    "\n",
    "# Group by \"situation_count\" and count the number of \"customer\"\n",
    "situation_count_counts = df_piv_merged.groupby(\"situation_count\")[\"customer\"].count().reset_index()\n",
    "\n",
    "# Make sure all situation_count categories are represented\n",
    "all_situation_counts = list(range(int(df_piv_merged[\"situation_count\"].min()), int(df_piv_merged[\"situation_count\"].max()) + 1))\n",
    "situation_count_counts = situation_count_counts.set_index(\"situation_count\").reindex(all_situation_counts, fill_value=0).reset_index()\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,  # 1 row, 2 columns\n",
    "    subplot_titles=(\"Situation category count\", \"Situation count per assessment\")\n",
    ")\n",
    "\n",
    "# Bar plot for simplified situation1_simplified with color mapping\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=situation_counts[\"situation1_simplified\"],\n",
    "        y=situation_counts[\"customer\"],\n",
    "        marker=dict(color=[situation_colors[sit] for sit in situation_counts[\"situation1_simplified\"]])\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Bar plot for situation_count with no text inside bars\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=situation_count_counts[\"situation_count\"],\n",
    "        y=situation_count_counts[\"customer\"],\n",
    "        marker=dict(color=situation_colors['other']),  # Using the color for 'other' as an example\n",
    "        width=0.7  # Adjust width as needed\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    showlegend=False,  # Hide legends as the colors are self-explanatory\n",
    "    title_text=\"\",\n",
    "    margin=dict(l=100, r=100, t=100, b=150),  # Adjust margins\n",
    "    plot_bgcolor='rgba(0,0,0,0)',  # Transparent background\n",
    "    barmode='group'  # Set barmode to group to control bar width manually\n",
    ")\n",
    "\n",
    "# Customize the axes labels\n",
    "fig.update_xaxes(title_text=\"\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "\n",
    "# Ensure all x-axis ticks are shown for situation_count\n",
    "fig.update_xaxes(tickmode='linear', dtick=1, row=1, col=2)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91371655-f2b0-4ec8-b582-60cb7a464be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_redcap_age = df_redcap_full.loc[df_redcap_full.age >18][[\"customer\", \"age\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d72a5-91f0-425c-9ef2-fe3271cb146d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming df_redcap_full is your DataFrame\n",
    "grouped_data = df_redcap_full.groupby(\"scid_cv_description\")[\"customer\"].nunique().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "grouped_data.columns = ['scid_cv_description', 'unique_customers']\n",
    "\n",
    "# Apply color mapping to the scid_cv_description categories\n",
    "colors = [diagnosis_colors[desc] for desc in grouped_data['scid_cv_description']]\n",
    "\n",
    "# Create a subplot with 1 row and 1 column\n",
    "fig = make_subplots(rows=1, cols=1, specs=[[{'type':'domain'}]])\n",
    "\n",
    "# Add the pie chart to the subplot\n",
    "fig.add_trace(go.Pie(labels=grouped_data['scid_cv_description'], \n",
    "                     values=grouped_data['unique_customers'], \n",
    "                     textinfo='label',  # Show only labels (no percentages)\n",
    "                     textposition='outside',   # Place labels outside the pie chart\n",
    "                     showlegend=True,\n",
    "                     marker=dict(colors=colors)),  # Apply the color mapping here\n",
    "              1, 1)\n",
    "\n",
    "# Update the layout to position the legend inside the plot area\n",
    "fig.update_layout(\n",
    "    title_text='Unique Customers per SCID CV Description',\n",
    "    annotations=[dict(text='', x=0.5, y=0.5, font_size=20, showarrow=False)],\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207b2b4-4667-4696-916a-e33d00fbe488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculate mean age\n",
    "mean_age = df_redcap_age['age'].mean()\n",
    "\n",
    "# Violin plot for age\n",
    "fig = px.violin(df_redcap_age, y='age', \n",
    "                title=\"Age Distribution\",\n",
    "                labels={'age': 'Age'},\n",
    "                box=True,  # Shows box plot inside the violin\n",
    "                height=400, \n",
    "                width=400)\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    plot_bgcolor='rgba(0,0,0,0)'\n",
    ")\n",
    "\n",
    "# Add annotation for the mean age\n",
    "fig.add_annotation(\n",
    "    text=f\"Mean Age: {mean_age:.2f}\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.5, y=-0.15,\n",
    "    showarrow=False,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0bb35-37f6-433f-93b6-fcc4153e42d3",
   "metadata": {},
   "source": [
    "## 6. Match with Passive data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1d799-a99f-4e42-b7fa-cc8fa58bc7bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = df_ema_passive.merge(df_redcap_full, on=[\"customer\", \"unique_day_id\", \"assess\"], how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db12ec-04e5-4a92-91d9-7757e9e7bb48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768098bc-129b-4a11-836d-d783b2c91e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['quest_hour'] = df_final['sensor_block_end'].dt.hour\n",
    "def categorize_time_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 14:\n",
    "        return 'noon'\n",
    "    elif 14 <= hour < 18:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'evening'\n",
    "\n",
    "df_final['time_of_day'] = df_final['quest_hour'].apply(categorize_time_of_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac1205-469d-4642-8603-514f1b78ee7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def categorize_season(date):\n",
    "    if pd.isna(date):  # Check if the value is NaT\n",
    "        return None  # Return None (or you can choose another label like 'Unknown')\n",
    "\n",
    "    day_of_year = date.timetuple().tm_yday\n",
    "\n",
    "    if 80 <= day_of_year < 172:\n",
    "        return 'spring'\n",
    "    elif 172 <= day_of_year < 264:\n",
    "        return 'summer'\n",
    "    elif 264 <= day_of_year < 355:\n",
    "        return 'autumn'\n",
    "    else:\n",
    "        return 'winter'\n",
    "\n",
    "df_final['season'] = df_final['sensor_block_end'].apply(categorize_season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb4f95-e0c6-4569-9b14-a98230aea2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final_red = df_final.loc[df_final.situation_count ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea52479-54a9-4994-a825-47e82ece4c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the average of the continuous variables per situation\n",
    "averages = df_final_red.groupby('situation1_simplified').agg({\n",
    "    'n_GPS': 'mean',\n",
    "    'n_steps': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Sort the DataFrame for n_GPS and n_steps\n",
    "averages_sorted_gps = averages.sort_values(by='n_GPS', ascending=False)\n",
    "averages_sorted_steps = averages.sort_values(by='n_steps', ascending=False)\n",
    "\n",
    "# Create a subplot figure with 1 row and 2 columns (side by side)\n",
    "fig = make_subplots(rows=1, cols=2, \n",
    "                    subplot_titles=(\"Average Number of GPS Points per Situation\", \"Average Number of Steps per Situation\"),\n",
    "                    horizontal_spacing=0.15)  # Adjust horizontal spacing as needed\n",
    "\n",
    "# First subplot for n_GPS\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=averages_sorted_gps['situation1_simplified'], \n",
    "        y=averages_sorted_gps['n_GPS'],\n",
    "        marker=dict(color=[situation_colors.get(sit, '#d3d3d3') for sit in averages_sorted_gps['situation1_simplified']]),\n",
    "        name='Number of GPS Points'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Second subplot for n_steps\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=averages_sorted_steps['situation1_simplified'], \n",
    "        y=averages_sorted_steps['n_steps'],\n",
    "        marker=dict(color=[situation_colors.get(sit, '#d3d3d3') for sit in averages_sorted_steps['situation1_simplified']]),\n",
    "        name='Number of Steps'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout to set titles and size\n",
    "fig.update_layout(\n",
    "    height=500,  # Adjust height to fit both subplots\n",
    "    width=1000,   # Adjust width\n",
    "    title_text=\"Average GPS Points and Steps per Situation\",\n",
    "    showlegend=False  # Hide legend (since color is self-explanatory)\n",
    ")\n",
    "\n",
    "# Update axes for both subplots\n",
    "fig.update_xaxes(title_text=\"Situation\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Average Number of GPS Points\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Situation\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Average Number of Steps\", row=1, col=2)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb83a3-4e11-4248-8336-18cd7220f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_final_red_red = df_final.loc[df_final.at_home_binary !=-1]\n",
    "# Calculate the average of total_distance_km and transition_minutes per situation\n",
    "averages = df_final_red.groupby('situation1_simplified').agg({\n",
    "    'total_distance_km': 'mean',\n",
    "    'transition_minutes': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Sort the DataFrame for total_distance_km and transition_minutes\n",
    "averages_sorted_distance = averages.sort_values(by='total_distance_km', ascending=False)\n",
    "averages_sorted_transition = averages.sort_values(by='transition_minutes', ascending=False)\n",
    "\n",
    "# Create a subplot figure with 1 row and 2 columns\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2, \n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]],  # Separate y-axes for both subplots\n",
    "    horizontal_spacing=0.1  # Adjust spacing between subplots\n",
    ")\n",
    "\n",
    "# First subplot for total_distance_km\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=averages_sorted_distance['situation1_simplified'], \n",
    "        y=averages_sorted_distance['total_distance_km'],\n",
    "        marker=dict(color=[situation_colors.get(sit, '#d3d3d3') for sit in averages_sorted_distance['situation1_simplified']]),\n",
    "        name='Total Distance (km)'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Second subplot for transition_minutes\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=averages_sorted_transition['situation1_simplified'], \n",
    "        y=averages_sorted_transition['transition_minutes'],\n",
    "        marker=dict(color=[situation_colors.get(sit, '#d3d3d3') for sit in averages_sorted_transition['situation1_simplified']]),\n",
    "        name='Transition Minutes'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout to set titles and adjust size\n",
    "fig.update_layout(\n",
    "    height=500,  # Adjust height to fit both subplots\n",
    "    width=1000,  # Adjust width\n",
    "    title_text=\"\",\n",
    "    showlegend=False  # Hide legend (since color is self-explanatory)\n",
    ")\n",
    "\n",
    "# Update axes for both subplots\n",
    "fig.update_xaxes(title_text=\"\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Average Distance Travelled (km)\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Average Transition Minutes\", row=1, col=2)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a77709-35dd-4312-800e-b97bd8b84889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " df_final_red_red = df_final.loc[df_final.at_home_binary !=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3bc25-4080-40cb-b5b6-333e07a60354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the average of at_home_minute and transition_minutes per situation\n",
    "averages = df_final_red_red.groupby('situation1_simplified').agg({\n",
    "    'at_home_minute': 'mean',\n",
    "    'transition_minutes': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Sort the DataFrame for at_home_minute and transition_minutes\n",
    "averages_sorted_home = averages.sort_values(by='at_home_minute', ascending=False)\n",
    "averages_sorted_transition = averages.sort_values(by='transition_minutes', ascending=False)\n",
    "\n",
    "# Create a subplot figure with 1 row and 2 columns\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2, \n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]],  # Separate y-axes for both subplots\n",
    "    horizontal_spacing=0.1  # Adjust spacing between subplots\n",
    ")\n",
    "\n",
    "# First subplot for at_home_minute\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=averages_sorted_home['situation1_simplified'], \n",
    "        y=averages_sorted_home['at_home_minute'],\n",
    "        marker=dict(color=[situation_colors.get(sit, '#d3d3d3') for sit in averages_sorted_home['situation1_simplified']]),\n",
    "        name='At Home Minutes'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Second subplot for transition_minutes\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=averages_sorted_transition['situation1_simplified'], \n",
    "        y=averages_sorted_transition['transition_minutes'],\n",
    "        marker=dict(color=[situation_colors.get(sit, '#d3d3d3') for sit in averages_sorted_transition['situation1_simplified']]),\n",
    "        name='Transition Minutes'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout to set titles and adjust size\n",
    "fig.update_layout(\n",
    "    height=500,  # Adjust height to fit both subplots\n",
    "    width=1000,  # Adjust width\n",
    "    title_text=\"Average At Home and Transition Minutes per Situation\",\n",
    "    showlegend=False  # Hide legend (since color is self-explanatory)\n",
    ")\n",
    "\n",
    "# Update axes for both subplots\n",
    "fig.update_xaxes(title_text=\"\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Average At Home Minutes\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Average Transition Minutes\", row=1, col=2)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ac9f8-8025-4ff0-9c66-69686ceaafd1",
   "metadata": {},
   "source": [
    "## 7. Multiclass prediction of situations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecda1d-30ed-4ad3-8cea-9837dc364f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import ClassifierChain, MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984efbc4-017c-4827-bcb9-7818b2fc87cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = df_final[['n_steps', 'n_GPS','total_distance_km',  'transition_minutes','transition','situation1', \n",
    "                 'customer', 'unique_day_id', 'at_home_binary', 'at_home_minute', \"weekend\", \"weekday\", \"season\", \"quest_hour\", \"time_of_day\"]]\n",
    "#data_add = df_final[['n_steps', 'n_GPS','total_distance_km', 'at_home', 'situation1', 'customer', 'unique_day_id',  \n",
    " #                    \"weekend\", \"weekday\", \"season\", \"quest_hour\", \"time_of_day\"]]\n",
    "# Example\n",
    "numerical_features = [\"n_steps\", \"n_GPS\", \"total_distance_km\", \"transition_minutes\", \"at_home_minute\"]  # replace with your numerical feature names\n",
    "categorical_features = [\"transition\", \"at_home_binary\", \"weekend\", \"weekday\", \"season\", \"quest_hour\", \"time_of_day\"]  # replace with your categorical feature names\n",
    "categorical_features_add = ['transition',\"weekend\", \"weekday\", \"season\", \"quest_hour\", \"time_of_day\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b6ca3-f488-46b2-9825-38add82e3f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Identify the 6 most frequent categories in 'situation1'\n",
    "top_6_situations = data['situation1'].value_counts().nlargest(9).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332418c-f090-4ec8-8613-4387d1a1ddbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Filter the dataframe to only include rows where 'situation1' is in the top 6 categories\n",
    "filtered_df = data[data['situation1'].isin(top_6_situations)]\n",
    "\n",
    "# Step 1: One-hot encode the 'situation1' column to create binary indicator columns\n",
    "situation_dummies = pd.get_dummies(filtered_df['situation1'], prefix='situation')\n",
    "\n",
    "# Step 2: Add the binary situation columns back to the original dataframe\n",
    "df_with_dummies = pd.concat([filtered_df, situation_dummies], axis=1)\n",
    "\n",
    "# Step 3: Group by both 'customer' and 'unique_day_id' and aggregate using max for binary columns\n",
    "aggregated_df = df_with_dummies.groupby(['customer', 'unique_day_id']).agg({\n",
    "    'n_steps': 'first',           # For numerical features, we'll assume first value for the day\n",
    "    'n_GPS': 'first',\n",
    "    'total_distance_km': 'first',\n",
    "    'transition_minutes': 'first',\n",
    "    'transition': 'first',        # Categorical feature\n",
    "    'at_home_binary': 'first',    # Binary feature (first because there should be one per day)\n",
    "    'at_home_minute': 'first',\n",
    "    'weekend': 'first',           # Categorical/binary features\n",
    "    'weekday': 'first',\n",
    "    'season': 'first',\n",
    "    'quest_hour': 'first',\n",
    "    'time_of_day': 'first',\n",
    "    # Binary columns for situations (max ensures that if situation was selected, it is recorded as 1)\n",
    "    **{col: 'max' for col in situation_dummies.columns}  \n",
    "}).reset_index()\n",
    "\n",
    "# Now 'aggregated_df' has one row per unique_day_id and binary indicator columns for each situation\n",
    "\n",
    "# Display the reshaped dataframe\n",
    "aggregated_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9bd72-395d-4743-af71-5228220664e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aggregated_df = aggregated_df.loc[aggregated_df.at_home_binary != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a7f02-ae6e-48b2-8fd2-171913ec51cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score, classification_report\n",
    "\n",
    "# Function to apply log1p (log(1 + x)) to numerical features\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "\n",
    "# ColumnTransformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('log', log_transformer),  # Apply log(1 + x) transformation\n",
    "            ('scale', StandardScaler())  # Then apply standard scaling\n",
    "        ]), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)  # Ensure OneHotEncoder works\n",
    "    ])\n",
    "\n",
    "# Base model (RandomForest) for the classifier chain\n",
    "base_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Use 'aggregated_df' for the features and target labels\n",
    "X = aggregated_df.drop(columns=['customer', 'unique_day_id'] + situation_dummies.columns.tolist())  # Drop non-feature and situation columns\n",
    "\n",
    "# Target labels (top situations)\n",
    "y_filtered = aggregated_df[situation_dummies.columns.tolist()]  # Add your binary situation columns\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "# Number of classifier chains to use in the ensemble\n",
    "n_chains = 10\n",
    "\n",
    "# Create an empty array to store the predictions\n",
    "y_pred_ensemble = np.zeros((X_test.shape[0], y_test.shape[1]))\n",
    "\n",
    "# Train multiple classifier chains with different random orders\n",
    "for i in range(n_chains):\n",
    "    print(f\"Training classifier chain {i+1}/{n_chains}...\")\n",
    "    chain = ClassifierChain(base_model, order='random', random_state=i)  # Random order for each chain\n",
    "    \n",
    "    # Create a pipeline with preprocessing and the current classifier chain\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', chain)])\n",
    "    \n",
    "    # Train the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions and add to the ensemble prediction array\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_ensemble += y_pred\n",
    "\n",
    "# Step 8: Aggregate the predictions by majority voting (for binary labels)\n",
    "y_pred_final = (y_pred_ensemble >= (n_chains / 2)).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_final)\n",
    "hamming = hamming_loss(y_test, y_pred_final)\n",
    "f1_micro = f1_score(y_test, y_pred_final, average='micro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Hamming Loss: {hamming}')\n",
    "print(f'Micro-averaged F1-score: {f1_micro}')\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=y_filtered.columns, zero_division=0))\n",
    "\n",
    "# Function to calculate subset accuracy (exact match ratio)\n",
    "def subset_accuracy_score(y_true, y_pred):\n",
    "    return np.mean(np.all(y_true == y_pred, axis=1))\n",
    "\n",
    "# Calculate subset accuracy\n",
    "subset_accuracy = subset_accuracy_score(y_test, y_pred_final)\n",
    "print(f'Subset Accuracy (Exact Match Ratio): {subset_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7cfa29-732c-4400-8aa2-ff5174c8c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_auc_score\n",
    "\n",
    "data = df_final[['n_steps', 'n_GPS','total_distance_km',  'transition_minutes','transition', 'mean_na_label','situation1',\n",
    "                 'customer', 'unique_day_id', 'at_home_binary', 'at_home_minute', \"weekend\", \"weekday\", \"season\", \"quest_hour\", \"time_of_day\"]]\n",
    "#data_add = df_final[['n_steps', 'n_GPS','total_distance_km', 'at_home', 'situation1', 'customer', 'unique_day_id',  \n",
    " #                    \"weekend\", \"weekday\", \"season\", \"quest_hour\", \"time_of_day\"]]\n",
    "# Example\n",
    "numerical_features = [\"n_steps\", \"n_GPS\", \"total_distance_km\", \"transition_minutes\", \"at_home_minute\"]  # replace with your numerical feature names\n",
    "categorical_features = [\"transition\", \"at_home_binary\", \"weekend\", \"weekday\", \"season\", \"quest_hour\", \"time_of_day\"]  # replace with your categorical feature names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08831f-8878-47df-bf63-f8c7a11631b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: One-hot encode the 'situation1' column to create binary indicator columns\n",
    "situation_dummies = pd.get_dummies(data['situation1'], prefix='situation')\n",
    "\n",
    "# Step 2: Add the binary situation columns back to the original dataframe\n",
    "df_with_dummies = pd.concat([data, situation_dummies], axis=1)\n",
    "\n",
    "# Step 3: Group by both 'customer' and 'unique_day_id' and aggregate using max for binary columns\n",
    "aggregated_df = df_with_dummies.groupby(['customer', 'unique_day_id']).agg({\n",
    "    'n_steps': 'first',           # For numerical features, we'll assume first value for the day\n",
    "    'n_GPS': 'first',\n",
    "    'total_distance_km': 'first',\n",
    "    'transition_minutes': 'first',\n",
    "    'transition': 'first',        # Categorical feature\n",
    "    'at_home_binary': 'first',    # Binary feature (first because there should be one per day)\n",
    "    'at_home_minute': 'first',\n",
    "    'weekend': 'first',           # Categorical/binary features\n",
    "    'weekday': 'first',\n",
    "    'season': 'first',\n",
    "    'quest_hour': 'first',\n",
    "    'time_of_day': 'first',\n",
    "    'mean_na_label':'first',\n",
    "    # Binary columns for situations (max ensures that if situation was selected, it is recorded as 1)\n",
    "    **{col: 'max' for col in situation_dummies.columns}  \n",
    "}).reset_index()\n",
    "\n",
    "# Now 'aggregated_df' has one row per unique_day_id and binary indicator columns for each situation\n",
    "\n",
    "# Display the reshaped dataframe\n",
    "aggregated_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c095f8-3ad4-4edd-8391-810fb623a226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aggregated_df = aggregated_df.loc[aggregated_df.at_home_binary != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966eb54f-cda7-47c3-b200-7502ada4ed91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to apply log1p (log(1 + x)) to numerical features\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "\n",
    "# ColumnTransformer for preprocessing (numerical and categorical features)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('log', log_transformer),  # Apply log(1 + x) transformation\n",
    "            ('scale', StandardScaler())  # Then apply standard scaling\n",
    "        ]), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# RandomForestClassifier as the model for binary classification\n",
    "classifier = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Create a pipeline with preprocessing and RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', classifier)])\n",
    "\n",
    "# Use 'aggregated_df_with_dummies' for the features and target labels\n",
    "# X: Features including dummy-encoded 'situation' variables as predictors\n",
    "X = aggregated_df.drop(columns=['customer', 'unique_day_id', 'mean_na_label'])  # Drop non-feature and target columns\n",
    "\n",
    "# y: Target variable (mean_na_label) is now binary\n",
    "y = aggregated_df['mean_na_label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model using the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1-score: {f1}')\n",
    "print(f'ROC-AUC score: {roc_auc}')\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e87bc-d817-44dd-b19f-bfb97a70efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182db654-9b65-46b1-871a-33a38661a59e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "# Initialize the SHAP explainer for the RandomForestClassifier\n",
    "explainer = shap.TreeExplainer(base_classifier)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize the SHAP values for a single prediction\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], X_test.iloc[0,:])\n",
    "\n",
    "# Summary plot of SHAP values across all test samples\n",
    "shap.summary_plot(shap_values[0], X_test, feature_names=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ec89b-32d3-4d1e-b6e1-6312280f0324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tiki]",
   "language": "python",
   "name": "conda-env-.conda-tiki-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
