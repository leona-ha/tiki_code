{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"TIKI: Data Compliance and Quality\"\n",
    "author: \"Leona Hammelrath\"\n",
    "date: \"10 April 2024\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "\n",
    "format: html\n",
    "execute:\n",
    "    eval: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "First Data for Passive Data Collection using Smartwatches and GPS from the PREACT Study. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Treatment personalization is highly discussed to counteract insufficient response rates in psychotherapy. In the quest for criteria allowing informed selection or adaptation, ambulatory assessment data (i.e. EMA, passive sensing)are a key component, as processes happening outside of therapy sessions can be depicted in high temporal and/or spatial resolution.\n",
    "\n",
    "PREACT is a multicenter prospective-longitudinal study investigating different predictors of non-response (i.e. EEG, fMRI) in around 500 patients undergoing cognitive behavioral therapy for internalizing disorders (https://forschungsgruppe5187.de/de). \n",
    "\n",
    "## Methods\n",
    "Patients can enroll for therapy-accompanying ambulatory assessment. They are provided with a customized study app and a state-of-the-art smartwatch collecting passive data like GPS and heart rate for up to 365 days. In parallel, three 14-day EMA phases (pre-, mid- and post-therapy) cover transdiagnostic (i.e. emotion regulation), contextual and therapy-related aspects.  \n",
    "\n",
    "Here, we present first results on data compliance and quality for the passive sensing data as well as EMA assessments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m file_list \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(file_pattern_back_1)\n\u001b[1;32m     43\u001b[0m file_list\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m---> 44\u001b[0m df_backup_small \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat((pd\u001b[38;5;241m.\u001b[39mread_csv(f, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m file_list), ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# big backup passive data\u001b[39;00m\n\u001b[1;32m     48\u001b[0m file_pattern_back_2 \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(datapath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw/tiki_backup_files/tiki_backup_*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Adjust the path and extension if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    373\u001b[0m     objs,\n\u001b[1;32m    374\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    375\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    376\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    377\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    378\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    379\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    380\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    381\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    382\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    383\u001b[0m )\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    426\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "# If your current working directory is the notebooks directory, use this:\n",
    "notebook_dir = os.getcwd()  # current working directory\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from config import datapath, proj_sheet,preprocessed_path\n",
    "\n",
    "today = date.today().strftime(\"%d%m%Y\")\n",
    "today_day = pd.to_datetime('today').normalize()\n",
    "\n",
    "today = \"29072024\"\n",
    "\n",
    "# actual passive + ema_data\n",
    "datapath1 = datapath + f\"raw/export_tiki_{today}/\"\n",
    "file_pattern = os.path.join(datapath1, \"epoch_part*.csv\")\n",
    "file_list = glob.glob(file_pattern)\n",
    "file_list.sort()\n",
    "df_complete = pd.concat((pd.read_csv(f, encoding=\"latin-1\", low_memory=False) for f in file_list), ignore_index=True)\n",
    "\n",
    "# small backup passive data\n",
    "#file_pattern_back_1 = os.path.join(datapath, 'raw/tiki_backup_files/export_tiki_27052024/\"epoch_part*.csv\"')\n",
    "file_pattern_back_1 = os.path.join(datapath, 'raw/tiki_backup_files/export_tiki_16072024/\"epoch_part*.csv\"')  # Adjust the path and extension if needed\n",
    "\n",
    "backup_files = glob.glob(file_pattern_back_1)\n",
    "file_list = glob.glob(file_pattern_back_1)\n",
    "file_list.sort()\n",
    "df_backup_small = pd.concat((pd.read_csv(f, encoding=\"latin-1\", low_memory=False) for f in file_list), ignore_index=True)\n",
    "\n",
    "\n",
    "# big backup passive data\n",
    "file_pattern_back_2 = os.path.join(datapath, 'raw/tiki_backup_files/tiki_backup_*.csv')  # Adjust the path and extension if needed\n",
    "big_backup_files = glob.glob(file_pattern_back_2)\n",
    "\n",
    "dataframes = []\n",
    "for file in big_backup_files:\n",
    "    df_backup = pd.read_csv(file, encoding=\"latin-1\", low_memory=False)  # Adjust read_csv parameters as needed\n",
    "    # Extract the date from the filename\n",
    "    filename_parts = os.path.basename(file).split('_')\n",
    "    date_str = filename_parts[2]\n",
    "    time_suffix = int(filename_parts[3].split('.')[0])  # Convert the suffix to an integer\n",
    "    date = pd.to_datetime(date_str)\n",
    "    df_backup['file_date'] = date\n",
    "    df_backup['time_suffix'] = time_suffix\n",
    "    dataframes.append(df_backup)\n",
    "\n",
    "df_backup_big = pd.concat(dataframes, ignore_index=True)\n",
    "df_backup_big = df_backup_big.sort_values(by=['file_date', 'time_suffix'])\n",
    "\n",
    "df_monitoring = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{proj_sheet}/export?format=csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = df_monitoring.copy()\n",
    "df_monitoring.rename(columns = {\"Pseudonym\": \"customer\", \"EMA_ID\": \"ema_id\", \"Status\": \"status\",\n",
    "                                \"Studienversion\":\"study_version\", \"FOR_ID\":\"for_id\", \n",
    "                           \"Start EMA Baseline\": \"ema_base_start\", \"Ende EMA Baseline\": \"ema_base_end\", \n",
    "                           \"Freischaltung/ Start EMA T20\": \"ema_t20_start\",\"Ende EMA T20\":\"ema_t20_end\", \n",
    "                               \"Termin 1. Gespräch\": \"first_call_date\", \"Freischaltung/ Start EMA Post\":\"ema_post_start\",\n",
    "                               \"Ende EMA Post\":\"ema_post_end\", \"T20=Post\":\"t20_post\" }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring[\"customer\"] = df_monitoring[\"customer\"].str[:4]\n",
    "df_monitoring[\"for_id\"] = df_monitoring.for_id.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring[\"ema_base_start\"] = pd.to_datetime(df_monitoring[\"ema_base_start\"], dayfirst=True)\n",
    "df_monitoring[\"ema_base_end\"] = pd.to_datetime(df_monitoring[\"ema_base_end\"], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = df_monitoring[['for_id', 'ema_id', 'customer', 'study_version', 'status',\n",
    "       't20_post', 'ema_base_start', 'ema_base_end',\n",
    "       'first_call_date', 'ema_t20_start', 'ema_t20_end',\n",
    "       'ema_post_start', 'ema_post_end']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1 Import epoch level passive + GPS data\n",
    "\n",
    "df_complete[\"customer\"] = df_complete.customer.str.split(\"@\").str.get(0)\n",
    "df_complete[\"customer\"] = df_complete[\"customer\"].str[:4]\n",
    "\n",
    "df_complete[\"start_end\"] = df_complete[\"endTimestamp\"] - df_complete[\"startTimestamp\"]\n",
    "df_complete[\"startTimestamp\"] = pd.to_datetime(df_complete[\"startTimestamp\"],unit='ms')\n",
    "df_complete[\"endTimestamp\"] = pd.to_datetime(df_complete[\"endTimestamp\"],unit='ms')\n",
    "df_complete[\"createdAt\"] = pd.to_datetime(df_complete[\"createdAt\"],unit='ms')\n",
    "\n",
    "\n",
    "## Timezone offset \n",
    "\n",
    "# Handle NaN in timezoneOffset by skipping or replacing with a default\n",
    "# Here, we skip adjustments for NaN offsets by using np.where to check for NaN\n",
    "df_complete['startTimestamp'] = np.where(df_complete['timezoneOffset'].isna(),\n",
    "                               df_complete['startTimestamp'],  # If NaN, keep original timestamp\n",
    "                               df_complete['startTimestamp'] + pd.to_timedelta(df_complete['timezoneOffset'], unit='m'))  # Else, apply offset\n",
    "\n",
    "df_complete['createdAt'] = np.where(df_complete['createdAt'].isna(),\n",
    "                               df_complete['createdAt'],  # If NaN, keep original timestamp\n",
    "                               df_complete['createdAt'] + pd.to_timedelta(df_complete['timezoneOffset'], unit='m'))  # Else, apply offset\n",
    "\n",
    "df_complete['endTimestamp'] = np.where(df_complete['endTimestamp'].isna(),\n",
    "                               df_complete['endTimestamp'],  # If NaN, keep original timestamp\n",
    "                               df_complete['endTimestamp'] + pd.to_timedelta(df_complete['timezoneOffset'], unit='m'))  # Else, apply offset\n",
    "\n",
    "\n",
    "df_complete[\"startTimestamp_day\"] = df_complete.startTimestamp.dt.normalize()\n",
    "df_complete[\"createdAt_day\"] = df_complete.createdAt.dt.normalize()\n",
    "\n",
    "df_complete[\"startTimestamp_hour\"] = df_complete.startTimestamp.dt.hour\n",
    "df_complete[\"createdAt_hour\"] = df_complete.createdAt.dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types after conversion to datetime:\n",
      "customer                                     object\n",
      "source                                       object\n",
      "type                                         object\n",
      "startTimestamp            datetime64[ns, UTC+01:00]\n",
      "endTimestamp              datetime64[ns, UTC+01:00]\n",
      "doubleValue                                 float64\n",
      "longValue                                   float64\n",
      "booleanValue                                 object\n",
      "dateValue                                   float64\n",
      "stringValue                                  object\n",
      "generation                                   object\n",
      "trustworthiness                              object\n",
      "medicalGrade                                 object\n",
      "userReliability                             float64\n",
      "chronologicalExactness                      float64\n",
      "timezoneOffset                              float64\n",
      "file_date                            datetime64[ns]\n",
      "time_suffix                                   int64\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_backup_complete' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m df_backup_big[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_backup_big[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_timedelta(df_backup_big[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimezoneOffset\u001b[39m\u001b[38;5;124m'\u001b[39m], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Format the datetime objects to the desired format\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m df_backup_big[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_backup_complete[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m df_backup_big[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_backup_complete[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m df_backup_big[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_backup_big[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_backup_complete' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert the 'startTimestamp' and 'endTimestamp' columns to datetime objects\n",
    "df_backup_big['startTimestamp'] = pd.to_datetime(df_backup_big['startTimestamp'], errors='coerce')\n",
    "df_backup_big['endTimestamp'] = pd.to_datetime(df_backup_big['endTimestamp'], errors='coerce')\n",
    "# Convert the 'startTimestamp' and 'endTimestamp' columns to datetime objects\n",
    "df_backup_big['startTimestamp'] = pd.to_datetime(df_backup_big['startTimestamp'], errors='coerce')\n",
    "df_backup_big['endTimestamp'] = pd.to_datetime(df_backup_big['endTimestamp'], errors='coerce')\n",
    "\n",
    "# Debug: Print data types after conversion\n",
    "print(\"\\nData types after conversion to datetime:\")\n",
    "print(df_backup_big.dtypes)\n",
    "\n",
    "# Adjust for timezone offset\n",
    "df_backup_big['startTimestamp'] = df_backup_big['startTimestamp'] + pd.to_timedelta(df_backup_big['timezoneOffset'], unit='m')\n",
    "df_backup_big['endTimestamp'] = df_backup_big['endTimestamp'] + pd.to_timedelta(df_backup_big['timezoneOffset'], unit='m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Format the datetime objects to the desired format\n",
    "df_backup_big['startTimestamp'] = df_backup_big['startTimestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_backup_big['endTimestamp'] = df_backup_big['endTimestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_backup_big['startTimestamp'] = pd.to_datetime(df_backup_big['startTimestamp'], errors='coerce')\n",
    "df_backup_big['endTimestamp'] = pd.to_datetime(df_backup_big['endTimestamp'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all 3 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer', 'source', 'type', 'startTimestamp', 'endTimestamp',\n",
       "       'doubleValue', 'longValue', 'booleanValue', 'dateValue', 'stringValue',\n",
       "       'generation', 'trustworthiness', 'medicalGrade', 'userReliability',\n",
       "       'chronologicalExactness', 'timezoneOffset', 'file_date', 'time_suffix'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_timestamp_big = df_backup_big['startTimestamp'].max()\n",
    "\n",
    "# Filter the second dataframe to include only entries after the latest timestamp\n",
    "df_backup_small_filtered = df_backup_small[df_backup_small['startTimestamp'] > latest_timestamp_big]\n",
    "\n",
    "# Concatenate the first dataframe with the filtered second dataframe\n",
    "result_df = pd.concat([df_backup_big, df_backup_small_filtered], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer', 'source', 'startTimestamp', 'endTimestamp', 'type',\n",
       "       'valueType', 'doubleValue', 'longValue', 'booleanValue', 'dateValue',\n",
       "       'stringValue', 'generation', 'trustworthiness', 'medicalGrade',\n",
       "       'userReliability', 'chronologicalExactness', 'timezoneOffset',\n",
       "       'createdAt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_timestamp_big = result_df['startTimestamp'].max()\n",
    "\n",
    "# Filter the second dataframe to include only entries after the latest timestamp\n",
    "df_complete_filtered = df_backup_small[df_backup_small['startTimestamp'] > latest_timestamp_big]\n",
    "\n",
    "# Concatenate the first dataframe with the filtered second dataframe\n",
    "result_df = pd.concat([df_backup_big, df_backup_small_filtered], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest timestamp from the first dataframe\n",
    "latest_timestamp = df1['timestamp'].max()\n",
    "\n",
    "# Filter the second dataframe to include only entries after the latest timestamp\n",
    "df2_filtered = df2[df2['timestamp'] > latest_timestamp]\n",
    "\n",
    "# Concatenate the first dataframe with the filtered second dataframe\n",
    "result = pd.concat([df1, df2_filtered], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract GPS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract GPS data\n",
    "df_loc_complete = df_complete[df_complete.type.isin([\"Latitude\", \"Longitude\"])]\n",
    "df_loc_complete = df_loc_complete[[\"customer\", \"startTimestamp\",\"endTimestamp\",\"type\", \"doubleValue\", \n",
    "                           'createdAt', 'startTimestamp_day', 'createdAt_day',\n",
    "       'startTimestamp_hour', 'createdAt_hour']]\n",
    "df_loc_complete = df_loc_complete.merge(df_monitoring, on=\"customer\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the earliest 'startTimestamp_day' for each customer\n",
    "earliest_timestamp_per_customer = df_loc_complete.groupby('customer')['startTimestamp_day'].min()\n",
    "\n",
    "# Map the earliest timestamp back to the original dataframe\n",
    "df_loc_complete['earliest_start_day'] = df_loc_complete['customer'].map(earliest_timestamp_per_customer)\n",
    "\n",
    "# Calculate 'relative_day' as the difference in days from the earliest day\n",
    "df_loc_complete['relative_day'] = (df_loc_complete['startTimestamp_day'] - df_loc_complete['earliest_start_day']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc_complete['potential_days_coverage'] = (today_day - df_loc_complete['earliest_start_day']).dt.days\n",
    "\n",
    "condition = (\n",
    "    (df_loc_complete['status'] == 'Abgeschlossen') & \n",
    "    (df_loc_complete['study_version'].isin(['Kurz', 'Kurz (Wechsel/Abbruch)']))\n",
    ")\n",
    "\n",
    "df_loc_complete['potential_days_coverage'] = np.where(\n",
    "    condition,\n",
    "    (df_loc_complete['ema_base_end'] - df_loc_complete['earliest_start_day']).dt.days,\n",
    "    df_loc_complete['potential_days_coverage']\n",
    ")\n",
    "\n",
    "# Count unique days with data for each customer\n",
    "actual_days = df_loc_complete.groupby('customer')['startTimestamp_day'].nunique()\n",
    "\n",
    "# Mapping the actual number of days back to the DataFrame\n",
    "df_loc_complete['actual_days_with_data'] = df_loc_complete['customer'].map(actual_days)\n",
    "\n",
    "df_loc_complete['data_coverage_per'] = (df_loc_complete['actual_days_with_data'] / df_loc_complete['potential_days_coverage']) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_complete = df_complete[~df_complete.type.isin([\"Latitude\", \"Longitude\"])]\n",
    "df_pd_complete = df_pd_complete[[\"customer\", \"startTimestamp\", \"endTimestamp\",\"start_end\",\"type\", \"doubleValue\", 'longValue', 'booleanValue', 'dateValue',\n",
    "       'stringValue',\"timezoneOffset\", 'createdAt', 'startTimestamp_day', 'createdAt_day',\n",
    "       'startTimestamp_hour', 'createdAt_hour']]\n",
    "df_pd_complete = df_pd_complete.merge(df_monitoring, on=\"customer\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the earliest 'startTimestamp_day' for each customer\n",
    "earliest_timestamp_per_customer = df_pd_complete.groupby('customer')['startTimestamp_day'].min()\n",
    "\n",
    "# Map the earliest timestamp back to the original dataframe\n",
    "df_pd_complete['earliest_start_day'] = df_pd_complete['customer'].map(earliest_timestamp_per_customer)\n",
    "\n",
    "# Calculate 'relative_day' as the difference in days from the earliest day\n",
    "df_pd_complete['relative_day'] = (df_pd_complete['startTimestamp_day'] - df_pd_complete['earliest_start_day']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_complete['potential_days_coverage'] = (today_day - df_pd_complete['earliest_start_day']).dt.days\n",
    "\n",
    "condition = (\n",
    "    (df_pd_complete['status'] == 'Abgeschlossen') & \n",
    "    (df_pd_complete['study_version'].isin(['Kurz', 'Kurz (Wechsel/Abbruch)']))\n",
    ")\n",
    "\n",
    "df_pd_complete['potential_days_coverage'] = np.where(\n",
    "    condition,\n",
    "    (df_pd_complete['ema_base_end'] - df_pd_complete['earliest_start_day']).dt.days,\n",
    "    df_pd_complete['potential_days_coverage']\n",
    ")\n",
    "# Count unique days with data for each customer\n",
    "actual_days = df_pd_complete.groupby('customer')['startTimestamp_day'].nunique()\n",
    "\n",
    "# Mapping the actual number of days back to the DataFrame\n",
    "df_pd_complete['actual_days_with_data'] = df_pd_complete['customer'].map(actual_days)\n",
    "\n",
    "df_pd_complete['data_coverage_per'] = (df_pd_complete['actual_days_with_data'] / df_pd_complete['potential_days_coverage']) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(preprocessed_path + f'/passive_data.pkl', 'wb') as file:\n",
    "    pickle.dump(df_pd_complete, file)\n",
    "    \n",
    "with open(preprocessed_path + f'/gps_data.pkl', 'wb') as file:\n",
    "    pickle.dump(df_loc_complete, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "session = pd.read_csv(datapath1 + \"questionnaireSession.csv\",low_memory=False)\n",
    "answers = pd.read_csv(datapath1 + \"answers.csv\", low_memory=False)\n",
    "choice = pd.read_csv(datapath1 + \"choice.csv\",low_memory=False)\n",
    "questions = pd.read_csv(datapath1 + \"questions.csv\",low_memory=False)\n",
    "questionnaire = pd.read_csv(datapath1 + \"questionnaires.csv\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session data\n",
    "session[\"user\"] = session[\"user\"].str[:4]\n",
    "session.rename(columns = {\"user\":\"customer\",\"completedAt\": \"quest_complete\", \"createdAt\": \"quest_create\", \"expirationTimestamp\": \"quest_expir\"}, inplace=True)\n",
    "session[\"quest_create\"] = (pd.to_datetime(session[\"quest_create\"],unit='ms'))\n",
    "session[\"quest_complete\"] = (pd.to_datetime(session[\"quest_complete\"],unit='ms'))\n",
    "\n",
    "df_sess = session[[\"customer\", \"sessionRun\", \"quest_create\", \"quest_complete\", \"study\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer data \n",
    "answers[\"user\"] = answers[\"user\"].str[:4]\n",
    "answers = answers[[\"user\", \"questionnaireSession\", \"questionnaire\", \"study\", \n",
    "                   \"question\", \"order\",\"element\", \"createdAt\"]]\n",
    "answers[\"createdAt\"] = (pd.to_datetime(answers[\"createdAt\"],unit='ms'))\n",
    "answers.rename(columns={\"user\":\"customer\",\"questionnaireSession\":\"session_unique\", \"createdAt\": \"quest_create\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item description data\n",
    "choice = choice[[\"element\", \"choice_id\", \"text\", \"question\"]]\n",
    "choice.rename(columns={\"text\":\"choice_text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question description data\n",
    "questions = questions[[\"id\", \"title\"]]\n",
    "questions.rename(columns={\"id\":\"question\",\"title\":\"quest_title\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire = questionnaire[[\"id\", \"name\"]]\n",
    "questionnaire.rename(columns={\"id\":\"questionnaire\",\"name\":\"questionnaire_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_merged = pd.merge(answers, choice, on= [\"question\",\"element\"])\n",
    "answer_merged = pd.merge(answer_merged, questions, on= \"question\")\n",
    "answer_merged = pd.merge(answer_merged, questionnaire, on= \"questionnaire\")\n",
    "answer_merged[\"quest_complete_day\"] = answer_merged.quest_create.dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_merged = pd.merge(answer_merged, df_monitoring, on = \"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = pd.merge(df_sess, df_monitoring, on = \"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess[['customer', 'sessionRun', 'quest_create', 'quest_complete', 'study',\n",
    "       'for_id', 'ema_id', 'study_version', 'status', 't20_post',\n",
    "       'ema_base_start', 'ema_base_end','ema_t20_start', 'ema_t20_end',\n",
    "       'ema_post_start', 'ema_post_end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.copy()\n",
    "df_sess[\"quest_create_day\"] = df_sess.quest_create.dt.normalize()\n",
    "df_sess[\"quest_complete_day\"] = df_sess.quest_complete.dt.normalize()\n",
    "\n",
    "df_sess[\"quest_create_hour\"] = df_sess.quest_create.dt.hour\n",
    "df_sess[\"quest_complete_hour\"] = df_sess.quest_complete.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of completed EMA beeps in first phase\n",
    "df_sess1 = df_sess.loc[df_sess.study.isin([24,25])]\n",
    "df_sess1 = df_sess1.copy()\n",
    "\n",
    "df_sess2 = df_sess.loc[df_sess.study.isin([33,34])]\n",
    "df_sess2 = df_sess2.copy()\n",
    "\n",
    "df_sess3 = df_sess.loc[df_sess.study.isin([38,39])]\n",
    "df_sess3 = df_sess3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess1['quest_complete_relative1'] = (df_sess1['quest_complete_day'] - df_sess1['ema_base_start']).dt.days\n",
    "\n",
    "\n",
    "sess_count1 = df_sess1.dropna(subset=[\"quest_complete\"]).groupby(\"customer\")[\"quest_complete\"].size()\\\n",
    ".reset_index()\n",
    "sess_count1 = sess_count1.rename(columns = {\"quest_complete\":\"nquest_EMA1\"})\n",
    "\n",
    "# count number of completed EMA beeps in second phase\n",
    "sess_count2 = df_sess2.dropna(subset=[\"quest_complete\"]).groupby(\"customer\")[\"quest_complete\"].size()\\\n",
    ".reset_index()\n",
    "sess_count2 = sess_count2.rename(columns = {\"quest_complete\":\"nquest_EMA2\"})\n",
    "\n",
    "# count number of completed EMA beeps in second phase\n",
    "sess_count3 = df_sess3.dropna(subset=[\"quest_complete\"]).groupby(\"customer\")[\"quest_complete\"].size()\\\n",
    ".reset_index()\n",
    "sess_count3 = sess_count3.rename(columns = {\"quest_complete\":\"nquest_EMA3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daily_counts = df_sess1.groupby(['customer', 'quest_complete_day','quest_complete_relative1']).size().reset_index(name='daily_entries_sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.merge(sess_count1, on=['customer'], how='left')\n",
    "df_sess = df_sess.merge(sess_count2, on=['customer'], how='left')\n",
    "df_sess = df_sess.merge(sess_count3, on=['customer'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(preprocessed_path + f'/ema_data.pkl', 'wb') as file:\n",
    "    pickle.dump(df_sess, file)\n",
    "    \n",
    "with open(preprocessed_path + f'/monitoring_data.pkl', 'wb') as file:\n",
    "    pickle.dump(df_monitoring, file)\n",
    "    \n",
    "with open(preprocessed_path + f'/ema_content.pkl', 'wb') as file:\n",
    "    pickle.dump(answer_merged, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
