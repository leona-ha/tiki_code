{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755b4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import date, datetime\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from config import proj_sheet, datapath, credential_path, drivesheet_url\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "scopes = ['https://www.googleapis.com/auth/spreadsheets',\n",
    "          'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "credentials = Credentials.from_service_account_file(credential_path, scopes=scopes)\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "today = date.today() \n",
    "week_ago = today - dt.timedelta(days=7)\n",
    "today = today.strftime(\"%d%m%Y\")\n",
    "week_ago = week_ago.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# passive + ema_data\n",
    "filepath = datapath + f\"export_{today}.csv\"\n",
    "datapath1 = datapath + f\"export_tiki_{today}/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d386d",
   "metadata": {},
   "source": [
    "## 1. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310925d",
   "metadata": {},
   "source": [
    "### 1.1 Import epoch level passive + GPS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e997b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath1 = datapath + f\"export_tiki_{today}/\"\n",
    "filepath_1 = datapath1 + \"epoch_part0001.csv\"\n",
    "filepath_2 = datapath1 + \"epoch_part0002.csv\"\n",
    "filepath_3 = datapath1 + \"epoch_part0003.csv\"\n",
    "filepath_4 = datapath1 + \"epoch_part0004.csv\"\n",
    "filepath_5 = datapath1 + \"epoch_part0005.csv\"\n",
    "filepath_6 = datapath1 + \"epoch_part0006.csv\"\n",
    "filepath_7= datapath1 + \"epoch_part0007.csv\"\n",
    "filepath_8= datapath1 + \"epoch_part0008.csv\"\n",
    "filepath_9= datapath1 + \"epoch_part0009.csv\"\n",
    "filepath_10= datapath1 + \"epoch_part0010.csv\"\n",
    "filepath_11= datapath1 + \"epoch_part0011.csv\"\n",
    "filepath_12= datapath1 + \"epoch_part0012.csv\"\n",
    "filepath_13= datapath1 + \"epoch_part0013.csv\"\n",
    "#filepath_14= datapath1 + \"epoch_part0014.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bdf154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(filepath_1, encoding= \"latin-1\", low_memory=False)\n",
    "df_2 = pd.read_csv(filepath_2, encoding= \"latin-1\", low_memory=False)\n",
    "df_3 = pd.read_csv(filepath_3, encoding= \"latin-1\", low_memory=False)\n",
    "df_4 = pd.read_csv(filepath_4, encoding= \"latin-1\", low_memory=False)\n",
    "df_5 = pd.read_csv(filepath_5, encoding= \"latin-1\", low_memory=False)\n",
    "df_6 = pd.read_csv(filepath_6, encoding= \"latin-1\", low_memory=False)\n",
    "df_7 = pd.read_csv(filepath_7, encoding= \"latin-1\", low_memory=False)\n",
    "df_8 = pd.read_csv(filepath_8, encoding= \"latin-1\", low_memory=False)\n",
    "df_9 = pd.read_csv(filepath_9, encoding= \"latin-1\", low_memory=False)\n",
    "df_10 = pd.read_csv(filepath_10, encoding= \"latin-1\", low_memory=False)\n",
    "df_11 = pd.read_csv(filepath_11, encoding= \"latin-1\", low_memory=False)\n",
    "df_12 = pd.read_csv(filepath_12, encoding= \"latin-1\", low_memory=False)\n",
    "df_13 = pd.read_csv(filepath_13, encoding= \"latin-1\", low_memory=False)\n",
    "#df_14 = pd.read_csv(filepath_14, encoding= \"latin-1\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c21b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10, df_11, df_12, df_13])#, df_14])\n",
    "df_complete[\"customer\"] = df_complete.customer.str.split(\"@\").str.get(0)\n",
    "df_complete[\"customer\"] = df_complete.customer.str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "028ae944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete[\"startTimestamp\"] = pd.to_datetime(df_complete[\"startTimestamp\"],unit='ms')\n",
    "df_complete[\"createdAt\"] = pd.to_datetime(df_complete[\"createdAt\"],unit='ms')\n",
    "\n",
    "df_complete[\"startTimestamp_day\"] = df_complete.startTimestamp.dt.strftime('%Y/%m/%d')\n",
    "df_complete[\"createdAt_day\"] = df_complete.startTimestamp.dt.strftime('%Y/%m/%d')\n",
    "\n",
    "df_complete[\"startTimestamp_hour\"] = df_complete.startTimestamp.dt.hour\n",
    "df_complete[\"createdAt_hour\"] = df_complete.startTimestamp.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054db53",
   "metadata": {},
   "source": [
    "### 1.2 Import passive data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f53acf",
   "metadata": {},
   "source": [
    "### Check location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2665c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location data\n",
    "df_loc_complete = df_complete[df_complete.type.isin([\"Latitude\", \"Longitude\"])]\n",
    "df_loc_complete = df_loc_complete[[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\", \n",
    "                           \"timezoneOffset\"]]\n",
    "df_loc_complete[\"startTimestamp\"] = (pd.to_datetime(df_loc_complete[\"startTimestamp\"],unit='ms'))\n",
    "\n",
    "df_loc_complete = df_loc_complete.groupby(\"customer\")[[\"startTimestamp\"]].max().rename_axis(None, axis=1).reset_index()\n",
    "df_loc_complete.rename(columns={\"startTimestamp\":\"last_day_GPS\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82bb03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passive data\n",
    "df_pd_complete = df_complete[df_complete.type.isin([\"HeartRate\", \"AtrialFibrillationDetection\", \"RawECGVoltage\", \n",
    "                                                   \"ActiveBurnedCalories\", 'SleepAwakeBinary','SleepBinary'])]\n",
    "                                                    \n",
    "df_pd_complete = df_pd_complete[[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\", \"timezoneOffset\"]]\n",
    "df_pd_complete[\"startTimestamp\"] = (pd.to_datetime(df_pd_complete[\"startTimestamp\"],unit='ms'))\n",
    "\n",
    "df_pd_complete = df_pd_complete.groupby(\"customer\")[[\"startTimestamp\"]].max().rename_axis(None, axis=1).reset_index()\n",
    "df_pd_complete.rename(columns={\"startTimestamp\":\"last_day_passive\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e91cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_int = pd.read_csv(filepath, encoding= \"latin-1\")\n",
    "#df_int.drop([\"Quelle\", \"Created At\"], axis=1, inplace=True)\n",
    "\n",
    "#df_int[\"Endkunde\"] = df_int.Endkunde.str.split(\"@\").str.get(0)\n",
    "#df_int[\"Endkunde\"] = df_int[\"Endkunde\"].str[:4]\n",
    "#df_int.rename(columns={\"Endkunde\":\"customer\",\"Tag\":\"day\", \"Typ\":\"type\",\"Wert(Fließkommazahl)\": \"float\", \"Wert(Ganzzahl)\": \"int\"}, inplace=True)\n",
    "\n",
    "#df_int[\"day\"] = pd.to_datetime(df_int[\"day\"], format=\"%d.%m.%Y\")\n",
    "#df_int[\"int\"].fillna(df_int[\"float\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ecbf04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_int[\"int\"].fillna(df_int[\"float\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120eb125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_int= df_int[[\"customer\", \"day\", \"type\", \"int\"]]\n",
    "#df_int = df_int.pivot(\n",
    "#    index=[\"customer\", \"day\"],\n",
    "#    columns=\"type\",\n",
    "#    values=\"int\")\n",
    "#df_int.reset_index(level=\"day\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e96be",
   "metadata": {},
   "source": [
    "### 1.3 Import montoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186f5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project management data\n",
    "df_sheet = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{proj_sheet}/export?format=csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fbc79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = df_sheet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f941d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = df_monitoring[['FOR_ID', 'EMA_ID', 'Pseudonym', 'Studienversion', 'Status',\n",
    "       'Besonderes', 'Start EMA Baseline', 'Ende EMA Baseline',\n",
    "       'Terminpräferenz', 'Termin 1. Gespräch', \n",
    "       'Begrüßungsmail verschickt?', 'Telefonat stattgefunden?',\n",
    "       'Reminder-Mail Wechsel verschickt?', 'Baseline T20 Update verschickt?', 'Freischaltung/ Start EMA T20',\n",
    "       'Ende EMA T20', 'Post T20 Update verschickt?',\n",
    "       'Studienende/ Dropout Mail verschickt?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2eb4935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ys/gx19nmhj6v30dlkkh5nfp5xh0000gn/T/ipykernel_30218/197733145.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_monitoring.rename(columns = {\"Pseudonym\": \"customer\", \"EMA_ID\": \"ema_id\", \"Status\": \"status\",\n"
     ]
    }
   ],
   "source": [
    "df_monitoring.rename(columns = {\"Pseudonym\": \"customer\", \"EMA_ID\": \"ema_id\", \"Status\": \"status\",\n",
    "                                \"Studienversion\":\"study_version\", \"FOR_ID\":\"for_id\", \n",
    "                           \"Start EMA Baseline\": \"ema_base_start\", \"Ende EMA Baseline\": \"ema_base_end\", \n",
    "                           \"Freischaltung/ Start EMA T20\": \"ema_t20_start\",\"Ende EMA T20\":\"ema_t20_end\", \n",
    "                               \"Termin 1. Gespräch\": \"first_call_date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d704c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ys/gx19nmhj6v30dlkkh5nfp5xh0000gn/T/ipykernel_30218/1246132069.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_monitoring[\"customer\"] = df_monitoring[\"customer\"].str[:4]\n"
     ]
    }
   ],
   "source": [
    "df_monitoring[\"customer\"] = df_monitoring[\"customer\"].str[:4]\n",
    "df_active = df_monitoring.copy()\n",
    "df_active = df_active[[\"customer\", \"ema_id\", \"ema_base_end\", \"ema_base_start\", \"study_version\", \"for_id\", \"status\"]]\n",
    "df_active[\"for_id\"] = df_active.for_id.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b8da5",
   "metadata": {},
   "source": [
    "### 1.4 Import connection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a046414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_con = pd.read_csv(datapath + f\"export_connection_{today}.csv\", encoding= \"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e279fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_con[\"Customer\"] = df_con.Customer.str.split(\"@\").str.get(0)\n",
    "#df_con[\"Customer\"] = df_con[\"Customer\"].str[:4]\n",
    "#df_con.drop([\"Data Source\", \"Timestamp\"], axis=1, inplace=True)\n",
    "#df_con.rename(columns = {\"Customer\": \"customer\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_con = pd.merge(df_active, df_con, on= \"customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c156e",
   "metadata": {},
   "source": [
    "### 1.5 Import EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c30bfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "session = pd.read_csv(datapath1 + \"questionnaireSession.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f426b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session data\n",
    "session[\"user\"] = session[\"user\"].str[:4]\n",
    "session.rename(columns = {\"user\":\"customer\",\"completedAt\": \"quest_complete\", \"createdAt\": \"quest_create\", \"expirationTimestamp\": \"quest_expir\"}, inplace=True)\n",
    "session[\"quest_create\"] = (pd.to_datetime(session[\"quest_create\"],unit='ms'))\n",
    "session[\"quest_complete\"] = (pd.to_datetime(session[\"quest_complete\"],unit='ms'))\n",
    "df_sess = session[[\"customer\", \"sessionRun\", \"quest_create\", \"quest_complete\", \"study\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f670143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of completed EMA beeps in first phase\n",
    "df_sess1 = df_sess.loc[df_sess.study.isin([24,25])]\n",
    "sess_count1 = df_sess1.dropna(subset=[\"quest_complete\"]).groupby(\"customer\")[\"quest_complete\"].size()\\\n",
    ".reset_index()\n",
    "sess_count1 = sess_count1.rename(columns = {\"quest_complete\":\"nquest_EMA1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92fb3433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of completed EMA beeps in second phase\n",
    "df_sess2 = df_sess.loc[df_sess.study.isin([33,34])]\n",
    "sess_count2 = df_sess2.dropna(subset=[\"quest_complete\"]).groupby(\"customer\")[\"quest_complete\"].size()\\\n",
    ".reset_index()\n",
    "sess_count2 = sess_count2.rename(columns = {\"quest_complete\":\"nquest_EMA2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622eb18",
   "metadata": {},
   "source": [
    "## 2. Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ba3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge monitoring and passive\n",
    "#df_merged = pd.merge(df_int, df_active, on=\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fef120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge monitoring and passive\n",
    "df_merged = pd.merge(df_pd_complete, df_active, on=\"customer\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2053e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge monitoring and connection\n",
    "#df_merged = pd.merge(df_merged, df_con, on=\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2516c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge monitoring and EMA\n",
    "df_merged = pd.merge(df_merged, sess_count1, on=\"customer\", how=\"outer\")\n",
    "df_merged = pd.merge(df_merged, sess_count2, on=\"customer\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34a4cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_merged, df_loc_complete, on=\"customer\", how=\"outer\")\n",
    "df_merged.to_csv(f\"data_compliance_{today}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6daa16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged[df_merged['status'].isin(['Erhebung_1_aktiv', 'Post_Erhebung_1', 'Post_Erhebung_2','Erhebung_2_aktiv'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate last day with passive data\n",
    "#grouped = df_merged.groupby([\"customer\"])[[\"ema_id\",\"for_id\",\"ema_base_start\",\"ema_base_end\",\"study_version\",\n",
    "#                                            \"status\",\"Is Connected\",\"nquest_EMA1\", \"nquest_EMA2\",\"day\"]].max() # last day with passive data\n",
    "\n",
    "#grouped_active = grouped[~(grouped['status'].isin([\"Dropout\", \"Pin_missing\", \"Abgeschlossen\"]))]\n",
    "\n",
    "#grouped_active = grouped_active.rename_axis(None, axis=1).reset_index()\n",
    "#grouped_active.rename(columns = {\"day\": \"last_day_passive\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429c7e9",
   "metadata": {},
   "source": [
    "## 3. Check missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa935458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get structure of the google spreadsheet\n",
    "df = pd.read_csv(\"https://docs.google.com/spreadsheets/d/1z8LZJBBMzzAmiXIS47X8SLk-zSMwDIXSKPit4IlmfuE/export?format=csv&gid=1512138040\")\n",
    "df = df.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c9d4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_remove = [\"FOR11001\", \"FOR11034\", \"FOR13023\", \"FOR14013\", \"FOR14064\"]\n",
    "remove_scanwatch = [\"FOR14029\", \"FOR14055\"]\n",
    "remove_gps = [\"FOR13013\", \"FOR14014\", \"FOR13019\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a907530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged[~df_merged['for_id'].isin(users_to_remove)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "319aaa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FOR11047', 'FOR11905', 'FOR11012']\n"
     ]
    }
   ],
   "source": [
    "# no pd since 7 days \n",
    "\n",
    "list_no_pd = df_merged.loc[(df_merged.last_day_GPS > week_ago) & (df_merged.last_day_passive < week_ago)][\"for_id\"].tolist()\n",
    "list_no_pd = [string for string in list_no_pd if string not in remove_scanwatch]\n",
    "print(list_no_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5adefa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FOR11024', 'FOR11063', 'FOR14086', 'FOR12017', 'FOR12024', 'FOR12022', 'FOR13030', 'FOR11016', 'FOR14059', 'FOR12007', 'FOR13031', 'FOR11011', 'FOR11040', 'FOR11043', 'FOR14051', 'FOR12030', 'FOR13025']\n"
     ]
    }
   ],
   "source": [
    "# no gps for > 7 days \n",
    "list_no_gps = df_merged.loc[(df_merged.last_day_GPS < week_ago)& (df_merged.last_day_passive > week_ago)][\"for_id\"].tolist()\n",
    "list_no_gps = [string for string in list_no_gps if string not in remove_gps]\n",
    "print(list_no_gps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "963b30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FOR14080', 'FOR13016', 'FOR13010', 'FOR11053', 'FOR14020']\n"
     ]
    }
   ],
   "source": [
    "# no gps and no pd for > 7 days\n",
    "\n",
    "list_no_gpspd = df_merged.loc[(df_merged.last_day_GPS < week_ago) & (df_merged.last_day_passive < week_ago)][\"for_id\"].tolist()\n",
    "list_no_gpspd = [string for string in list_no_gpspd if string not in users_to_remove]\n",
    "print(list_no_gpspd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c66972ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# no gps at all \n",
    "\n",
    "list_no_gps_at_all = df_merged.loc[df_merged.last_day_GPS.isna()].for_id.tolist()\n",
    "list_no_gps_at_all = [string for string in list_no_gps_at_all if string not in remove_gps]\n",
    "print(list_no_gps_at_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4be2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Append each list to the DataFrame\n",
    "date_today = datetime.today().date()\n",
    "for column_name, entries in zip(['no_pd', 'no_gps', 'no_gpspd', 'no_gps_at_all'], \n",
    "                                [list_no_pd, list_no_gps, list_no_gpspd, list_no_gps_at_all]):\n",
    "    # Create a new DataFrame for the current list\n",
    "    temp_df = pd.DataFrame(entries, columns=[column_name])\n",
    "    \n",
    "    # Add the \"Datum\" column with today's date\n",
    "    temp_df['Datum'] = date_today\n",
    "    \n",
    "    # Since we're appending column-wise, we align other columns by setting them to NaN\n",
    "    # This step ensures the DataFrame has all the necessary columns\n",
    "    for col in df.columns:\n",
    "        if col not in temp_df.columns:\n",
    "            temp_df[col] = \"\"\n",
    "    \n",
    "    # Concatenate the new DataFrame to the original DataFrame\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "# Reorder df columns to match the original order, if necessary\n",
    "df = df[['Datum', 'no_pd', 'no_gps', 'no_gpspd', 'data_deleted', \n",
    "         'no_gps_at_all', 'Status', 'Smartphone', 'Grund', \n",
    "         'Grund (frei)', 'Unnamed: 10']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6304b",
   "metadata": {},
   "source": [
    "## 4. Create Monitoring Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10b9494c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'datetime.date' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m date_part\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Apply the conversion function to preprocess the dates\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df_monitoring[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_call_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_monitoring[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_call_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(convert_date)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Convert the preprocessed date strings to datetime objects\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#df_monitoring['first_call_date'] = pd.to_datetime(df_monitoring['first_call_date'], format=\"%d.%m.%Y\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m date_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema_base_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema_base_start\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema_t20_start\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema_t20_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_call_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m, in \u001b[0;36mconvert_date\u001b[0;34m(date_str)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_date\u001b[39m(date_str):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Remove the day of the week by splitting on the comma and taking the second part\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     date_part \u001b[38;5;241m=\u001b[39m date_str\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m date_part\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'datetime.date' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "def convert_date(date_str):\n",
    "# Remove the day of the week by splitting on the comma and taking the second part\n",
    "    date_part = date_str.split(\", \")[1]\n",
    "    return date_part\n",
    "\n",
    "# Apply the conversion function to preprocess the dates\n",
    "df_monitoring['first_call_date'] = df_monitoring['first_call_date'].apply(convert_date)\n",
    "\n",
    "# Convert the preprocessed date strings to datetime objects\n",
    "#df_monitoring['first_call_date'] = pd.to_datetime(df_monitoring['first_call_date'], format=\"%d.%m.%Y\")\n",
    "\n",
    "date_columns = [\"ema_base_end\", \"ema_base_start\", \"ema_t20_start\", \"ema_t20_end\", \"first_call_date\"]\n",
    "df_monitoring = df_monitoring.copy()\n",
    "\n",
    "# Convert multiple columns to datetime'\n",
    "for col in date_columns:\n",
    "    df_monitoring[col] = pd.to_datetime(df_monitoring[col], errors='coerce', dayfirst=True).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14abb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday 18.03:  Onboarding Call: ['FOR14094', 'FOR14095']\n",
      "Tuesday 19.03:  Kurzversion Wechsel Reminder: ['FOR14091']; Baseline Ende Lang (Status ändern): ['FOR14090', 'FOR11062']; T20 Ende (Status ändern): ['FOR14009']\n",
      "Wednesday 20.03:  Onboarding Call: ['FOR12036']; Baseline Ende Lang (Status ändern): ['FOR11063', 'FOR13036']; Kurzversion Ende: ['FOR14091']\n",
      "Thursday 21.03:  T20 Ende (Status ändern): ['FOR14013']\n",
      "Friday 22.03:  Baseline Ende Lang (Status ändern): ['FOR12035', 'FOR13037']; T20 Start (Status ändern): ['FOR14005']; T20 Ende (Status ändern): ['FOR14002']\n",
      "Saturday 23.03: No actions needed\n",
      "Sunday 24.03: No actions needed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use today's date, but focus only on the date part\n",
    "today = pd.Timestamp('today').date()\n",
    "\n",
    "# Calculate the most recent Monday as the start of the week\n",
    "week_start_date = today - pd.Timedelta(days=today.weekday())\n",
    "\n",
    "# Loop through each day of the week from Monday to Friday\n",
    "for day in range(7):  # Monday to Friday\n",
    "    target_date = week_start_date + pd.Timedelta(days=day)\n",
    "\n",
    "    # Format the date to include the weekday name and day.month\n",
    "    formatted_date = target_date.strftime(\"%A %d.%m\")  # e.g., \"Monday 12.02\"\n",
    "\n",
    "    # Initialize message list\n",
    "    messages = []\n",
    "\n",
    "    # Assuming the filtering logic is already correctly implemented\n",
    "\n",
    "    # Debug prints to check if conditions are met (example messages for illustration)\n",
    "    \n",
    "    onboarding_ids = df_monitoring[df_monitoring['first_call_date'] == target_date]['for_id'].tolist()      \n",
    "    \n",
    "    reminder_ids = df_monitoring[(df_monitoring['ema_base_end'] == (target_date + pd.Timedelta(days=1))) &\n",
    "                                 (df_monitoring['study_version'].isin(['Kurz', 'Kurz (Wechsel/Abbruch)']))]['for_id'].tolist()\n",
    "\n",
    "    baseline_ended_ids = df_monitoring[(df_monitoring['ema_base_end'] == target_date - pd.Timedelta(days=1)) &\n",
    "                                       (df_monitoring['study_version'].isin(['Lang', 'Lang(Wechsel)']))]['for_id'].tolist()\n",
    "    \n",
    "    study_finished_ids = df_monitoring[(df_monitoring['ema_base_end'] == target_date) &\n",
    "                                       (df_monitoring['study_version'].isin(['Kurz', 'Kurz (Wechsel/Abbruch)']))]['for_id'].tolist()\n",
    "\n",
    "    t20_ended_ids = df_monitoring[df_monitoring['ema_t20_end'] == (target_date - pd.Timedelta(days=1))]['for_id'].tolist()\n",
    "    \n",
    "    t20_start_ids = df_monitoring[df_monitoring['ema_t20_start'] == target_date]['for_id'].tolist()\n",
    "\n",
    "\n",
    "    if onboarding_ids:\n",
    "        messages.append(f\"Onboarding Call: {onboarding_ids}\")\n",
    "    if reminder_ids:\n",
    "        messages.append(f\"Kurzversion Wechsel Reminder: {reminder_ids}\")\n",
    "    if baseline_ended_ids:\n",
    "        messages.append(f\"Baseline Ende Lang (Status ändern): {baseline_ended_ids}\")\n",
    "    if t20_start_ids:\n",
    "        messages.append(f\"T20 Start (Status ändern): {t20_start_ids}\")\n",
    "    if t20_ended_ids:\n",
    "        messages.append(f\"T20 Ende (Status ändern): {t20_ended_ids}\")\n",
    "    if study_finished_ids:\n",
    "        messages.append(f\"Kurzversion Ende: {study_finished_ids}\")\n",
    "\n",
    "    # Print the formatted date along with the actions\n",
    "    if messages:\n",
    "        print(f\"{formatted_date}: \", \"; \".join(messages))\n",
    "    else:\n",
    "        print(f\"{formatted_date}: No actions needed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8b93a",
   "metadata": {},
   "source": [
    "## 5. Export missing data to google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd9ca8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.applymap(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2dba8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a google sheet\n",
    "gs = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1z8LZJBBMzzAmiXIS47X8SLk-zSMwDIXSKPit4IlmfuE\")\n",
    "# select a work sheet from its name\n",
    "worksheet1 = gs.worksheet('Datenqualität')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc3809d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ys/gx19nmhj6v30dlkkh5nfp5xh0000gn/T/ipykernel_30218/4283573234.py:8: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  worksheet1.update(start_range, missing_data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1z8LZJBBMzzAmiXIS47X8SLk-zSMwDIXSKPit4IlmfuE',\n",
       " 'updatedRange': \"'Datenqualität'!A635:K659\",\n",
       " 'updatedRows': 25,\n",
       " 'updatedColumns': 11,\n",
       " 'updatedCells': 275}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "next_row = len(worksheet1.col_values(1)) + 1  # Assuming column A has index 1\n",
    "\n",
    "# Construct the range string where you want to start appending data\n",
    "# For example, if starting from column A and the next available row is 10, the range would be 'A10'\n",
    "start_range = f'A{next_row}'\n",
    "\n",
    "# Use the update method to append data starting from the specified cell\n",
    "worksheet1.update(start_range, missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eea0a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = pd.read_csv(\"https://docs.google.com/spreadsheets/d/1z8LZJBBMzzAmiXIS47X8SLk-zSMwDIXSKPit4IlmfuE/export?format=csv&gid=1512138040\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28951a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame\n",
    "df_long = df_missing.melt(id_vars=[\"Datum\"], value_vars=[\"no_pd\", \"no_gps\", \"no_gpspd\"], var_name=\"condition\", value_name=\"for_id\")\n",
    "\n",
    "# Drop rows where ID is NaN\n",
    "df_long = df_long.dropna(subset=[\"for_id\"])\n",
    "\n",
    "# Count occurrences of each ID per condition\n",
    "df_count = df_long.groupby(\"for_id\")[\"condition\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Optionally, rename columns back to original condition names for clarity\n",
    "df_count.columns = [\"no_pd_count\", \"no_gps_count\", \"no_gpspd_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ee80a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count[\"missing_sum\"] = df_count.no_pd_count + df_count.no_gps_count + df_count.no_gpspd_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "724f90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count[\"missing_relative\"] = df_count.missing_sum / df_long.Datum.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c53eded9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    109.000000\n",
       "mean       0.169586\n",
       "std        0.164542\n",
       "min        0.030303\n",
       "25%        0.060606\n",
       "50%        0.090909\n",
       "75%        0.212121\n",
       "max        0.787879\n",
       "Name: missing_relative, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count.missing_relative.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d92c1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    109.000000\n",
       "mean       5.596330\n",
       "std        5.429889\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        3.000000\n",
       "75%        7.000000\n",
       "max       26.000000\n",
       "Name: missing_sum, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count.missing_sum.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72a7fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = pd.merge(df_active, df_count, on = \"for_id\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5b8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
