{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada85fd6",
   "metadata": {},
   "source": [
    "# GPS_Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2274b264",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/leonahammelrath/FU_Psychoinformatik/Github/tiki_code/data//ema_data_29042024.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m today \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m29042024\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(datapath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/ema_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     24\u001b[0m     df_active \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(datapath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/gps_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/leonahammelrath/FU_Psychoinformatik/Github/tiki_code/data//ema_data_29042024.pkl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from config import datapath\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import statistics \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns \n",
    "sns.set_context(\"notebook\", rc={\"axes.labelsize\": 14, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': True})\n",
    "%matplotlib inline\n",
    "\n",
    "today = \"29042024\"\n",
    "\n",
    "with open(datapath + f'ema_data_{today}.pkl', 'rb') as file:\n",
    "    df_active = pickle.load(file)\n",
    "\n",
    "with open(datapath + f'gps_data_{today}.pkl', 'rb') as file:\n",
    "    df_gps = pickle.load(file)\n",
    "    \n",
    "with open(datapath + f'passive_data_{today}.pkl', 'rb') as file:\n",
    "    df_passive = pickle.load(file)\n",
    "\n",
    "with open(datapath + f'monitoring_data_{today}.pkl', 'rb') as file:\n",
    "    df_monitoring = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2957c",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33abe348",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hour_daily = 10\n",
    "min_days_data = 20\n",
    "\n",
    "#stationary filtering\n",
    "max_distance = 150 \n",
    "speed_limit = 1.4  # Max allowed speed in m/s\n",
    "\n",
    "# DBSCAN\n",
    "kms_per_radian = 6371.0088 # equitorial radius of the earth = 6,378.1 \n",
    "epsilon = 0.05/kms_per_radian\n",
    "min_samples = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e185a",
   "metadata": {},
   "source": [
    "## Prepare the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92362062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for participants that have finished\n",
    "df_gps_merged = df_gps.merge(df_monitoring, on = \"customer\", how=\"inner\")\n",
    "\n",
    "df_gps_merged = df_gps_merged.loc[df_gps_merged.status == \"Abgeschlossen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d925d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_gps.pivot_table(\n",
    "    index=[\"customer\", \"startTimestamp\"],\n",
    "    columns=\"type\",\n",
    "    values=[\"doubleValue\", \"startTimestamp_hour\", \"startTimestamp_day\"],\n",
    "    aggfunc='first'  # Using 'first' since each type should theoretically have only one entry per customer and timestamp\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex in columns\n",
    "df_int.columns = ['_'.join(col).strip() for col in df_int.columns.values]\n",
    "\n",
    "df_int = df_int.rename_axis(None, axis=1).reset_index()\n",
    "\n",
    "# Drop redundant day and hour columns for longitude (assuming latitude day and hour are kept)\n",
    "df_int = df_int.drop(columns=[\n",
    "    'startTimestamp_day_Longitude',\n",
    "    'startTimestamp_hour_Longitude'\n",
    "])\n",
    "\n",
    "# Rename the columns for clarity\n",
    "df_int = df_int.rename(columns={\n",
    "    'doubleValue_Latitude': 'Latitude',\n",
    "    'doubleValue_Longitude': 'Longitude',\n",
    "    'startTimestamp_day_Latitude': 'Day',  # Keeping one 'Day' column\n",
    "    'startTimestamp_hour_Latitude': 'Hour'  # Keeping one 'Hour' column\n",
    "})\n",
    "\n",
    "df_int['weekday'] = df_int['Day'].dt.day_name()\n",
    "df_int[\"n_hours\"] = df_int.groupby([\"customer\", \"Day\"])[\"Hour\"].transform(\"nunique\")\n",
    "df_int[\"n_data\"] = df_int.groupby(\"customer\")[\"Longitude\"].transform(\"size\")\n",
    "df_int[\"n_data_day\"] = df_int.groupby([\"customer\", \"Day\"])[\"Longitude\"].transform(\"size\")\n",
    "df_int[\"n_data_hour\"] = df_int.groupby([\"customer\", \"Hour\"])[\"Longitude\"].transform(\"size\")\n",
    "\n",
    "df_int = df_int.loc[df_int[\"n_hours\"] >= min_hour_daily]\n",
    "df_int[\"n_days_8\"] = df_int.groupby(\"customer\")[\"Day\"].transform(\"nunique\")\n",
    "df_int = df_int.loc[df_int[\"n_days_8\"] >= min_days_data]\n",
    "\n",
    "# Ensure your DataFrame is sorted by customer and day\n",
    "df_int = df_int.sort_values(by=['customer', 'Day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c5836",
   "metadata": {},
   "source": [
    "## Descriptive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6de323",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x='customer', y='n_data', data=df_int)\n",
    "plt.title('Number of GPS points per ID',fontsize=14)\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "\n",
    "#plt.savefig(\"barplot_high_quality.png\", dpi=300, format='png', bbox_inches='tight')\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x='customer', y='n_data_day', data=df_int)\n",
    "plt.title('Number of data per day per customer')\n",
    "plt.ylabel('Number of data per day')\n",
    "plt.xlabel('Customer ID')\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0fcd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x='Hour', y='n_data_hour', data=df_int)\n",
    "plt.title('Number of data per day per hour')\n",
    "plt.ylabel('Number of data per day')\n",
    "plt.xlabel('Customer ID')\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a95639",
   "metadata": {},
   "source": [
    "## Data filtering by converting GPS data to stationary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c69a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the DataFrame to get previous row’s data\n",
    "df_speed = df_int.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32083ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine formula to calculate distance between two lat/lon points in meters\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    R = 6371000  # Radius of Earth in meters\n",
    "    phi_1 = np.radians(lat1)\n",
    "    phi_2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(delta_phi/2.0)**2 + np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda/2.0)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    meters = R * c  # Output distance in meters\n",
    "    return meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88601da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing columns to store calculated values\n",
    "df_speed['distance'] = np.nan\n",
    "df_speed['time_diff'] = np.nan\n",
    "df_speed['speed'] = np.nan\n",
    "\n",
    "# Calculating distance, time difference, and speed for each customer independently\n",
    "for customer in df_speed['customer'].unique():\n",
    "    mask = df_speed['customer'] == customer\n",
    "    \n",
    "    df_speed.loc[mask, 'distance'] = np.concatenate([\n",
    "        haversine(\n",
    "            df_speed.loc[mask, 'Longitude'].values[:-1], df_speed.loc[mask, 'Latitude'].values[:-1],\n",
    "            df_speed.loc[mask, 'Longitude'].values[1:], df_speed.loc[mask, 'Latitude'].values[1:]\n",
    "        ),\n",
    "        [0]\n",
    "    ])\n",
    "    \n",
    "    df_speed.loc[mask, 'time_diff'] = df_speed.loc[mask, 'startTimestamp'].diff().dt.total_seconds().fillna(0)\n",
    "    \n",
    "    # Avoid division by zero and replace NaN if time_diff is 0\n",
    "    df_speed.loc[mask, 'speed'] = df_speed.loc[mask, 'distance'] / df_speed.loc[mask, 'time_diff'].replace(0, np.nan)\n",
    "\n",
    "\n",
    "# Creating the stationary DataFrame\n",
    "stationary_df = df_speed[(df_speed['speed'] < speed_limit) & (df_speed['distance'] < max_distance)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_df.n_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299eb2af",
   "metadata": {},
   "source": [
    "## Prepare DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a31d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for clustering\n",
    "def db2(x):\n",
    "    clustering_model = DBSCAN(eps=epsilon, min_samples=min_samples, metric=\"haversine\")\n",
    "    cluster_labels = clustering_model.fit_predict(x[['Longitude', 'Latitude']].apply(np.radians))\n",
    "    return pd.DataFrame({'cluster_100m': cluster_labels})\n",
    "\n",
    "# Group by 'userID' and apply clustering function\n",
    "geodata_cluster_df = stationary_df.groupby('customer').apply(lambda x: db2(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge this with the main data frame\n",
    "geodata_clusters = pd.concat([stationary_df.reset_index(drop=True), geodata_cluster_df['cluster_100m']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b943f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a new DataFrame to store processed data\n",
    "plot_data = pd.DataFrame()\n",
    "\n",
    "# Calculating the count of \"-\" values per customer\n",
    "plot_data['negative_count'] = geodata_clusters[geodata_clusters['cluster_100m'] == -1].groupby('customer').size()\n",
    "\n",
    "# Calculating the count of non \"-\" values per customer\n",
    "plot_data['positive_count'] = geodata_clusters[geodata_clusters['cluster_100m'] != -1].groupby('customer').size()\n",
    "\n",
    "# Filling NaN with 0s (for customers with no \"-\" values)\n",
    "plot_data = plot_data.fillna(0)\n",
    "\n",
    "# Plotting\n",
    "ax = plot_data.plot(kind='bar', stacked=True, figsize=(10, 6), color=['salmon', 'cornflowerblue'], width=0.8)\n",
    "plt.title('Results of DBScan Clustering with eps=50m and min_points=10',  fontsize=14)\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "\n",
    "# Adjusting the legend\n",
    "plt.legend([\"Noise\", \"Assigned to Cluster\"], loc='upper right')\n",
    "\n",
    "plt.savefig('dbscan_count.png', dpi=300)\n",
    "# Showing the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_clusters = geodata_clusters[geodata_clusters['cluster_100m'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique clusters per customer\n",
    "unique_clusters = geodata_cluster_df.groupby('customer')['cluster_100m'].nunique()\n",
    "# Plotting\n",
    "unique_clusters.plot(kind='bar', figsize=(10, 6), color='skyblue')\n",
    "plt.xlabel('Customer')\n",
    "plt.ylabel('Number of Unique Clusters')\n",
    "plt.title('Number of Unique Clusters per Customer')\n",
    "plt.xticks(rotation=45)  # Rotate labels to avoid overlap, adjust as necessary\n",
    "plt.tight_layout()  # Adjusts plot to ensure everything fits without overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique IDs for clusters\n",
    "geodata_clusters['clusterID'] = geodata_clusters['customer'].astype(str) + '00' + \\\n",
    "geodata_clusters['cluster_100m'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a4719",
   "metadata": {},
   "source": [
    "## Generate Home Location from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for night hours (midnight to 6:00 am)\n",
    "geodata_night = geodata_clusters.loc[(geodata_clusters['Hour'] >= 0) & (geodata_clusters['Hour'] < 6)]\n",
    "\n",
    "# Find the mode of clusterID per user during night hours\n",
    "geodata_night = geodata_night.copy()\n",
    "geodata_night['home'] = geodata_night.groupby('customer')['clusterID'].transform(lambda x: statistics.mode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating various metrics to validate the home cluster\n",
    "geodata_night['nights_with_obs'] = geodata_night.groupby('customer')['Day'].transform('nunique')\n",
    "geodata_night['night_obs'] = geodata_night.groupby('customer')['Day'].transform('size')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca88966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the frequency of the mode\n",
    "geodata_night['n_home'] = geodata_night.groupby('customer')['home'].transform(lambda x: x.value_counts().iloc[0])\n",
    "geodata_night['f_home'] = geodata_night['n_home'] / geodata_night['night_obs']\n",
    "\n",
    "# Updating the 'home' label based on conditions\n",
    "geodata_night['home'] = geodata_night.apply(\n",
    "    lambda x: x['home'] if x['nights_with_obs'] >= 7 and x['f_home'] > 0.5 else None, axis=1\n",
    ")\n",
    "\n",
    "# Extracting a mapping of userID to home cluster\n",
    "user_home_mapping = geodata_night[['customer', 'home']].drop_duplicates()\n",
    "\n",
    "# Merging back to the full dataset\n",
    "geodata_clusters = pd.merge(geodata_clusters, user_home_mapping, on='customer', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab relevant columns\n",
    "cbusdata = geodata[['customer', 'date']]\n",
    "clusterdata = geodata_clusters[['customer', 'date', 'clusterID']]\n",
    "\n",
    "# Merge dataframes\n",
    "df_int = pd.merge(df_int, clusterdata, how='left', on=['customer', 'date'])\n",
    "df_int = pd.merge(df_int, cbusdata, how='left', on=['customer', 'date'])\n",
    "\n",
    "# Order the data\n",
    "df_int = df_int.sort_values(by=['customer', 'date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd6863",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
