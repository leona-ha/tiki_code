{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdcf9a0",
   "metadata": {},
   "source": [
    "# Explore GPS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5bb069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.preview in file /Users/leonahammelrath/opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file /Users/leonahammelrath/opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file /Users/leonahammelrath/opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file /Users/leonahammelrath/opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file /Users/leonahammelrath/opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file /Users/leonahammelrath/opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.stats import mode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "today = date.today()\n",
    "week_ago = today - dt.timedelta(days=7)\n",
    "today = today.strftime(\"%d%m%Y\")\n",
    "week_ago = week_ago.strftime(\"%Y-%m-%d\")\n",
    "today = \"06102023\"\n",
    "\n",
    "datapath = \"/Users/leonahammelrath/FU_Psychoinformatik/Github/tiki_code/data/\"\n",
    "filepath = datapath + f\"export_{today}.csv\"\n",
    "\n",
    "datapath1 = datapath + f\"export_tiki_{today}/\"\n",
    "filepath_1 = datapath1 + \"epoch_part0001.csv\"\n",
    "filepath_2 = datapath1 + \"epoch_part0002.csv\"\n",
    "filepath_3 = datapath1 + \"epoch_part0003.csv\"\n",
    "filepath_4 = datapath1 + \"epoch_part0004.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636c2900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonahammelrath/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(filepath_1, encoding= \"latin-1\")\n",
    "df_2 = pd.read_csv(filepath_2, encoding= \"latin-1\")\n",
    "df_3 = pd.read_csv(filepath_3, encoding= \"latin-1\")\n",
    "df_4 = pd.read_csv(filepath_4, encoding= \"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f68b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location_1 = df_1[df_1.type.isin([\"Latitude\", \"Longitude\"])]\n",
    "df_location_2 = df_2[df_2.type.isin([\"Latitude\", \"Longitude\"])]\n",
    "df_location_3 = df_3[df_3.type.isin([\"Latitude\", \"Longitude\"])]\n",
    "df_location_4 = df_4[df_4.type.isin([\"Latitude\", \"Longitude\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ff81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active = pd.read_csv(\"/Users/leonahammelrath/FU_Psychoinformatik/Github/tiki_code/data/active_participants.csv\")\n",
    "df_active.rename(columns = {\"Pseudonym \": \"customer\", \"EMA ID\": \"ema_id\", \"Ende EMA Baseline\": \"end_ema\", \"Status\": \"status\",\n",
    "                            \"Start EMA Baseline\": \"start_ema\", \"Studienversion\":\"study_version\", \"FOR_ID\":\"for_id\"}, inplace=True)\n",
    "df_active[\"customer\"] = df_active[\"customer\"].str[:4]\n",
    "df_active = df_active[[\"customer\", \"end_ema\", \"start_ema\", \"status\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bcc9606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = pd.concat([df_location_1, df_location_2, df_location_3,df_location_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8015903a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete.customer.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb2ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = df_complete[[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\", \n",
    "                           \"timezoneOffset\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df78c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete[\"customer\"] = df_complete.customer.str.split(\"@\").str.get(0)\n",
    "df_complete[\"customer\"] = df_complete[\"customer\"].str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51fe4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete[\"timezoneOffset\"] = df_complete[\"timezoneOffset\"] * 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7e0d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete[\"date\"] = df_complete[\"startTimestamp\"] + df_complete[\"timezoneOffset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30918199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = pd.merge(df_complete, df_active, on=\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74afca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = df_complete.loc[df_complete.status == \"Abgeschlossen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa708c60",
   "metadata": {},
   "source": [
    "## Analyze GPS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae88908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_complete.pivot(\n",
    "    index=[\"customer\", \"date\"],\n",
    "    columns=\"type\",\n",
    "    values=\"doubleValue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fab41385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_int.rename_axis(None, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72d8d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_int= df_int.sort_values(by=[\"customer\", \"date\"]).drop_duplicates(subset=[\"date\"], keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf28aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int[\"date\"] = (pd.to_datetime(df_int[\"date\"],unit='ms'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6180984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int.dropna(subset = ['Latitude', 'Longitude', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4080e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int[\"day\"] = df_int.date.dt.strftime('%Y/%m/%d')\n",
    "df_int[\"hours\"] = df_int.date.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a17249b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer\n",
       "3ZqH    18\n",
       "3s90    21\n",
       "42aH    40\n",
       "5Ger    17\n",
       "5qL5    37\n",
       "Emob    15\n",
       "Fcz9    16\n",
       "GuiZ    16\n",
       "L8UG    21\n",
       "bNPw    15\n",
       "bVec    18\n",
       "f7RF    19\n",
       "i0to    15\n",
       "tOQ8    22\n",
       "uN1K    14\n",
       "vPEr    16\n",
       "Name: day, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_int.groupby(\"customer\")[\"day\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194603b6",
   "metadata": {},
   "source": [
    "## Perform DBScan on data from single ID 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5078db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single = df_int.loc[df_int.customer == \"0ePW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a76a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = df_single[['Latitude', 'Longitude']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49292c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equitorial radius of the earth = 6,378.1 \n",
    "kms_per_radian = 6371.0088\n",
    "epsilon = 0.2 / kms_per_radian\n",
    "min_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32055765",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=epsilon, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "cluster_labels = db.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16652232",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_l = pd.Series(cluster_labels)\n",
    "df_single[\"cluster_label\"] = cluster_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f63ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = len(set(cluster_labels))- (1 if -1 in cluster_labels else 0)\n",
    "clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)])\n",
    "print('Number of clusters: {}'.format(num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reduced set of coordinate points\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 6)\n",
    "rs_scatter = ax.scatter(df_single['Longitude'], df_single['Latitude'], c='m', edgecolor='k', alpha=.4, s=150)\n",
    " \n",
    "# set axis labels, tick labels, and title\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "#At this scale, only a few dozen of the 1,759 data points are really visible. \n",
    "#Even zoomed in very close, several locations have hundreds of data points stacked directly \n",
    "#on top of each other due to the duration of time spent at one location. Unless we are interested in time \n",
    "#dynamics, we simply do not need all of these spatially redundant points – they just bloat the data set’s size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f9714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reduced set of coordinate points\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 6)\n",
    "\n",
    "rs_scatter = ax.scatter(coords[:, 0], coords[:,1], c=cluster_labels,cmap=\"Paired\",alpha=.8, s=100)\n",
    "\n",
    " \n",
    "# set axis labels, tick labels, and title\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae6bab",
   "metadata": {},
   "source": [
    "## Perform DBScan on data from single ID 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458cd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single2 = df_int.loc[df_int.customer == \"gfo4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ee76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords2 = df_single2[['Latitude', 'Longitude']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reduced set of coordinate points\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 6)\n",
    "rs_scatter = ax.scatter(df_single2['Longitude'], df_single2['Latitude'], c=\"m\", edgecolor='k', alpha=.4, s=150)\n",
    " \n",
    "# set axis labels, tick labels, and title\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26549ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = DBSCAN(eps=epsilon, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(coords2))\n",
    "cluster_labels2 = db2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters2 = len(set(cluster_labels2))- (1 if -1 in cluster_labels2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters2 = len(set(cluster_labels2))\n",
    "clusters = pd.Series([coords2[cluster_labels2 == n] for n in range(num_clusters)])\n",
    "print('Number of clusters: {}'.format(num_clusters2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfac3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reduced set of coordinate points\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 6)\n",
    "rs_scatter = ax.scatter(df_single2['Longitude'], df_single2['Latitude'], c=cluster_labels2, cmap=\"plasma\", alpha=.2, s=100)\n",
    " \n",
    "# set axis labels, tick labels, and title\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722defdc",
   "metadata": {},
   "source": [
    "## Apply DBScan to whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int[\"n_hours\"] = df_int.groupby([\"customer\", \"day\"])[\"hours\"].transform(\"nunique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_int.loc[df_int[\"n_hours\"] >= 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d6c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int[\"n_days_8\"] = df_int.groupby(\"customer\")[\"day\"].transform(\"nunique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4311a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_int.loc[df_int[\"n_days_8\"] >= 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616bda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_day_index(user_group):\n",
    "    user_group['day_index'] = (user_group[\"day\"] != user_group[\"day\"].shift()).cumsum()\n",
    "    return user_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dfb2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_int.groupby('customer').apply(assign_day_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int['hourID'] = df_int['customer'].astype(str) + '0' + \\\n",
    "                    df_int['day_index'].astype(str) + '0' + \\\n",
    "                    df_int['hours'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a006ef",
   "metadata": {},
   "source": [
    "## Transform lon lat coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f889f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For spatial analyses focused on Berlin, Germany, you might opt to use the ETRS89 / UTM zone 33N, which is commonly used for Northern Germany. Its EPSG code is 25833.\n",
    "# Here's a short description: ETRS89: European Terrestrial Reference System 1989. \n",
    "#ETRS89 is a geodetic reference system intended for use over the whole of Europe and is an update to \n",
    "#the older European Datum 1950. UTM zone 33N: Universal Transverse Mercator coordinate system, \n",
    "# zone 33N represents a longitudinal zone of 6 degrees that covers Berlin and is suitable for small to \n",
    "# medium-scale mapping efforts, especially when accuracy in metric measurements is required.\n",
    "\n",
    "# Assuming you have a DataFrame named `data` with 'longitude' and 'latitude'\n",
    "# Creating a GeoDataFrame, assuming 'lon' and 'lat' are your coordinate column names\n",
    "geometry = [Point(xy) for xy in zip(df_int['Longitude'], df_int['Latitude'])]\n",
    "geodata = gpd.GeoDataFrame(df_int, geometry=geometry)\n",
    "\n",
    "# Defining the CRS for WGS 84 and ETRS89 / UTM zone 33N\n",
    "wgs84 = {'init': 'epsg:4326'}\n",
    "utm_33n = {'init': 'epsg:25833'}\n",
    "\n",
    "# Assigning the initial CRS and reprojecting to UTM 33N\n",
    "geodata.crs = wgs84\n",
    "geodata = geodata.to_crs(utm_33n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2435f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equitorial radius of the earth = 6,378.1 \n",
    "kms_per_radian = 6371.0088\n",
    "epsilon = 0.2 / kms_per_radian\n",
    "min_samples = 10\n",
    "\n",
    "geodata['x'] = geodata['geometry'].x\n",
    "geodata['y'] = geodata['geometry'].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d64364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for clustering\n",
    "def db2(x):\n",
    "    clustering_model = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "    cluster_labels = clustering_model.fit_predict(x[['x', 'y']])\n",
    "    return pd.DataFrame({'cluster_100m': cluster_labels})\n",
    "\n",
    "# Group by 'userID' and apply clustering function\n",
    "geodata_cluster_df = geodata.groupby('customer').apply(lambda x: db2(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25cc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge this with the main data frame\n",
    "geodata_clusters = pd.concat([geodata.reset_index(drop=True), geodata_cluster_df['cluster_100m']], axis=1)\n",
    "geodata_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e99798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete outliers (where cluster_20m == -1)\n",
    "geodata_clusters = geodata_clusters[geodata_clusters['cluster_100m'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_clusters.groupby(\"customer\").cluster_100m.nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_clusters.cluster_100m.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique IDs for clusters\n",
    "geodata_clusters['clusterID'] = geodata_clusters['customer'].astype(str) + '00' + \\\n",
    "geodata_clusters['cluster_100m'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e34ac8e",
   "metadata": {},
   "source": [
    "## Generate Home Location from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for night hours (midnight to 6:00 am)\n",
    "geodata_night = geodata_clusters.loc[(geodata_clusters['hours'] >= 0) & (geodata_clusters['hours'] < 6)]\n",
    "\n",
    "# Find the mode of clusterID per user during night hours\n",
    "geodata_night = geodata_night.copy()\n",
    "geodata_night['home'] = geodata_night.groupby('customer')['clusterID'].transform(lambda x: mode(x)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating various metrics to validate the home cluster\n",
    "geodata_night['nights_with_obs'] = geodata_night.groupby('customer')['day'].transform('nunique')\n",
    "geodata_night['night_obs'] = geodata_night.groupby('customer')['day'].transform('size')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the frequency of the mode\n",
    "geodata_night['n_home'] = geodata_night.groupby('customer')['home'].transform(lambda x: x.value_counts().iloc[0])\n",
    "geodata_night['f_home'] = geodata_night['n_home'] / geodata_night['night_obs']\n",
    "\n",
    "# Updating the 'home' label based on conditions\n",
    "geodata_night['home'] = geodata_night.apply(\n",
    "    lambda x: x['home'] if x['nights_with_obs'] >= 7 and x['f_home'] > 0.5 else None, axis=1\n",
    ")\n",
    "\n",
    "# Extracting a mapping of userID to home cluster\n",
    "user_home_mapping = geodata_night[['customer', 'home']].drop_duplicates()\n",
    "\n",
    "# Merging back to the full dataset\n",
    "geodata_clusters = pd.merge(geodata_clusters, user_home_mapping, on='customer', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab relevant columns\n",
    "cbusdata = geodata[['customer', 'date']]\n",
    "clusterdata = geodata_clusters[['customer', 'date', 'clusterID']]\n",
    "\n",
    "# Merge dataframes\n",
    "df_int = pd.merge(df_int, clusterdata, how='left', on=['customer', 'date'])\n",
    "df_int = pd.merge(df_int, cbusdata, how='left', on=['customer', 'date'])\n",
    "\n",
    "# Order the data\n",
    "df_int = df_int.sort_values(by=['customer', 'date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc567ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shift vectors\n",
    "def shift_vector(vec, shift):\n",
    "    if shift > 0:\n",
    "        return np.concatenate((np.full(shift, np.nan), vec[:-shift]))\n",
    "    elif shift < 0:\n",
    "        return np.concatenate((vec[-shift:], np.full(-shift, np.nan)))\n",
    "    else:\n",
    "        return vec\n",
    "\n",
    "# Shift vectors for lat and long\n",
    "df_int['lat_p1'] = df_int.groupby('customer')['x'].transform(lambda x: shift_vector(x, -1))\n",
    "df_int['lon_p1'] = df_int.groupby('customer')['y'].transform(lambda x: shift_vector(x, -1))\n",
    "\n",
    "# haversine formula for distance calculation between two lat,lon points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000  # radius of Earth in meters\n",
    "    phi_1 = np.radians(lat1)\n",
    "    phi_2 = np.radians(lat2)\n",
    "    \n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    \n",
    "    a = np.sin(delta_phi/2.0)**2 + np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda/2.0)**2\n",
    "    \n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    \n",
    "    meters = R * c  # output distance in meters\n",
    "    return meters\n",
    "\n",
    "# Calculate distance (in meters) using the haversine function\n",
    "df_int['dist_to_prev'] = haversine(df_int['lat'], df_int['lon'], df_int['lat_p1'], df_int['lon_p1'])\n",
    "\n",
    "# Shift vectors for clusterID\n",
    "df_int['clusterID_p1'] = df_int.groupby('customer')['clusterID'].transform(lambda x: shift_vector(x, -1))\n",
    "\n",
    "# Set distance traveled to 0 if the cluster was the same\n",
    "df_int.loc[df_int['clusterID'] == df_int['clusterID_p1'], 'dist_to_prev'] = 0\n",
    "\n",
    "# Aggregate distance for each user\n",
    "agg = df_int.groupby('customer')['dist_to_prev'].sum().reset_index(name='distanceM')\n",
    "agg['distanceKM'] = agg['distanceM'] / 1000\n",
    "\n",
    "# Add to main dataframe\n",
    "df_int = pd.merge(df_int, agg, on='customer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec02484",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
